---
title: "Code-enabled language models can outperform reasoning models on diverse tasks"
date: 2025-10-28
url: https://arxiv.org/abs/2510.20909
domain: arxiv.org
lang: en
tags: ["Paper"]
---
# Code-enabled language models can outperform reasoning models on diverse tasks

## 🔗 引用元
- **URL**: https://arxiv.org/abs/2510.20909
- **サイト**: arXiv.org
- **著者**: [Submitted on 23 Oct 2025]
- **言語**: English
## 🧭 概要
View PDF
    HTML (experimental)
            Abstract:Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. In this paper, we show that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, our simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%).

## 📝 詳細レポート
[View PDF](https://arxiv.org/pdf/2510.20909) [HTML (experimental)](https://arxiv.org/html/2510.20909v1)

> Abstract:Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. In this paper, we show that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, our simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%). Furthermore, the code-augmented reasoning traces display rich and varied problem-solving strategies. Our findings support that (1) CodeAdapt-style learning and reasoning may be robust and domain general and (2) code-enabled LMs are cognitively grounded and powerful systems, potentially providing a strong foundation for in-weight reinforcement learning.

Submission history
------------------

From: Cedegao Zhang \[[view email](https://arxiv.org/show-email/d09e1455/2510.20909)\]  
**\[v1\]** Thu, 23 Oct 2025 18:04:03 UTC (995 KB)
