---
title: "Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications"
date: 2025-10-28
url: https://arxiv.org/abs/2510.21131
domain: arxiv.org
lang: en
tags: ["Paper"]
---
# Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications

## 🔗 引用元
- **URL**: https://arxiv.org/abs/2510.21131
- **サイト**: arXiv.org
- **著者**: [Submitted on 24 Oct 2025]
- **言語**: English
## 🧭 概要
View PDF
    HTML (experimental)
            Abstract:Large Language Models (LLMs) have achieved remarkable success in natural language processing through strong semantic understanding and generation. However, their black-box nature limits structured and multi-hop reasoning. In contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures enriched with textual context, yet often lack semantic depth. Recent research shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG representation learning and improving the reasoning and interpretability of LLMs.

## 📝 詳細レポート
[View PDF](https://arxiv.org/pdf/2510.21131) [HTML (experimental)](https://arxiv.org/html/2510.21131v1)

> Abstract:Large Language Models (LLMs) have achieved remarkable success in natural language processing through strong semantic understanding and generation. However, their black-box nature limits structured and multi-hop reasoning. In contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures enriched with textual context, yet often lack semantic depth. Recent research shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG representation learning and improving the reasoning and interpretability of LLMs. This survey provides the first systematic review of LLM--TAG integration from an orchestration perspective. We introduce a novel taxonomy covering two fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and TAG for LLM, where structured graphs improve LLM reasoning. We categorize orchestration strategies into sequential, parallel, and multi-module frameworks, and discuss advances in TAG-specific pretraining, prompting, and parameter-efficient fine-tuning. Beyond methodology, we summarize empirical insights, curate available datasets, and highlight diverse applications across recommendation systems, biomedical analysis, and knowledge-intensive question answering. Finally, we outline open challenges and promising research directions, aiming to guide future work at the intersection of language and graph learning.

Submission history
------------------

From: Guangxin Su \[[view email](https://arxiv.org/show-email/a5b685ca/2510.21131)\]  
**\[v1\]** Fri, 24 Oct 2025 03:53:00 UTC (1,378 KB)
