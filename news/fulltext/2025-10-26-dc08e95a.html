<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="norton-safeweb-site-verification" content="24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd" />
        
        <link rel="preconnect" href="https://substackcdn.com" />
        

        
            <title data-rh="true">Andrej Karpathy — AGI is still a decade away</title>
            
            <meta data-rh="true" name="theme-color" content="#ffffff"/><meta data-rh="true" name="twitter:player" content="https://www.dwarkesh.com/embed/podcast/andrej-karpathy?autoplay=1"/><meta data-rh="true" name="twitter:player:width" content="1"/><meta data-rh="true" name="twitter:player:height" content="1"/><meta data-rh="true" name="twitter:text:player_width" content="1"/><meta data-rh="true" name="twitter:text:player_height" content="1"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="og:title" content="Andrej Karpathy — AGI is still a decade away"/><meta data-rh="true" name="twitter:title" content="Andrej Karpathy — AGI is still a decade away"/><meta data-rh="true" name="description" content="&quot;The problems are tractable, but they&#x27;re still difficult”"/><meta data-rh="true" property="og:description" content="&quot;The problems are tractable, but they&#x27;re still difficult”"/><meta data-rh="true" name="twitter:description" content="&quot;The problems are tractable, but they&#x27;re still difficult”"/><meta data-rh="true" property="og:image" content="https://substackcdn.com/image/fetch/$s_!FQmr!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F176425744%2F248940bd-cafb-4de4-bf60-6b24fae0fa2e%2Ftranscoded-1760720000.png"/><meta data-rh="true" name="twitter:image" content="https://substackcdn.com/image/fetch/$s_!r_Hd!,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fdwarkesh.substack.com%2Fapi%2Fv1%2Fpost_preview%2F176425744%2Ftwitter.jpg%3Fversion%3D4"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="interactionStatistic" content="[{&quot;@type&quot;:&quot;InteractionCounter&quot;,&quot;interactionType&quot;:&quot;https://schema.org/LikeAction&quot;,&quot;userInteractionCount&quot;:224},{&quot;@type&quot;:&quot;InteractionCounter&quot;,&quot;interactionType&quot;:&quot;https://schema.org/CommentAction&quot;,&quot;userInteractionCount&quot;:15}]"/>
            
            
        

        

        <style>
          @layer legacy, tailwind, pencraftReset, pencraft;
        </style>

        
        <link rel="preload" as="style" href="https://substackcdn.com/bundle/theme/main.53c04d88310d2b674768.css" />
        
        
        
        <link rel="preload" as="font" href="https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M5knjsS_ul.woff2" crossorigin />
        

        
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/main.7e4083a2.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/948.e74a5e4b.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6576.36026474.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8268.73c36061.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6333.5ffa7bb6.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/2524.46bd0255.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/7230.aef34607.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8553.a8b44b20.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/7309.eac6820e.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/7426.f27dfd5c.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/833.56b2288a.css" />
            
        

        
        
        
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover" />
        <meta name="author" content="Dwarkesh Patel" />
        <meta property="og:url" content="https://www.dwarkesh.com/p/andrej-karpathy" />
        
        
        <link rel="canonical" href="https://www.dwarkesh.com/p/andrej-karpathy" />
        

        

        

        
            
                <link rel="shortcut icon" href="https://substackcdn.com/image/fetch/$s_!sJZo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Ffavicon.ico">
            
        
            
                <link rel="icon" type="image/png" sizes="16x16" href="https://substackcdn.com/image/fetch/$s_!klY3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Ffavicon-16x16.png">
            
        
            
                <link rel="icon" type="image/png" sizes="32x32" href="https://substackcdn.com/image/fetch/$s_!ouq6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Ffavicon-32x32.png">
            
        
            
                <link rel="icon" type="image/png" sizes="48x48" href="https://substackcdn.com/image/fetch/$s_!NKLv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Ffavicon-48x48.png">
            
        
            
                <link rel="apple-touch-icon" sizes="57x57" href="https://substackcdn.com/image/fetch/$s_!Dxm3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-57x57.png">
            
        
            
                <link rel="apple-touch-icon" sizes="60x60" href="https://substackcdn.com/image/fetch/$s_!w9st!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-60x60.png">
            
        
            
                <link rel="apple-touch-icon" sizes="72x72" href="https://substackcdn.com/image/fetch/$s_!3e0p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-72x72.png">
            
        
            
                <link rel="apple-touch-icon" sizes="76x76" href="https://substackcdn.com/image/fetch/$s_!Mgfe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-76x76.png">
            
        
            
                <link rel="apple-touch-icon" sizes="114x114" href="https://substackcdn.com/image/fetch/$s_!tz2S!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-114x114.png">
            
        
            
                <link rel="apple-touch-icon" sizes="120x120" href="https://substackcdn.com/image/fetch/$s_!-gEb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-120x120.png">
            
        
            
                <link rel="apple-touch-icon" sizes="144x144" href="https://substackcdn.com/image/fetch/$s_!yE4L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-144x144.png">
            
        
            
                <link rel="apple-touch-icon" sizes="152x152" href="https://substackcdn.com/image/fetch/$s_!sitx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-152x152.png">
            
        
            
                <link rel="apple-touch-icon" sizes="167x167" href="https://substackcdn.com/image/fetch/$s_!P_BK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-167x167.png">
            
        
            
                <link rel="apple-touch-icon" sizes="180x180" href="https://substackcdn.com/image/fetch/$s_!I4DV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-180x180.png">
            
        
            
                <link rel="apple-touch-icon" sizes="1024x1024" href="https://substackcdn.com/image/fetch/$s_!HtI2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ba3bcc4-643d-4197-98f2-e7439f9ae2d1%2Fapple-touch-icon-1024x1024.png">
            
        
            
        
            
        
            
        

        

        
            <link rel="alternate" type="application/rss+xml" href="/feed" title="Dwarkesh Podcast"/>
        

        
        
          <style>
            @font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8on7mTNmnUHowCw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onXmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onTmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onrmTNmnUHo.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M9knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M2knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M3knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M5knjsS_ul.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3FafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3OafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3PafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3BafaPWnII.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}
            @font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqJ2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqt2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqB2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqF2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoq92nPWc3ZyhTg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwf7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMw77I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwX7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwT7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwf7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMw77I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwX7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwT7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}
          </style>
        
        

        <style>:root{--color_theme_bg_pop:#f3c016;--background_pop:#f3c016;--color_theme_bg_web:#ffffff;--cover_bg_color:#ffffff;--background_pop_darken:#e4b20c;--print_on_pop:#ffffff;--color_theme_bg_pop_darken:#e4b20c;--color_theme_print_on_pop:#ffffff;--color_theme_bg_pop_20:rgba(243, 192, 22, 0.2);--color_theme_bg_pop_30:rgba(243, 192, 22, 0.3);--border_subtle:rgba(204, 204, 204, 0.5);--background_subtle:rgba(253, 246, 220, 0.4);--print_pop:#f3c016;--color_theme_accent:#f3c016;--cover_print_primary:#363737;--cover_print_secondary:#757575;--cover_print_tertiary:#b6b6b6;--cover_border_color:#f3c016;--font_family_headings_preset:Lora,sans-serif;--font_weight_headings_preset:600;--font_preset_heading:fancy_serif;--home_hero:podcast;--home_posts:custom;--home_show_top_posts:true;--web_bg_color:#ffffff;--background_contrast_1:#f0f0f0;--color_theme_bg_contrast_1:#f0f0f0;--background_contrast_2:#dddddd;--color_theme_bg_contrast_2:#dddddd;--background_contrast_3:#b7b7b7;--color_theme_bg_contrast_3:#b7b7b7;--background_contrast_4:#929292;--color_theme_bg_contrast_4:#929292;--background_contrast_5:#515151;--color_theme_bg_contrast_5:#515151;--color_theme_bg_elevated:#ffffff;--color_theme_bg_elevated_secondary:#f0f0f0;--color_theme_bg_elevated_tertiary:#dddddd;--color_theme_detail:#e6e6e6;--background_contrast_pop:rgba(243, 192, 22, 0.4);--color_theme_bg_contrast_pop:rgba(243, 192, 22, 0.4);--input_background:#ffffff;--cover_input_background:#ffffff;--tooltip_background:#191919;--web_bg_color_h:0;--web_bg_color_s:0%;--web_bg_color_l:100%;--print_on_web_bg_color:#363737;--print_secondary_on_web_bg_color:#868787;--selected_comment_background_color:#fdf9f3;--background_pop_rgb:243, 192, 22;--background_pop_rgb_pc:243 192 22;--color_theme_bg_pop_rgb:243, 192, 22;--color_theme_bg_pop_rgb_pc:243 192 22;--color_theme_accent_rgb:243, 192, 22;--color_theme_accent_rgb_pc:243 192 22;}</style>

        
            <link rel="stylesheet" href="https://substackcdn.com/bundle/theme/main.53c04d88310d2b674768.css" />
        

        <style></style>

        

        

        

        
    </head>

    <body class="">
        

        

        

        

        

        

        <div id="entry">
            <div id="main" class="main typography use-theme-bg"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div data-testid="navbar" class="main-menu"><div class="mainMenuContent-DME8DR"><div style="position:relative;height:71px;" class="pencraft pc-display-flex pc-gap-12 pc-paddingLeft-20 pc-paddingRight-20 pc-justifyContent-space-between pc-alignItems-center pc-reset border-bottom-detail-k1F6C4 topBar-pIF0J1"><div style="flex-basis:0px;flex-grow:1;" class="logoContainer-p12gJb"><a href="/" native class="pencraft pc-display-contents pc-reset"><div draggable="false" class="pencraft pc-display-flex pc-position-relative pc-reset"><div style="width:40px;height:40px;" class="pencraft pc-display-flex pc-reset bg-white-ZBV5av pc-borderRadius-sm overflow-hidden-WdpwT6 sizing-border-box-DggLA4"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QEPJ!,w_80,h_80,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png"/><img src="https://substackcdn.com/image/fetch/$s_!QEPJ!,w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png" sizes="100vw" alt="Dwarkesh Podcast" width="80" height="80" style="width:40px;height:40px;" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"/></picture></div></div></a></div><div style="flex-grow:0;" class="titleContainer-DJYq5v"><h1 class="pencraft pc-reset font-pub-headings-FE5byy reset-IxiVJZ title-oOnUGd"><a href="/" class="pencraft pc-display-contents pc-reset">Dwarkesh Podcast</a></h1></div><div style="flex-basis:0px;flex-grow:1;" class="pencraft pc-display-flex pc-justifyContent-flex-end pc-alignItems-center pc-reset"><div class="buttonsContainer-SJBuep"><div class="pencraft pc-display-flex pc-gap-8 pc-justifyContent-flex-end pc-alignItems-center pc-reset navbar-buttons"><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><span><button tabindex="0" type="button" aria-label="Search" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button></span><button tabindex="0" type="button" aria-label="Share Publication" id="headlessui-menu-button-P0-4" aria-haspopup="menu" aria-expanded="false" data-headlessui-state class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></button></div><button tabindex="0" type="button" data-testid="noncontributor-cta-button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o">Subscribe</button><button tabindex="0" type="button" native data-href="https://substack.com/sign-in?redirect=%2Fp%2Fandrej-karpathy&amp;for_pub=dwarkesh" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_tertiary-rlke8z size_md-gCDS3o">Sign in</button></div></div></div></div></div><div style="height:72px;"></div></div></div><div><script type="application/ld+json">{"@context":"https://schema.org","@type":"NewsArticle","url":"https://www.dwarkesh.com/p/andrej-karpathy","mainEntityOfPage":"https://www.dwarkesh.com/p/andrej-karpathy","headline":"Andrej Karpathy \u2014 AGI is still a decade away","description":"\"The problems are tractable, but they're still difficult\u201D","image":[{"@type":"ImageObject","url":"https://substack-video.s3.amazonaws.com/video_upload/post/176425744/248940bd-cafb-4de4-bf60-6b24fae0fa2e/transcoded-1760720000.png"}],"datePublished":"2025-10-17T16:54:33+00:00","dateModified":"2025-10-17T16:54:33+00:00","isAccessibleForFree":true,"author":[{"@type":"Person","name":"Dwarkesh Patel","url":"https://substack.com/@dwarkesh","description":"Host of Dwarkesh Podcast","identifier":"user:4281466","sameAs":["https://twitter.com/dwarkesh_sp"],"image":{"@type":"ImageObject","contentUrl":"https://substackcdn.com/image/fetch/$s_!5eJb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!5eJb!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg"}}],"publisher":{"@type":"Organization","name":"Dwarkesh Podcast","url":"https://www.dwarkesh.com","description":"Deeply researched interviews","interactionStatistic":{"@type":"InteractionCounter","name":"Subscribers","interactionType":"https://schema.org/SubscribeAction","userInteractionCount":10000},"identifier":"pub:69345","logo":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/$s_!QEPJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png","contentUrl":"https://substackcdn.com/image/fetch/$s_!QEPJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!QEPJ!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png"},"image":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/$s_!QEPJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png","contentUrl":"https://substackcdn.com/image/fetch/$s_!QEPJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!QEPJ!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png"},"sameAs":["https://twitter.com/dwarkesh_sp"]}}</script><div aria-label="Post" role="main" class="single-post-container"><div><div class="single-post"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><article class="typography podcast-post post tw-p-0 shows-post"><div class="visibility-check"></div><div><div class="container-dlhqPD"><div class="video-wrapper-lforaE"><div><div class="fullscreen"><div style="width:100%;height:100%;background-color:#000;" class="pencraft pc-display-flex pc-justifyContent-center pc-position-relative pc-reset overflow-hidden-WdpwT6"><div style="transform:scale(1);transform-origin:0 50%;transition:transform 0.15s ease-in-out;" class="pencraft pc-display-flex pc-position-relative pc-reset flex-grow-rzmknG"><div role="region" aria-label="Video player" class="with-preview full-width video-player-with-background video-player-wrapper full-width"><div style="padding-bottom:56.2500%;" class="video-player video-player video-player-with-background"><div class="pencraft pc-position-absolute pc-reset buttonContainer-tH3LP9 video-player-button"><button style="width:72px;height:72px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="20" height="20" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div></div></div></div><div style="left:0px;right:0px;bottom:0px;width:100%;transition:width 0.15s ease-in-out;" class="pencraft pc-display-flex pc-position-absolute pc-reset"><div style="z-index:0;"><div></div><div style="opacity:0;pointer-events:none;" class="settingsControlsContainer-V3A25d"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-display-flex pc-reset settingsControlsBox-svKH2p"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-paddingTop-8 pc-paddingBottom-8 pc-reset flex-grow-rzmknG seetingsMenu-zc8kI3"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-padding-8 pc-justifyContent-space-between pc-alignItems-center pc-reset menuItem-Z86vY3"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-gauge"><path d="m12 14 4-4"></path><path d="M3.34 19a10 10 0 1 1 17.32 0"></path></svg><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ">Playback speed</div></div><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ">×</div></div></div><div class="pencraft pc-display-flex pc-padding-8 pc-justifyContent-space-between pc-alignItems-center pc-reset menuItem-Z86vY3"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ">Share post</div></div><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ"></div></div></div><div class="pencraft pc-display-flex pc-padding-8 pc-justifyContent-space-between pc-alignItems-center pc-reset menuItem-Z86vY3"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ">Share post at current time</div></div><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-reset color-white-rGgpJs line-height-16-mdHjij font-text-qe4AeH size-12-mmZ61m weight-regular-mUq6Gb reset-IxiVJZ"></div></div></div></div></div></div><div style="height:60px;"></div></div></div><div style="transform:translateY(0);transition:transform 0.5s ease;transition-delay:0.1s;" class="bottomControlsContainer-kx5Iet"><div style="opacity:1;" class="backDrop-OMzeTr"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-paddingTop-4 pc-paddingBottom-4 pc-reset flex-grow-rzmknG"><div style="opacity:1;transition:opacity 0.5s ease;" class="bottomInnerControlsContainer-nA_qdP"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-paddingBottom-4 pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-position-relative pc-reset flex-grow-rzmknG cursor-pointer-LYORKw timelineContainer-QAunx2"><div class="timeline-nCVBQY"><div style="width:0%;height:100%;" class="progress-K0IenH"></div></div><div style="position:absolute;left:0%;opacity:0;" class="pencraft pc-display-flex pc-flexDirection-column pc-position-absolute pc-reset thumbnailContainer-vAgzNA"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-opacity-100 pc-padding-8 pc-reset bg-glass-inverted-medium-KqAZ5Z pc-borderRadius-sm"><div class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-reset"><div style="position:relative;"></div></div><div id="headlessui-menu-button-P0-6" aria-haspopup="menu" aria-expanded="false" data-headlessui-state class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-display-flex pc-gap-8 pc-paddingTop-4 pc-paddingBottom-4 pc-justifyContent-center pc-reset flex-grow-rzmknG cursor-pointer-LYORKw"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><div class="pencraft pc-reset color-white-rGgpJs line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV reset-IxiVJZ">Share from 0:00</div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-paddingLeft-4 pc-paddingRight-4 pc-reset"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="white" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-play"><polygon points="6 3 20 12 6 21 6 3"></polygon></svg></button></span><div style="position:relative;"><button tabindex="0" type="button" disabled class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="white" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-skip-forward"><polygon points="5 4 15 12 5 20 5 4"></polygon><line x1="19" x2="19" y1="5" y2="19"></line></svg></button></div><div class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset mainContainer-_32N5B"><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-volume"><path d="M11 4.702a.705.705 0 0 0-1.203-.498L6.413 7.587A1.4 1.4 0 0 1 5.416 8H3a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h2.416a1.4 1.4 0 0 1 .997.413l3.383 3.384A.705.705 0 0 0 11 19.298z"></path></svg></button></span><div class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-reset volumeBarContainer-AVLNVa"><div class="volumeBar-N1rUCF"><div style="width:0%;" class="volumeLevel-VDMLnw"></div></div></div></div><div class="pencraft pc-display-flex pc-gap-4 pc-justifyContent-center pc-alignItems-center pc-reset"><div class="pencraft pc-reset color-white-rGgpJs line-height-20-t4M0El font-digit-mBKKGP size-14-MLPa7j weight-bold-DmI9lw transform-uppercase-yKDgcq reset-IxiVJZ digit-FHltgX">0:00</div><div class="pencraft pc-reset color-white-rGgpJs line-height-20-t4M0El font-digit-mBKKGP size-14-MLPa7j weight-bold-DmI9lw transform-uppercase-yKDgcq reset-IxiVJZ digit-FHltgX">/</div><div class="pencraft pc-reset color-white-rGgpJs line-height-20-t4M0El font-digit-mBKKGP size-14-MLPa7j weight-bold-DmI9lw transform-uppercase-yKDgcq reset-IxiVJZ digit-FHltgX">0:00</div></div></div><div class="pencraft pc-display-flex pc-reset flex-grow-rzmknG"></div><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></button></span><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg role="img" width="20" height="16" viewBox="0 0 20 16" fill="none" stroke-width="1.8" stroke="#808080" xmlns="http://www.w3.org/2000/svg"><g><title></title><path fill-rule="evenodd" clip-rule="evenodd" d="M0 2.51429C0 1.12568 1.14532 0 2.55814 0H17.4419C18.8547 0 20 1.12568 20 2.51429V13.4857C20 14.8743 18.8547 16 17.4419 16H2.55814C1.14532 16 0 14.8743 0 13.4857V2.51429ZM2.55814 1.37143C1.91595 1.37143 1.39535 1.8831 1.39535 2.51429V13.4857C1.39535 14.1169 1.91595 14.6286 2.55814 14.6286H17.4419C18.0841 14.6286 18.6047 14.1169 18.6047 13.4857V2.51429C18.6047 1.8831 18.0841 1.37143 17.4419 1.37143H2.55814Z" fill="white" stroke-width="0.25"></path><path d="M12.1858 7.94143V8.6719C12.1858 9.03793 12.2336 9.34826 12.329 9.60289C12.4277 9.85752 12.5694 10.0501 12.754 10.1806C12.9418 10.3111 13.1661 10.3763 13.4271 10.3763C13.6563 10.3763 13.8536 10.3286 14.0192 10.2331C14.1878 10.1344 14.3183 10.0008 14.4106 9.83206C14.5029 9.66337 14.5555 9.4708 14.5682 9.25437H16.0626C16.0626 9.2862 16.0626 9.31802 16.0626 9.34985C16.0626 9.38168 16.0626 9.41192 16.0626 9.44057C16.053 9.88617 15.9321 10.2777 15.6997 10.615C15.4705 10.9492 15.1586 11.2102 14.7639 11.398C14.3693 11.5826 13.9205 11.6749 13.4176 11.6749C12.8097 11.6749 12.294 11.5524 11.8707 11.3073C11.4506 11.0622 11.1307 10.7153 10.9111 10.2665C10.6946 9.81774 10.5864 9.2846 10.5864 8.66713V7.94143C10.5864 7.32077 10.6962 6.78605 10.9159 6.33727C11.1355 5.8853 11.4553 5.53677 11.8755 5.29169C12.2988 5.04661 12.8128 4.92407 13.4176 4.92407C13.7964 4.92407 14.1449 4.97818 14.4632 5.0864C14.7814 5.19143 15.0584 5.34421 15.2939 5.54473C15.5294 5.74525 15.714 5.98556 15.8477 6.26565C15.9846 6.54256 16.0562 6.85289 16.0626 7.19664C16.0626 7.22847 16.0626 7.2603 16.0626 7.29213C16.0626 7.32077 16.0626 7.34942 16.0626 7.37807H14.5682C14.5555 7.15526 14.5014 6.95633 14.4059 6.78128C14.3136 6.60622 14.1847 6.46936 14.0192 6.37069C13.8536 6.26883 13.6563 6.21791 13.4271 6.21791C13.1661 6.21791 12.9433 6.28634 12.7587 6.4232C12.5741 6.55688 12.4325 6.75104 12.3338 7.00567C12.2352 7.2603 12.1858 7.57222 12.1858 7.94143Z" fill="white" stroke-width="0.25"></path><path d="M5.87381 7.94143V8.6719C5.87381 9.03793 5.92155 9.34826 6.01704 9.60289C6.1157 9.85752 6.25734 10.0501 6.44195 10.1806C6.62974 10.3111 6.85413 10.3763 7.11513 10.3763C7.34429 10.3763 7.54163 10.3286 7.70714 10.2331C7.87583 10.1344 8.00633 10.0008 8.09863 9.83206C8.19094 9.66337 8.24345 9.4708 8.25618 9.25437H9.75054C9.75054 9.2862 9.75054 9.31802 9.75054 9.34985C9.75054 9.38168 9.75054 9.41192 9.75054 9.44057C9.74099 9.88617 9.62004 10.2777 9.3877 10.615C9.15853 10.9492 8.84661 11.2102 8.45193 11.398C8.05726 11.5826 7.60847 11.6749 7.10558 11.6749C6.49765 11.6749 5.98202 11.5524 5.5587 11.3073C5.13856 11.0622 4.81868 10.7153 4.59907 10.2665C4.38263 9.81774 4.27441 9.2846 4.27441 8.66713V7.94143C4.27441 7.32077 4.38422 6.78605 4.60384 6.33727C4.82346 5.8853 5.14334 5.53677 5.56348 5.29169C5.9868 5.04661 6.50083 4.92407 7.10558 4.92407C7.48434 4.92407 7.83286 4.97818 8.15115 5.0864C8.46944 5.19143 8.74635 5.34421 8.98188 5.54473C9.21741 5.74525 9.40202 5.98556 9.5357 6.26565C9.67256 6.54256 9.74418 6.85289 9.75054 7.19664C9.75054 7.22847 9.75054 7.2603 9.75054 7.29213C9.75054 7.32077 9.75054 7.34942 9.75054 7.37807H8.25618C8.24345 7.15526 8.18934 6.95633 8.09386 6.78128C8.00156 6.60622 7.87265 6.46936 7.70714 6.37069C7.54163 6.26883 7.34429 6.21791 7.11513 6.21791C6.85413 6.21791 6.63133 6.28634 6.44672 6.4232C6.26212 6.55688 6.12048 6.75104 6.02181 7.00567C5.92314 7.2603 5.87381 7.57222 5.87381 7.94143Z" fill="white" stroke-width="0.25"></path></g></svg></button></span><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-picture-in-picture2 lucide-picture-in-picture-2"><path d="M21 9V6a2 2 0 0 0-2-2H4a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h4"></path><rect width="10" height="7" x="12" y="13" rx="2"></rect></svg></button></span><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize"><path d="M8 3H5a2 2 0 0 0-2 2v3"></path><path d="M21 8V5a2 2 0 0 0-2-2h-3"></path><path d="M3 16v3a2 2 0 0 0 2 2h3"></path><path d="M16 21h3a2 2 0 0 0 2-2v-3"></path></svg></button></span><span><button tabindex="0" type="button" class="pencraft pc-reset pencraft playerButton-qCsqQJ buttonBase-GK1x3M"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-ellipsis-vertical"><circle cx="12" cy="12" r="1"></circle><circle cx="12" cy="5" r="1"></circle><circle cx="12" cy="19" r="1"></circle></svg></button></span></div></div></div></div></div></div></div></div><div style="top:0px;bottom:0px;right:0px;transform:translateX(20%);transition:transform 0.15s ease-in-out, opacity 0.15s ease-in-out;width:360px;" class="pencraft pc-display-flex pc-opacity-0 pc-pointerEvents-none pc-position-absolute pc-reset overflow-auto-7WTsTi color-white-rGgpJs"><div class="pencraft pc-display-contents pc-reset dark-theme"><div style="background-color:#161718;" class="pencraft pc-display-flex pc-flexDirection-column pc-position-absolute pc-inset-0 pc-reset"><div class="pencraft pc-display-flex pc-padding-16 pc-justifyContent-space-between pc-alignItems-center pc-reset"><div class="pencraft pc-reset color-primary-zABazT line-height-24-jnGwiv font-text-qe4AeH size-17-JHHggF weight-semibold-uqA4FV reset-IxiVJZ">Transcript</div><button tabindex="0" type="button" aria-label="X" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button></div><div class="pencraft pc-display-flex pc-paddingLeft-16 pc-paddingRight-16 pc-paddingBottom-16 pc-reset"><div class="pencraft pc-display-flex pc-gap-8 pc-justifyContent-space-between pc-alignItems-center pc-reset searchForm-gEUJ79"><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-reset inputWrapper-Wvkzvn"><span><div class="pencraft pc-display-flex pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></div></span><input placeholder="Search" type="text" class="inputWithIcons-vs0rlV"/><span><div class="pencraft pc-display-flex pc-reset cursor-pointer-LYORKw"></div></span></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-absolute pc-inset-0 pc-reset"></div></div></div></div></div></div></div></div><div class="sidebar-NzGH2W sidebar-right-ktL8if"><div class="pencraft pc-display-flex pc-paddingLeft-8 pc-paddingRight-8 pc-paddingTop-16 pc-paddingBottom-16 pc-reset bg-glass-inverted-thick-BJiYjF sidebar-icons-YDR6ix"><div class="post-ufi style-large-on-dark themed vertically-stacked"><a role="button" title="upgrade in order to watch this video and be able to clip it" aria-label="upgrade in order to watch this video and be able to clip it" class="post-ufi-button style-large-on-dark state-disabled no-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-scissors icon"><circle cx="6" cy="6" r="3"></circle><path d="M8.12 8.12 12 12"></path><path d="M20 4 8.12 15.88"></path><circle cx="6" cy="18" r="3"></circle><path d="M14.8 14.8 20 20"></path></svg></a><div class="like-button-container post-ufi-button style-large-on-dark"><a role="button" aria-label="Like (224)" aria-pressed="false" class="post-ufi-button style-large-on-dark state-disabled has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">224</div></a></div><a role="button" href="https://www.dwarkesh.com/p/andrej-karpathy/comments" aria-label="View comments (15)" class="post-ufi-button style-large-on-dark post-ufi-comment-button has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">15</div></a><a role="button" class="post-ufi-button style-large-on-dark has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg><div class="label">51</div></a><a role="button" href="javascript:void(0)" class="post-ufi-button style-large-on-dark no-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share icon"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></a></div></div></div><div class="sidebar-NzGH2W sidebar-left-K3vrOP"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-padding-4 pc-pointerEvents-auto pc-reset bg-glass-inverted-medium-KqAZ5Z pc-borderRadius-full"><span><button tabindex="0" type="button" aria-label="Video" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-video"><path d="m16 13 5.223 3.482a.5.5 0 0 0 .777-.416V7.87a.5.5 0 0 0-.752-.432L16 10.5"></path><rect x="2" y="6" width="14" height="12" rx="2"></rect></svg></button></span><div class="pencraft pc-display-flex pc-position-relative pc-reset"><span><button tabindex="0" type="button" aria-label="Headphones" id="switcher-audio-button" class="pencraft pc-reset pencraft unselectedButton-LnSrSa media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-headphones"><path d="M3 14h3a2 2 0 0 1 2 2v3a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-7a9 9 0 0 1 18 0v7a2 2 0 0 1-2 2h-1a2 2 0 0 1-2-2v-3a2 2 0 0 1 2-2h3"></path></svg></button></span></div></div></div></div></div><div class="main-content-and-sidebar-fw1PHW"><div class="main-content-qKkUCg"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-reset"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><h2 dir="auto" class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-36-XIK16z font-pub-headings-FE5byy size-30-tZAWf_ weight-bold-DmI9lw reset-IxiVJZ title-X77sOw">Andrej Karpathy — AGI is still a decade away</h2><div dir="auto" class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-24-jnGwiv font-pub-headings-FE5byy size-17-JHHggF weight-regular-mUq6Gb reset-IxiVJZ subtitle-HEEcLo">&quot;The problems are tractable, but they're still difficult”</div></div><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset byline-wrapper byline-wrapper--swap-on-mobile-Cs2Jac"><div class="pencraft pc-display-flex pc-reset byline-faces--swap-on-mobile-ucRXf3"><div class="pencraft pc-display-flex pc-flexDirection-row pc-gap-8 pc-alignItems-center pc-justifyContent-flex-start pc-reset"><div style="--scale:36px;--offset:9px;--border-width:4.5px;" class="pencraft pc-display-flex pc-flexDirection-row pc-alignItems-center pc-justifyContent-flex-start pc-reset ltr-qDBmby"><a href="https://substack.com/@dwarkesh" aria-label="View Dwarkesh Patel's profile" class="pencraft pc-display-contents pc-reset"><div style="--scale:36px;" tabindex="0" class="pencraft pc-display-flex pc-width-36 pc-height-36 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB last-JfNEJ_"><div style="--scale:36px;" title="Dwarkesh Patel" class="pencraft pc-display-flex pc-width-36 pc-height-36 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5eJb!,w_36,h_36,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 36w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_72,h_72,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 72w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_108,h_108,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 108w" sizes="36px"/><img src="https://substackcdn.com/image/fetch/$s_!5eJb!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg" sizes="36px" alt="Dwarkesh Patel's avatar" srcset="https://substackcdn.com/image/fetch/$s_!5eJb!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 36w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_72,h_72,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 72w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_108,h_108,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 108w" width="36" height="36" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"/></picture></div></div></a></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/@dwarkesh" class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ">Dwarkesh Patel</a></div></div><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Oct 17, 2025</div></div></div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG border-top-detail-themed-k9TZAY border-bottom-detail-themed-Ua9186 post-ufi"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="like-button-container post-ufi-button style-button"><a role="button" aria-label="Like (224)" aria-pressed="false" class="post-ufi-button style-button has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">224</div></a></div><a role="button" href="https://www.dwarkesh.com/p/andrej-karpathy/comments" aria-label="View comments (15)" class="post-ufi-button style-button post-ufi-comment-button has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">15</div></a><a role="button" class="post-ufi-button style-button has-label with-border"><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg><div class="label">51</div></a></div><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><a role="button" href="javascript:void(0)" class="post-ufi-button style-button no-icon has-label with-border"><div class="label">Share</div></a><a role="button" class="post-ufi-button style-button has-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-align-left"><path d="M15 12H3"></path><path d="M17 18H3"></path><path d="M21 6H3"></path></svg><div class="label">Transcript</div></a></div></div></div></div><div class="postContentWrapper-MYe7fH"><div class="available-content"><div dir="auto" class="body markup"><p><span>The </span><a href="https://x.com/karpathy" rel>Andrej Karpathy</a><span> episode.</span></p><p>Andrej explains why reinforcement learning is terrible (but everything else is much worse), why model collapse prevents LLMs from learning the way humans do, why AGI will just blend into the previous ~2.5 centuries of 2% GDP growth, why self driving took so long to crack, and what he sees as the future of education.</p><p><span>Watch on </span><a href="https://youtu.be/lXUZvyajciY" rel>YouTube</a><span>; listen on </span><a href="https://podcasts.apple.com/us/podcast/andrej-karpathy-agi-is-still-a-decade-away/id1516093381?i=1000732326311" rel>Apple Podcasts</a><span> or </span><a href="https://open.spotify.com/episode/3iIYVmmhXwh3fOumypWVpC?si=33d37708b2b44e2f" rel>Spotify</a><span>.</span></p><div id="youtube2-lXUZvyajciY" data-attrs="{&quot;videoId&quot;:&quot;lXUZvyajciY&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM" class="youtube-wrap"><div class="youtube-inner"><iframe src="https://www.youtube-nocookie.com/embed/lXUZvyajciY?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></div></div><h3 class="header-anchor-post">Sponsors<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§sponsors" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/sponsors" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><ul><li><p><a href="https://labelbox.com/dwarkesh" rel>Labelbox</a><span> helps you get data that is more detailed, more accurate, and higher signal than you could get by default, no matter your domain or training paradigm. Reach out today at </span><a href="https://labelbox.com/dwarkesh" rel>labelbox.com/dwarkesh</a></p></li><li><p><a href="https://mercury.com" rel>Mercury</a><span> helps you run your business better. It’s the banking platform we use for the podcast — we love that we can see our accounts, cash flows, AR, and AP all in one place. Apply online in minutes at </span><a href="https://mercury.com" rel>mercury.com</a></p></li><li><p><a href="https://blog.google/technology/ai/veo-updates-flow/" rel>Google’s Veo 3.1</a><span> update is a notable improvement to an already great model. Veo 3.1’s generations are more coherent and the audio is even higher-quality. If you have a Google AI Pro or Ultra plan, you can try it in Gemini today by visiting </span><a href="https://gemini.google" rel>https://gemini.google</a></p></li></ul><h3 class="header-anchor-post">Timestamps<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§timestamps" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/timestamps" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><a href="https://www.dwarkesh.com/i/176425744/agi-is-still-a-decade-away" rel>(00:00:00) – AGI is still a decade away</a></p><p><a href="https://www.dwarkesh.com/i/176425744/llm-cognitive-deficits" rel>(00:29:45) – LLM cognitive deficits</a></p><p><a href="https://www.dwarkesh.com/i/176425744/rl-is-terrible" rel>(00:40:05) – RL is terrible</a></p><p><a href="https://www.dwarkesh.com/i/176425744/how-do-humans-learn" rel>(00:49:38) – How do humans learn?</a></p><p><a href="https://www.dwarkesh.com/i/176425744/agi-will-blend-into-gdp-growth" rel>(01:06:25) – AGI will blend into 2% GDP growth</a></p><p><a href="https://www.dwarkesh.com/i/176425744/asi" rel>(01:17:36) – ASI</a></p><p><a href="https://www.dwarkesh.com/i/176425744/evolution-of-intelligence-and-culture" rel>(01:32:50) – Evolution of intelligence &amp; culture</a></p><p><a href="https://www.dwarkesh.com/i/176425744/why-self-driving-took-so-long" rel>(01:42:55) - Why self driving took so long</a></p><p><a href="https://www.dwarkesh.com/i/176425744/future-of-education" rel>(01:56:20) - Future of education</a></p><h3 class="header-anchor-post">Transcript<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§transcript" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/transcript" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><h3 class="header-anchor-post">00:00:00 – AGI is still a decade away<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§agi-is-still-a-decade-away" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/agi-is-still-a-decade-away" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>00:00:00</em></p><p><span>Today I’m speaking with </span><a href="https://karpathy.ai/" rel>Andrej Karpathy</a><span>. Andrej, why do you say that this will be the decade of </span><a href="https://en.wikipedia.org/wiki/Agentic_AI" rel>agents</a><span> and not the year of agents?</span></p><p><strong>Andrej Karpathy </strong><em>00:00:07</em></p><p>First of all, thank you for having me here. I’m excited to be here.</p><p><span>The quote you’ve just mentioned, </span><a href="https://x.com/karpathy/status/1882544526033924438?lang=en" rel>“It’s the decade of agents,”</a><span> is actually a reaction to a pre-existing quote. I’m not actually sure who said this but they were alluding to this being the year of agents with respect to </span><a href="https://en.wikipedia.org/wiki/Large_language_model" rel>LLMs</a><span> and how they were going to evolve. I was triggered by that because there’s some over-prediction going on in the industry. In my mind, this is more accurately described as the decade of agents.</span></p><p><span>We have some very early agents that are extremely impressive and that I use daily—Claude and </span><a href="https://en.wikipedia.org/wiki/OpenAI_Codex" rel>Codex</a><span> and so on—but I still feel there’s so much work to be done. My reaction is we’ll be working with these things for a decade. They’re going to get better, and it’s going to be wonderful. I was just reacting to the timelines of the implication.</span></p><p><strong>Dwarkesh Patel </strong><em>00:00:58</em></p><p>What do you think will take a decade to accomplish? What are the bottlenecks?</p><p><strong>Andrej Karpathy </strong><em>00:01:02</em></p><p>Actually making it work. When you’re talking about an agent, or what the labs have in mind and maybe what I have in mind as well, you should think of it almost like an employee or an intern that you would hire to work with you. For example, you work with some employees here. When would you prefer to have an agent like Claude or Codex do that work?</p><p><span>Currently, of course they can’t. What would it take for them to be able to do that? Why don’t you do it today? The reason you don’t do it today is because they just don’t work. They don’t have enough intelligence, they’re not </span><a href="https://en.wikipedia.org/wiki/Multimodal_learning" rel>multimodal</a><span> enough, they can’t do </span><a href="https://a16z.com/the-rise-of-computer-use-and-agentic-coworkers/" rel>computer use</a><span> and all this stuff.</span></p><p><span>They don’t do a lot of the things you’ve alluded to earlier. They don’t have </span><a href="https://en.wikipedia.org/wiki/Incremental_learning" rel>continual learning</a><span>. You can’t just tell them something and they’ll remember it. They’re cognitively lacking and it’s just not working. It will take about a decade to work through all of those issues.</span></p><p><strong>Dwarkesh Patel </strong><em>00:01:44</em></p><p>Interesting. As a professional podcaster and a viewer of AI from afar, it’s easy for me to identify what’s lacking: continual learning is lacking, or multimodality is lacking. But I don’t really have a good way of trying to put a timeline on it. If somebody asks how long continual learning will take, I have no prior about whether this is a project that should take 5 years, 10 years, or 50 years. Why a decade? Why not one year? Why not 50 years?</p><p><strong>Andrej Karpathy </strong><em>00:02:16</em></p><p><span>This is where you get into a bit of my own intuition, and doing a bit of an extrapolation with respect to my own experience in the field. I’ve been in AI for almost two decades. It’s going to be 15 years or so, not that long. You had </span><a href="https://www.dwarkesh.com/p/richard-sutton" rel>Richard Sutton</a><span> here, who was around for much longer. I do have about 15 years of experience of people making predictions, of seeing how they turned out. Also I was in the industry for a while, I was in research, and I’ve worked in the industry for a while. I have a general intuition that I have left from that.</span></p><p>I feel like the problems are tractable, they’re surmountable, but they’re still difficult. If I just average it out, it just feels like a decade to me.</p><p><strong>Dwarkesh Patel </strong><em>00:02:57</em></p><p>This is quite interesting. I want to hear not only the history, but what people in the room felt was about to happen at various different breakthrough moments. What were the ways in which their feelings were either overly pessimistic or overly optimistic? Should we just go through each of them one by one?</p><p><strong>Andrej Karpathy </strong><em>00:03:16</em></p><p>That’s a giant question because you’re talking about 15 years of stuff that happened. AI is so wonderful because there have been a number of seismic shifts where the entire field has suddenly looked a different way. I’ve maybe lived through two or three of those. I still think there will continue to be some because they come with almost surprising regularity.</p><p><span>When my career began, when I started to work on </span><a href="https://en.wikipedia.org/wiki/Deep_learning" rel>deep learning</a><span>, when I became interested in deep learning, this was by chance of being right next to </span><a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel>Geoff Hinton</a><span> at the University of Toronto. Geoff Hinton, of course, is the godfather figure of AI. He was training all these </span><a href="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)" rel>neural networks</a><span>. I thought it was incredible and interesting. This was not the main thing that everyone in AI was doing by far. This was a niche little subject on the side. That’s maybe the first dramatic seismic shift that came with the AlexNet and so on.</span></p><p>AlexNet reoriented everyone, and everyone started to train neural networks, but it was still very per-task, per specific task. Maybe I have an image classifier or I have a neural machine translator or something like that. People became very slowly interested in agents. People started to think, “Okay, maybe we have a check mark next to the visual cortex or something like that, but what about the other parts of the brain, and how can we get a full agent or a full entity that can interact in the world?”</p><p><span>The </span><a href="https://arxiv.org/abs/1312.5602" rel>Atari deep reinforcement learning shift in 2013</a><span> or so was part of that early effort of agents, in my mind, because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. At the time, this was Atari games.</span></p><p><span>I feel that was a misstep. It was a misstep that even the early </span><a href="https://en.wikipedia.org/wiki/OpenAI" rel>OpenAI</a><span> that I was a part of adopted because at that time, the zeitgeist was </span><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel>reinforcement learning</a><span> environments, games, game playing, beat games, get lots of different types of games, and OpenAI was doing a lot of that. That was another prominent part of AI where maybe for two or three or four years, everyone was doing reinforcement learning on games. That was all a bit of a misstep.</span></p><p><span>What I was trying to do at OpenAI is I was always a bit suspicious of games as being this thing that would lead to </span><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" rel>AGI</a><span>. Because in my mind, you want something like an accountant or something that’s interacting with the real world. I just didn’t see how games add up to it. My project at OpenAI, for example, was within the scope of the </span><a href="https://openai.com/index/universe/" rel>Universe project</a><span>, on an agent that was using keyboard and mouse to operate web pages. I really wanted to have something that interacts with the actual digital world that can do knowledge work.</span></p><p>It just so turns out that this was extremely early, way too early, so early that we shouldn’t have been working on that. Because if you’re just stumbling your way around and keyboard mashing and mouse clicking and trying to get rewards in these environments, your reward is too sparse and you just won’t learn. You’re going to burn a forest computing, and you’re never going to get something off the ground. What you’re missing is this power of representation in the neural network.</p><p><span>For example, today people are training those computer-using agents, but they’re doing it on top of a large language model. You have to get the language model first, you have to get the representations first, and you have to do that by all the </span><a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer" rel>pre-training</a><span> and all the LLM stuff.</span></p><p>I feel maybe loosely speaking, people kept trying to get the full thing too early a few times, where people really try to go after agents too early, I would say. That was Atari and Universe and even my own experience. You actually have to do some things first before you get to those agents. Now the agents are a lot more competent, but maybe we’re still missing some parts of that stack.</p><p>I would say those are the three major buckets of what people were doing: training neural nets per-tasks, trying the first round of agents, and then maybe the LLMs and seeking the representation power of the neural networks before you tack on everything else on top.</p><p><strong>Dwarkesh Patel </strong><em>00:07:02</em></p><p><span>Interesting. If I were to </span><a href="https://en.wikipedia.org/wiki/Straw_man#Steelmanning" rel>steelman</a><span> the </span><a href="https://www.dwarkesh.com/p/richard-sutton" rel>Sutton perspective</a><span>, it would be that humans can just take on everything at once, or even animals can take on everything at once. Animals are maybe a better example because they don’t even have the scaffold of language. They just get thrown out into the world, and they just have to make sense of everything without any labels.</span></p><p>The vision for AGI then should just be something which looks at sensory data, looks at the computer screen, and it just figures out what’s going on from scratch. If a human were put in a similar situation and had to be trained from scratch… This is like a human growing up or an animal growing up. Why shouldn’t that be the vision for AI, rather than this thing where we’re doing millions of years of training?</p><p><strong>Andrej Karpathy </strong><em>00:07:41</em></p><p><span>That’s a really good question. Sutton was on your podcast and I saw the podcast and </span><a href="https://x.com/karpathy/status/1973435013875314729" rel>I had a write-up about that podcast</a><span> that gets into a bit of how I see things. I’m very careful to make analogies to animals because they came about by a very different optimization process. Animals are evolved, and they come with a huge amount of hardware that’s built in.</span></p><p><span>For example, my example in the post was the zebra. A zebra gets born, and a few minutes later it’s running around and following its mother. That’s an extremely complicated thing to do. That’s not reinforcement learning. That’s something that’s baked in. Evolution obviously has some way of encoding the </span><a href="https://deepai.org/machine-learning-glossary-and-terms/weight-artificial-neural-network" rel>weights</a><span> of our neural nets in </span><a href="https://en.wikipedia.org/wiki/Nucleotide_base" rel>ATCGs</a><span>, and I have no idea how that works, but it apparently works.</span></p><p>Brains just came from a very different process, and I’m very hesitant to take inspiration from it because we’re not actually running that process. In my post, I said we’re not building animals. We’re building ghosts or spirits or whatever people want to call it, because we’re not doing training by evolution. We’re doing training by imitation of humans and the data that they’ve put on the Internet.</p><p>You end up with these ethereal spirit entities because they’re fully digital and they’re mimicking humans. It’s a different kind of intelligence. If you imagine a space of intelligences, we’re starting off at a different point almost. We’re not really building animals. But it’s also possible to make them a bit more animal-like over time, and I think we should be doing that.</p><p>One more point. I do feel Sutton has a very... His framework is, “We want to build animals.” I think that would be wonderful if we can get that to work. That would be amazing. If there were a single algorithm that you can just run on the Internet and it learns everything, that would be incredible. I’m not sure that it exists and that’s certainly not what animals do, because animals have this outer loop of evolution.</p><p>A lot of what looks like learning is more like maturation of the brain. I think there’s very little reinforcement learning for animals. A lot of the reinforcement learning is more like motor tasks; it’s not intelligence tasks. So I actually kind of think humans don’t really use RL, roughly speaking.</p><p><strong>Dwarkesh Patel </strong><em>00:09:52</em></p><p>Can you repeat the last sentence? A lot of that intelligence is not motor task…it’s what, sorry?</p><p><strong>Andrej Karpathy </strong><em>00:09:54</em></p><p><span>A lot of the reinforcement learning, in my perspective, would be things that are a lot more motor-like, simple tasks like throwing a hoop. But I don’t think that humans use reinforcement learning for a lot of intelligence tasks like problem-solving and so on.</span><strong> </strong><span>That doesn’t mean we shouldn’t do that for research, but I just feel like that’s what animals do or don’t.</span></p><p><strong>Dwarkesh Patel </strong><em>00:10:17</em></p><p>I’m going to take a second to digest that because there are a lot of different ideas. Here’s one clarifying question I can ask to understand the perspective. You suggest that evolution is doing the kind of thing that pre-training does in the sense of building something which can then understand the world.</p><p>The difference is that evolution has to be titrated in the case of humans through three gigabytes of DNA. That’s very unlike the weights of a model. Literally, the weights of the model are a brain, which obviously does not exist in the sperm and the egg. So it has to be grown. Also, the information for every single synapse in the brain simply cannot exist in the three gigabytes that exist in the DNA.</p><p>Evolution seems closer to finding the algorithm which then does the lifetime learning. Now, maybe the lifetime learning is not analogous to RL, to your point. Is that compatible with the thing you were saying, or would you disagree with that?</p><p><strong>Andrej Karpathy </strong><em>00:11:17</em></p><p>I think so. I would agree with you that there’s some miraculous compression going on because obviously, the weights of the neural net are not stored in ATCGs. There’s some dramatic compression. There are some learning algorithms encoded that take over and do some of the learning online. I definitely agree with you on that. I would say I’m a lot more practically minded. I don’t come at it from the perspective of, let’s build animals. I come from it from the perspective of, let’s build useful things. I have a hard hat on, and I’m just observing that we’re not going to do evolution, because I don’t know how to do that.</p><p>But it does turn out we can build these ghosts, spirit-like entities, by imitating internet documents. This works. It’s a way to bring you up to something that has a lot of built-in knowledge and intelligence in some way, similar to maybe what evolution has done. That’s why I call pre-training this crappy evolution. It’s the practically possible version with our technology and what we have available to us to get to a starting point where we can do things like reinforcement learning and so on.</p><p><strong>Dwarkesh Patel </strong><em>00:12:15</em></p><p>Just to steelman the other perspective, after doing this Sutton interview and thinking about it a bit, he has an important point here. Evolution does not give us the knowledge, really. It gives us the algorithm to find the knowledge, and that seems different from pre-training.</p><p>Perhaps the perspective is that pre-training helps build the kind of entity which can learn better. It teaches meta-learning, and therefore it is similar to finding an algorithm. But if it’s “Evolution gives us knowledge, pre-training gives us knowledge,” that analogy seems to break down.</p><p><strong>Andrej Karpathy </strong><em>00:12:42</em></p><p><span>It’s subtle and I think you’re right to push back on it, but basically the thing that pre-training is doing, you’re getting the </span><a href="https://research.google/pubs/mechanics-of-next-token-prediction-with-transformers/" rel>next-token predictor</a><span> over the internet, and you’re training that into a neural net. It’s doing two things that are unrelated. Number one, it’s picking up all this knowledge, as I call it. Number two, it’s actually becoming intelligent.</span></p><p><span>By observing the algorithmic patterns in the internet, it boots up all these little circuits and algorithms inside the neural net to do things like </span><a href="https://www.hopsworks.ai/dictionary/in-context-learning-icl#:~:text=In%2Dcontext%20learning%20(ICL)%20learns%20a%20new%20task%20from,objective%20of%20next%20token%20prediction." rel>in-context learning</a><span> and all this stuff. You don’t need or want the knowledge. I think that’s probably holding back the neural networks overall because it’s getting them to rely on the knowledge a little too much sometimes.</span></p><p><span>For example, I feel agents, one thing they’re not very good at, is going off the data manifold of what exists on the internet. If they had less knowledge or less memory, maybe they would be better. What I think we have to do going forward—and this would be part of the research paradigms—is figure out ways to remove some of the knowledge and to keep what I call this </span><a href="https://x.com/karpathy/status/1938626382248149433" rel>cognitive core</a><span>. It’s this intelligent entity that is stripped from knowledge but contains the algorithms and contains the magic of intelligence and problem-solving and the strategies of it and all this stuff.</span></p><p><strong>Dwarkesh Patel </strong><em>00:13:50</em></p><p>There’s so much interesting stuff there. Let’s start with in-context learning. This is an obvious point, but I think it’s worth just saying it explicitly and meditating on it. The situation in which these models seem the most intelligent—in which I talk to them and I’m like, “Wow, there’s really something on the other end that’s responding to me thinking about things—is if it makes a mistake it’s like, “Oh wait, that’s the wrong way to think about it. I’m backing up.” All that is happening in context. That’s where I feel like the real intelligence is that you can visibly see.</p><p><span>That in-context learning process is developed by </span><a href="https://en.wikipedia.org/wiki/Gradient_descent" rel>gradient descent</a><span> on pre-training. It spontaneously meta-learns in-context learning, but the in-context learning itself is not gradient descent, in the same way that our lifetime intelligence as humans to be able to do things is conditioned by evolution but our learning during our lifetime is happening through some other process.</span></p><p><strong>Andrej Karpathy </strong><em>00:14:42</em></p><p>I don’t fully agree with that, but you should continue your thought.</p><p><strong>Dwarkesh Patel </strong><em>00:14:44</em></p><p>Well, I’m very curious to understand how that analogy breaks down.</p><p><strong>Andrej Karpathy </strong><em>00:14:48</em></p><p><span>I’m hesitant to say that in-context learning is not doing gradient descent. It’s not doing explicit gradient descent. In-context learning is pattern completion within a </span><a href="https://blogs.nvidia.com/blog/ai-tokens-explained/" rel>token</a><span> window. It just turns out that there’s a huge amount of patterns on the internet. You’re right, the model learns to complete the pattern, and that’s inside the weights. The weights of the neural network are trying to discover patterns and complete the pattern. There’s some adaptation that happens inside the neural network, which is magical and just falls out from the internet just because there’s a lot of patterns.</span></p><p><span>I will say that there have been some papers that I thought were interesting that look at the mechanisms behind in-context learning. I do think it’s possible that in-context learning runs a small gradient descent loop internally in the layers of the neural network. </span><a href="https://arxiv.org/abs/2211.15661" rel>I recall one paper in particular</a><span> where they were doing linear regression using in-context learning. Your inputs into the neural network are XY pairs, XY, XY, XY that happen to be on the line. Then you do X and you expect Y. The neural network, when you train it in this way, does </span><a href="https://en.wikipedia.org/wiki/Linear_regression" rel>linear regression</a><span>.</span></p><p><span>Normally when you would run linear regression, you have a small gradient descent </span><a href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel>optimizer</a><span> that looks at XY, looks at an error, calculates the gradient of the weights and does the update a few times. It just turns out that when they looked at the weights of that in-context learning algorithm, they found some analogies to gradient descent mechanics. In fact, I think the paper was even stronger because they hardcoded the weights of a neural network to do gradient descent through attention and all the internals of the neural network.</span></p><p>That’s just my only pushback. Who knows how in-context learning works, but I think that it’s probably doing a bit of some funky gradient descent internally. I think that that’s possible. I was only pushing back on your saying that it’s not doing in-context learning. Who knows what it’s doing, but it’s probably maybe doing something similar to it, but we don’t know.</p><p><strong>Dwarkesh Patel </strong><em>00:16:39</em></p><p>So then it’s worth thinking okay, if in-context learning and pre-training are both implementing something like gradient descent, why does it feel like with in-context learning we’re getting to this continual learning, real intelligence-like thing? Whereas you don’t get the analogous feeling just from pre-training. You could argue that.</p><p><span>If it’s the same algorithm, what could be different? One way you could think about it is, how much information does the model store per information it receives from training? If you look at pre-training, if you look at </span><a href="https://www.llama.com/models/llama-3/" rel>Llama 3</a><span> for example, I think it’s trained on 15 trillion tokens. If you look at the 70B model, that would be the equivalent of 0.07 bits per token that it sees in pre-training, in terms of the information in the weights of the model compared to the tokens it reads. Whereas if you look at the </span><a href="https://huggingface.co/blog/not-lain/kv-caching" rel>KV cache</a><span> and how it grows per additional token in in-context learning, it’s like 320 kilobytes. So that’s a 35 million-fold difference in how much information per token is assimilated by the model. I wonder if that’s relevant at all.</span></p><p><strong>Andrej Karpathy </strong><em>00:17:46</em></p><p>I kind of agree. The way I usually put this is that anything that happens during the training of the neural network, the knowledge is only a hazy recollection of what happened in training time. That’s because the compression is dramatic. You’re taking 15 trillion tokens and you’re compressing it to just your final neural network of a few billion parameters. Obviously it’s a massive amount of compression going on. So I refer to it as a hazy recollection of the internet documents.</p><p>Whereas anything that happens in the context window of the neural network—you’re plugging in all the tokens and building up all those KV cache representations—is very directly accessible to the neural net. So I compare the KV cache and the stuff that happens at test time to more like a working memory. All the stuff that’s in the context window is very directly accessible to the neural net.</p><p>There’s always these almost surprising analogies between LLMs and humans. I find them surprising because we’re not trying to build a human brain directly. We’re just finding that this works and we’re doing it. But I do think that anything that’s in the weights, it’s a hazy recollection of what you read a year ago. Anything that you give it as a context at test time is directly in the working memory. That’s a very powerful analogy to think through things.</p><p><span>When you, for example, go to an LLM and you ask it about some book and what happened in it, like </span><a href="https://amzn.to/3KROqqU" rel>Nick Lane’s book</a><span> or something like that, the LLM will often give you some stuff which is roughly correct. But if you give it the full chapter and ask it questions, you’re going to get much better results because it’s now loaded in the working memory of the model. So a very long way of saying I agree and that’s why.</span></p><p><strong>Dwarkesh Patel </strong><em>00:19:11</em></p><p>Stepping back, what is the part about human intelligence that we have most failed to replicate with these models?</p><p><strong>Andrej Karpathy </strong><em>00:19:20</em></p><p><span>Just a lot of it. So maybe one way to think about it, I don’t know if this is the best way, but I almost feel like — again, making these analogies imperfect as they are — we’ve stumbled by with the </span><a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" rel>transformer</a><span> neural network, which is extremely powerful, very general. You can train transformers on audio, or video, or text, or whatever you want, and it just learns patterns and they’re very powerful, and it works really well. That to me almost indicates that this is some piece of </span><a href="https://en.wikipedia.org/wiki/Cerebral_cortex" rel>cortical tissue</a><span>. It’s something like that, because the cortex is famously very plastic as well. You can rewire parts of brains. </span><a href="https://news.mit.edu/2000/brain" rel>There were the slightly gruesome experiments</a><span> with rewiring the visual cortex to the auditory cortex, and this animal learned fine, et cetera.</span></p><p><span>So I think that this is cortical tissue. I think when we’re doing reasoning and planning inside the neural networks, doing reasoning traces for thinking models, that’s kind of like the </span><a href="https://en.wikipedia.org/wiki/Prefrontal_cortex" rel>prefrontal cortex</a><span>. Maybe those are like little checkmarks, but I still think there are many brain parts and nuclei that are not explored. For example, there’s a </span><a href="https://en.wikipedia.org/wiki/Basal_ganglia" rel>basal ganglia</a><span> doing a bit of reinforcement learning when we </span><a href="https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)" rel>fine-tune</a><span> the models on reinforcement learning. But where’s the </span><a href="https://en.wikipedia.org/wiki/Hippocampus" rel>hippocampus</a><span>? Not obvious what that would be. Some parts are probably not important. Maybe the </span><a href="https://en.wikipedia.org/wiki/Cerebellum" rel>cerebellum</a><span> is not important to cognition, its thoughts, so maybe we can skip some of it. But I still think there’s, for example, the </span><a href="https://en.wikipedia.org/wiki/Amygdala" rel>amygdala</a><span>, all the emotions and instincts. There’s probably a bunch of other nuclei in the brain that are very ancient that I don’t think we’ve really replicated.</span></p><p>I don’t know that we should be pursuing the building of an analog of a human brain. I’m an engineer mostly at heart. Maybe another way to answer the question is that you’re not going to hire this thing as an intern. It’s missing a lot of it because it comes with a lot of these cognitive deficits that we all intuitively feel when we talk to the models. So it’s not fully there yet. You can look at it as not all the brain parts are checked off yet.</p><p><strong>Dwarkesh Patel </strong><em>00:21:16</em></p><p>This is maybe relevant to the question of thinking about how fast these issues will be solved. Sometimes people will say about continual learning, “Look, you could easily replicate this capability. Just as in-context learning emerged spontaneously as a result of pre-training, continual learning over longer horizons will emerge spontaneously if the model is incentivized to recollect information over longer horizons, or horizons longer than one session.” So if there’s some outer loop RL which has many sessions within that outer loop, then this continual learning where it fine-tunes itself, or it writes to an external memory or something, will just emerge spontaneously. Do you think things like that are plausible? I just don’t have a prior over how plausible that is. How likely is that to happen?</p><p><strong>Andrej Karpathy </strong><em>00:22:07</em></p><p>I don’t know that I fully resonate with that. These models, when you boot them up and they have zero tokens in the window, they’re always restarting from scratch where they were. So I don’t know in that worldview what it looks like. Maybe making some analogies to humans—just because I think it’s roughly concrete and interesting to think through—I feel like when I’m awake, I’m building up a context window of stuff that’s happening during the day. But when I go to sleep, something magical happens where I don’t think that context window stays around. There’s some process of distillation into the weights of my brain. This happens during sleep and all this stuff.</p><p><span>We don’t have an equivalent of that in large language models. That’s to me more adjacent to when you talk about continual learning and so on as absent. These models don’t really have a distillation phase of taking what happened, analyzing it obsessively, thinking through it, doing some synthetic data generation process and distilling it back into the weights, and maybe having a specific neural net per person. Maybe it’s a </span><a href="https://www.ibm.com/think/topics/lora" rel>LoRA</a><span>. It’s not a full-weight neural network. It’s just some small sparse subset of the weights that are changed.</span></p><p><span>But we do want to create ways of creating these individuals that have very long context. It’s not only remaining in the context window because the context windows grow very, very long. Maybe we have some very elaborate, sparse attention over it. But I still think that humans obviously have some process for distilling some of that knowledge into the weights. We’re missing it. I do also think that humans have some very elaborate, </span><a href="https://medium.com/@vishal09vns/sparse-attention-dad17691478c" rel>sparse attention scheme</a><span>, which I think we’re starting to see some early hints of. </span><a href="https://api-docs.deepseek.com/news/news250929" rel>DeepSeek v3.2</a><span> just came out and I saw that they have sparse attention as an example, and this is one way to have very, very long context windows. So I feel like we are redoing a lot of the cognitive tricks that evolution came up with through a very different process. But we’re going to converge on a similar architecture cognitively.</span></p><p><strong>Dwarkesh Patel </strong><em>00:24:02</em></p><p><span>In 10 years, do you think it’ll still be something like a transformer, but with much more modified attention and more sparse </span><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel>MLPs</a><span> and so forth?</span></p><p><strong>Andrej Karpathy </strong><em>00:24:10</em></p><p><span>The way I like to think about it is </span><a href="https://en.wikipedia.org/wiki/Translational_symmetry" rel>translation invariance</a><span> in time. So 10 years ago, where were we? 2015. In 2015, we had </span><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel>convolutional neural networks</a><span> primarily, </span><a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel>residual networks</a><span> just came out. So remarkably similar, I guess, but quite a bit different still. The transformer was not around. All these more modern tweaks on the transformer were not around. Maybe some of the things that we can bet on, I think in 10 years by translational equivariance, is that we’re still training giant neural networks with a </span><a href="https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc/" rel>forward backward pass</a><span> and update through gradient descent, but maybe it looks a bit different, and it’s just that everything is much bigger.</span></p><p><span>Recently I went back all the way to 1989 which was a fun exercise for me, a few years ago, because I was reproducing </span><a href="https://en.wikipedia.org/wiki/Yann_LeCun" rel>Yann LeCun’s</a><span> </span><a href="https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf" rel>1989 convolutional network</a><span>, which was </span><a href="https://en.wikipedia.org/wiki/LeNet" rel>the first neural network</a><span> I’m aware of trained via gradient descent, like modern neural network trained gradient descent on digit recognition. I was just interested in how I could modernize this. How much of this is algorithms? How much of this is data? How much of this progress is compute and systems? I was able to very quickly halve the learning just by time traveling by 33 years.</span></p><p>So if I time travel by algorithms 33 years, I could adjust what Yann LeCun did in 1989, and I could halve the error. But to get further gains, I had to add a lot more data, I had to 10x the training set, and then I had to add more computational optimizations. I had to train for much longer with dropout and other regularization techniques.</p><p><span>So all these things have to improve simultaneously. We’re probably going to have a lot more data, we’re probably going to have a lot better hardware, probably going to have a lot better </span><a href="https://en.wikipedia.org/wiki/Compute_kernel" rel>kernels</a><span> and software, we’re probably going to have better algorithms. All of those, it’s almost like no one of them is winning too much. All of them are surprisingly equal. This has been the trend for a while.</span></p><p>So to answer your question, I expect differences algorithmically to what’s happening today. But I do also expect that some of the things that have stuck around for a very long time will probably still be there. It’s probably still a giant neural network trained with gradient descent. That would be my guess.</p><p><strong>Dwarkesh Patel </strong><em>00:26:16</em></p><p>It’s surprising that all of those things together only halved the error, 30 years of progress…. Maybe half is a lot. Because if you halve the error, that actually means that…</p><p><strong>Andrej Karpathy </strong><em>00:26:30</em></p><p>Half is a lot. But I guess what was shocking to me is everything needs to improve across the board: architecture, optimizer, loss function. It also has improved across the board forever. So I expect all those changes to be alive and well.</p><p><strong>Dwarkesh Patel </strong><em>00:26:43</em></p><p><span>Yeah. I was about to ask you a very similar question about </span><a href="https://x.com/karpathy/status/1977755427569111362" rel>nanochat</a><span>. Since you just coded it up recently, every single step in the process of building a chatbot is fresh in your RAM. I’m curious if you had similar thoughts about, “Oh, there was no one thing that was relevant to going from GPT-2 to nanochat.” What are some surprising takeaways from the experience?</span></p><p><strong>Andrej Karpathy </strong><em>00:27:08</em></p><p>Of building nanochat? So nanochat is a repository I released. Was it yesterday or the day before? I can’t remember.</p><p><strong>Dwarkesh Patel </strong><em>00:27:15</em></p><p>We can see the sleep deprivation that went into the…</p><p><strong>Andrej Karpathy </strong><em>00:27:18</em></p><p>It’s trying to be the simplest complete repository that covers the whole pipeline end-to-end of building a ChatGPT clone. So you have all of the steps, not just any individual step, which is a bunch. I worked on all the individual steps in the past and released small pieces of code that show you how that’s done in an algorithmic sense, in simple code. But this handles the entire pipeline. In terms of learning, I don’t know that I necessarily found something that I learned from it. I already had in my mind how you build it. This is just the process of mechanically building it and making it clean enough so that people can learn from it and that they find it useful.</p><p><strong>Dwarkesh Patel </strong><em>00:28:04</em></p><p>What is the best way for somebody to learn from it? Is it to just delete all the code and try to reimplement from scratch, try to add modifications to it?</p><p><strong>Andrej Karpathy </strong><em>00:28:10</em></p><p>That’s a great question. Basically it’s about 8,000 lines of code that takes you through the entire pipeline. I would probably put it on the right monitor. If you have two monitors, you put it on the right. You want to build it from scratch, you build it from the start. You’re not allowed to copy-paste, you’re allowed to reference, you’re not allowed to copy-paste. Maybe that’s how I would do it.</p><p>But I also think the repository by itself is a pretty large beast. When you write this code, you don’t go from top to bottom, you go from chunks and you grow the chunks, and that information is absent. You wouldn’t know where to start. So it’s not just a final repository that’s needed, it’s the building of the repository, which is a complicated chunk-growing process. So that part is not there yet. I would love to add that probably later this week. It’s probably a video or something like that. Roughly speaking, that’s what I would try to do. Build the stuff yourself, but don’t allow yourself copy-paste.</p><p>I do think that there’s two types of knowledge, almost. There’s the high-level surface knowledge, but when you build something from scratch, you’re forced to come to terms with what you don’t understand and you don’t know that you don’t understand it.</p><p><span>It always leads to a deeper understanding. It’s the only way to build. If I can’t build it, I don’t understand it. That’s a </span><a href="https://en.wikipedia.org/wiki/Richard_Feynman" rel>Feynman</a><span> </span><a href="https://www.quora.com/What-did-Richard-Feynman-mean-when-he-said-What-I-cannot-create-I-do-not-understand" rel>quote</a><span>, I believe. I 100% have always believed this very strongly, because there are all these micro things that are just not properly arranged and you don’t really have the knowledge. You just think you have the knowledge. So don’t write blog posts, don’t do slides, don’t do any of that. Build the code, arrange it, get it to work. It’s the only way to go. Otherwise, you’re missing knowledge.</span></p><h3 class="header-anchor-post">00:29:45 – LLM cognitive deficits<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§llm-cognitive-deficits" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/llm-cognitive-deficits" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>00:29:45</em></p><p>You tweeted out that coding models were of very little help to you in assembling this repository. I’m curious why that was.</p><p><strong>Andrej Karpathy </strong><em>00:29:53</em></p><p>I guess I built the repository over a period of a bit more than a month. I would say there are three major classes of how people interact with code right now. Some people completely reject all of LLMs and they are just writing by scratch. This is probably not the right thing to do anymore.</p><p><span>The intermediate part, which is where I am, is you still write a lot of things from scratch, but you use the autocomplete that’s available now from these models. So when you start writing out a little piece of it, it will autocomplete for you and you can just tap through. Most of the time it’s correct, sometimes it’s not, and you edit it. But you’re still very much the architect of what you’re writing. Then there’s the </span><a href="https://en.wikipedia.org/wiki/Vibe_coding" rel>vibe coding</a><span>: “Hi, please implement this or that,” enter, and then let the model do it. That’s the agents.</span></p><p>I do feel like the agents work in very specific settings, and I would use them in specific settings. But these are all tools available to you and you have to learn what they’re good at, what they’re not good at, and when to use them. So the agents are pretty good, for example, if you’re doing boilerplate stuff. Boilerplate code that’s just copy-paste stuff, they’re very good at that. They’re very good at stuff that occurs very often on the Internet because there are lots of examples of it in the training sets of these models. There are features of things where the models will do very well.</p><p>I would say nanochat is not an example of those because it’s a fairly unique repository. There’s not that much code in the way that I’ve structured it. It’s not boilerplate code. It’s intellectually intense code almost, and everything has to be very precisely arranged. The models have so many cognitive deficits. One example, they kept misunderstanding the code because they have too much memory from all the typical ways of doing things on the Internet that I just wasn’t adopting. The models, for example—I don’t know if I want to get into the full details—but they kept thinking I’m writing normal code, and I’m not.</p><p><strong>Dwarkesh Patel </strong><em>00:31:49</em></p><p>Maybe one example?</p><p><strong>Andrej Karpathy </strong><em>00:31:51</em></p><p><span>You have eight </span><a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" rel>GPUs</a><span> that are all doing forward, backwards. The way to synchronize gradients between them is to use a </span><a href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel>Distributed Data Parallel</a><span> container of </span><a href="https://en.wikipedia.org/wiki/PyTorch" rel>PyTorch</a><span>, which automatically as you’re doing the backward, it will start communicating and synchronizing gradients. I didn’t use DDP because I didn’t want to use it, because it’s not necessary. I threw it out and wrote my own synchronization routine that’s inside the step of the optimizer. The models were trying to get me to use the DDP container. They were very concerned. This gets way too technical, but I wasn’t using that container because I don’t need it and I have a custom implementation of something like it.</span></p><p><strong>Dwarkesh Patel </strong><em>00:32:26</em></p><p>They just couldn’t internalize that you had your own.</p><p><strong>Andrej Karpathy </strong><em>00:32:28</em></p><p><span>They couldn’t get past that. They kept trying to mess up the style. They’re way too over-defensive. They make all these try-catch statements. They keep trying to make a production code base, and I have a bunch of assumptions in my code, and it’s okay. I don’t need all this extra stuff in there. So I feel like they’re bloating the code base, bloating the complexity, they keep misunderstanding, they’re using deprecated </span><a href="https://en.wikipedia.org/wiki/API" rel>APIs</a><span> a bunch of times. It’s a total mess. It’s just not net useful. I can go in, I can clean it up, but it’s not net useful.</span></p><p>I also feel like it’s annoying to have to type out what I want in English because it’s too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear and I start typing out the first few letters, autocomplete gets it and just gives you the code. This is a very high information bandwidth to specify what you want. You point to the code where you want it, you type out the first few pieces, and the model will complete it.</p><p>So what I mean is, these models are good in certain parts of the stack. There are two examples where I use the models that I think are illustrative. One was when I generated the report. That’s more boilerplate-y, so I partially vibe-coded some of that stuff. That was fine because it’s not mission-critical stuff, and it works fine.</p><p><span>The other part is when I was rewriting the tokenizer in </span><a href="https://en.wikipedia.org/wiki/Rust_(programming_language)" rel>Rust</a><span>. I’m not as good at Rust because I’m fairly new to Rust. So there’s a bit of vibe coding going on when I was writing some of the Rust code. But I had a Python implementation that I fully understand, and I’m just making sure I’m making a more efficient version of it, and I have tests so I feel safer doing that stuff. They increase accessibility to languages or paradigms that you might not be as familiar with. I think they’re very helpful there as well. There’s a ton of Rust code out there, the models are pretty good at it. I happen to not know that much about it, so the models are very useful there.</span></p><p><strong>Dwarkesh Patel </strong><em>00:34:23</em></p><p><span>The reason this question is so interesting is because the main story people have about AI exploding and getting to </span><a href="https://en.wikipedia.org/wiki/Superintelligence" rel>superintelligence</a><span> pretty rapidly is AI automating AI engineering and AI research. They’ll look at the fact that you can have </span><a href="https://www.claude.com/product/claude-code" rel>Claude Code</a><span> and make entire applications, </span><a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete" rel>CRUD</a><span> applications, from scratch and think, “If you had this same capability inside of OpenAI and DeepMind and everything, just imagine a thousand of you or a million of you in parallel, finding little architectural tweaks.”</span></p><p><span>It’s quite interesting to hear you say that this is the thing they’re asymmetrically worse at. It’s quite relevant to forecasting whether the </span><a href="https://ai-2027.com/" rel>AI 2027</a><span>-type explosion is likely to happen anytime soon.</span></p><p><strong>Andrej Karpathy </strong><em>00:35:05</em></p><p>That’s a good way of putting it, and you’re getting at why my timelines are a bit longer. You’re right. They’re not very good at code that has never been written before, maybe it’s one way to put it, which is what we’re trying to achieve when we’re building these models.</p><p><strong>Dwarkesh Patel </strong><em>00:35:19</em></p><p><span>Very naive question, but the architectural tweaks that you’re adding to nanochat, they’re in a paper somewhere, right? They might even be in a repo somewhere. Is it surprising that they aren’t able to integrate that into whenever you’re like, “Add </span><a href="https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html" rel>RoPE embeddings</a><span>” or something, they do that in the wrong way?</span></p><p><strong>Andrej Karpathy </strong><em>00:35:42</em></p><p>It’s tough. They know, but they don’t fully know. They don’t know how to fully integrate it into the repo and your style and your code and your place, and some of the custom things that you’re doing and how it fits with all the assumptions of the repository. They do have some knowledge, but they haven’t gotten to the place where they can integrate it and make sense of it.</p><p><span>A lot of the stuff continues to improve. Currently, the state-of-the-art model that I go to is the </span><a href="https://platform.openai.com/docs/models/gpt-5-pro" rel>GPT-5 Pro</a><span>, and that’s a very powerful model. If I have 20 minutes, I will copy-paste my entire repo and I go to GPT-5 Pro, the oracle, for some questions. Often it’s not too bad and surprisingly good compared to what existed a year ago.</span></p><p>Overall, the models are not there. I feel like the industry is making too big of a jump and is trying to pretend like this is amazing, and it’s not. It’s slop. They’re not coming to terms with it, and maybe they’re trying to fundraise or something like that. I’m not sure what’s going on, but we’re at this intermediate stage. The models are amazing. They still need a lot of work. For now, autocomplete is my sweet spot. But sometimes, for some types of code, I will go to an LLM agent.</p><p><strong>Dwarkesh Patel </strong><em>00:36:53</em></p><p><span>Here’s another reason this is really interesting. Through the history of programming, there have been many productivity improvements—</span><a href="https://en.wikipedia.org/wiki/Compiler" rel>compilers</a><span>, </span><a href="https://en.wikipedia.org/wiki/Lint_(software)" rel>linting</a><span>, better programming languages—which have increased programmer productivity but have not led to an explosion. That sounds very much like the autocomplete tab, and this other category is just automation of the programmer. It’s interesting you’re seeing more in the category of the historical analogies of better compilers or something.</span></p><p><strong>Andrej Karpathy </strong><em>00:37:26</em></p><p>Maybe this gets to one other thought. I have a hard time differentiating where AI begins and stops because I see AI as fundamentally an extension of computing in a pretty fundamental way. I see a continuum of this recursive self-improvement or speeding up programmers all the way from the beginning: code editors, syntax highlighting, or checking even of the types, like data type checking—all these tools that we’ve built for each other.</p><p>Even search engines. Why aren’t search engines part of AI? Ranking is AI. At some point, Google, even early on, was thinking of themselves as an AI company doing Google Search engine, which is totally fair.</p><p><span>I see it as a lot more of a continuum than other people do, and it’s hard for me to draw the line. I feel like we’re now getting a much better autocomplete, and now we’re also getting some agents which are these loopy things, but they go off-rails sometimes. What’s going on is that the human is progressively doing a bit less and less of the low-level stuff. We’re not writing the </span><a href="https://en.wikipedia.org/wiki/Assembly_language" rel>assembly code</a><span> because we have compilers. Compilers will take my high-level language in </span><a href="https://en.wikipedia.org/wiki/C_(programming_language)" rel>C</a><span> and write the assembly code.</span></p><p>We’re abstracting ourselves very, very slowly. There’s this what I call “autonomy slider,” where more and more stuff is automated—of the stuff that can be automated at any point in time—and we’re doing a bit less and less and raising ourselves in the layer of abstraction over the automation.</p><h3 class="header-anchor-post">00:40:05 – RL is terrible<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§rl-is-terrible" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/rl-is-terrible" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>00:40:05</em></p><p><span>Let’s talk about RL a bit. </span><a href="https://x.com/karpathy/status/1944435412489171119" rel>You tweeted some very interesting things about this</a><span>. Conceptually, how should we think about the way that humans are able to build a rich </span><a href="https://www.youtube.com/watch?v=hguIUmMsvA4" rel>world model</a><span> just from interacting with our environment, and in ways that seem almost irrespective of the final reward at the end of the episode?</span></p><p><span>If somebody is starting a business, and at the end of 10 years, she finds out whether the business succeeded or failed, we say that she’s earned a bunch of wisdom and experience. But it’s not because the log probs of every single thing that happened over the last 10 years are up-weighted or down-weighted. Something much more deliberate and rich is happening. What is the </span><a href="https://en.wikipedia.org/wiki/Machine_learning" rel>ML</a><span> analogy, and how does that compare to what we’re doing with LLMs right now?</span></p><p><strong>Andrej Karpathy </strong><em>00:40:47</em></p><p>Maybe the way I would put it is that humans don’t use reinforcement learning, as I said. I think they do something different. Reinforcement learning is a lot worse than I think the average person thinks. Reinforcement learning is terrible. It just so happens that everything that we had before it is much worse because previously we were just imitating people, so it has all these issues.</p><p>In reinforcement learning, say you’re solving a math problem, because it’s very simple. You’re given a math problem and you’re trying to find the solution. In reinforcement learning, you will try lots of things in parallel first. You’re given a problem, you try hundreds of different attempts. These attempts can be complex. They can be like, “Oh, let me try this, let me try that, this didn’t work, that didn’t work,” etc. Then maybe you get an answer. Now you check the back of the book and you see, “Okay, the correct answer is this.” You can see that this one, this one, and that one got the correct answer, but these other 97 of them didn’t. Literally what reinforcement learning does is it goes to the ones that worked really well and every single thing you did along the way, every single token gets upweighted like, “Do more of this.”</p><p>The problem with that is people will say that your estimator has high variance, but it’s just noisy. It’s noisy. It almost assumes that every single little piece of the solution that you made that arrived at the right answer was the correct thing to do, which is not true. You may have gone down the wrong alleys until you arrived at the right solution. Every single one of those incorrect things you did, as long as you got to the correct solution, will be upweighted as, “Do more of this.” It’s terrible. It’s noise.</p><p>You’ve done all this work only to find, at the end, you get a single number of like, “Oh, you did correct.” Based on that, you weigh that entire trajectory as like, upweight or downweight. The way I like to put it is you’re sucking supervision through a straw. You’ve done all this work that could be a minute of rollout, and you’re sucking the bits of supervision of the final reward signal through a straw and you’re broadcasting that across the entire trajectory and using that to upweight or downweight that trajectory. It’s just stupid and crazy.</p><p>A human would never do this. Number one, a human would never do hundreds of rollouts. Number two, when a person finds a solution, they will have a pretty complicated process of review of, “Okay, I think these parts I did well, these parts I did not do that well. I should probably do this or that.” They think through things. There’s nothing in current LLMs that does this. There’s no equivalent of it. But I do see papers popping out that are trying to do this because it’s obvious to everyone in the field.</p><p><span>The first </span><a href="https://en.wikipedia.org/wiki/Imitation_learning" rel>imitation learning</a><span>, by the way, was extremely surprising and miraculous and amazing, that we can fine-tune by imitation on humans. That was incredible. Because in the beginning, all we had was base models. Base models are autocomplete. It wasn’t obvious to me at the time, and I had to learn this. </span><a href="https://arxiv.org/abs/2203.02155" rel>The paper that blew my mind</a><span> was InstructGPT, because it pointed out that you can take the pretrained model, which is autocomplete, and if you just fine-tune it on text that looks like conversations, the model will very rapidly adapt to become very conversational, and it keeps all the knowledge from pre-training. This blew my mind because I didn’t understand that stylistically, it can adjust so quickly and become an assistant to a user through just a few loops of fine-tuning on that kind of data. It was very miraculous to me that that worked. So incredible. That was two to three years of work.</span></p><p><span>Now came RL. And RL allows you to do a bit better than just imitation learning because you can have these </span><a href="https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning" rel>reward functions</a><span> and you can </span><a href="https://en.wikipedia.org/wiki/Hill_climbing" rel>hill-climb</a><span> on the reward functions. Some problems have just correct answers, you can hill-climb on that without getting expert trajectories to imitate. So that’s amazing. The model can also discover solutions that a human might never come up with. This is incredible. Yet, it’s still stupid.</span></p><p><span>We need more. I saw a paper from Google yesterday that tried to have this reflect &amp; review idea in mind. Was it the </span><a href="https://arxiv.org/pdf/2509.25140" rel>memory bank paper</a><span> or something? I don’t know. I’ve seen a few papers along these lines. So I expect there to be some major update to how we do algorithms for LLMs coming in that realm. I think we need three or four or five more, something like that.</span></p><p><strong>Dwarkesh Patel </strong><em>00:44:54</em></p><p>You’re so good at coming up with evocative phrases. “Sucking supervision through a straw.” It’s so good.</p><p><span>You’re saying the problem with outcome-based reward is that you have this huge trajectory, and then at the end, you’re trying to learn every single possible thing about what you should do and what you should learn about the world from that one final bit. Given the fact that this is obvious, why hasn’t </span><a href="https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/" rel>process-based supervision</a><span> as an alternative been a successful way to make models more capable? What has been preventing us from using this alternative paradigm?</span></p><p><strong>Andrej Karpathy </strong><em>00:45:29</em></p><p>Process-based supervision just refers to the fact that we’re not going to have a reward function only at the very end. After you’ve done 10 minutes of work, I’m not going to tell you you did well or not well. I’m going to tell you at every single step of the way how well you’re doing. The reason we don’t have that is it’s tricky how you do that properly. You have partial solutions and you don’t know how to assign credit.  So when you get the right answer, it’s just an equality match to the answer. It’s very simple to implement. If you’re doing process supervision, how do you assign in an automatable way, a partial credit assignment? It’s not obvious how you do it.</p><p>Lots of labs are trying to do it with these LLM judges. You get LLMs to try to do it. You prompt an LLM, “Hey, look at a partial solution of a student. How well do you think they’re doing if the answer is this?” and they try to tune the prompt.</p><p><span>The reason that this is tricky is quite subtle. It’s the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with billions of </span><a href="https://www.ibm.com/think/topics/model-parameters" rel>parameters</a><span>, and they’re gameable. If you’re reinforcement learning with respect to them, you will find adversarial examples for your LLM judges, almost guaranteed. So you can’t do this for too long. You do maybe 10 steps or 20 steps, and maybe it will work, but you can’t do 100 or 1,000. I understand it’s not obvious, but basically the model will find little cracks. It will find all these spurious things in the nooks and crannies of the giant model and find a way to cheat it.</span></p><p>One example that’s prominently in my mind, this was probably public, if you’re using an LLM judge for a reward, you just give it a solution from a student and ask it if the student did well or not. We were training with reinforcement learning against that reward function, and it worked really well. Then, suddenly, the reward became extremely large. It was a massive jump, and it did perfect. You’re looking at it like, “Wow, this means the student is perfect in all these problems. It’s fully solved math.”</p><p>But when you look at the completions that you’re getting from the model, they are complete nonsense. They start out okay, and then they change to “dhdhdhdh.” It’s just like, “Oh, okay, let’s take two plus three and we do this and this, and then dhdhdhdh.” You’re looking at it, and it’s like, this is crazy. How is it getting a reward of one or 100%? You look at the LLM judge, and it turns out that “dhdhdhdh” is an adversarial example for the model, and it assigns 100% probability to it.</p><p>It’s just because this is an out-of-sample example to the LLM. It’s never seen it during training, and you’re in pure generalization land. It’s never seen it during training, and in the pure generalization land, you can find these examples that break it.</p><p><strong>Dwarkesh Patel </strong><em>00:47:52</em></p><p><span>You’re basically training the LLM to be a </span><a href="https://en.wikipedia.org/wiki/Prompt_injection" rel>prompt injection</a><span> model.</span></p><p><strong>Andrej Karpathy </strong><em>00:47:56</em></p><p>Not even that. Prompt injection is way too fancy. You’re finding adversarial examples, as they’re called. These are nonsensical solutions that are obviously wrong, but the model thinks they are amazing.</p><p><strong>Dwarkesh Patel </strong><em>00:48:07</em></p><p><span>To the extent you think this is the bottleneck to making RL more functional, then that will require making LLMs better judges, if you want to do this in an automated way. Is it just going to be some sort of </span><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel>GAN</a><span>-like approach where you have to train models to be more robust?</span></p><p><strong>Andrej Karpathy </strong><em>00:48:22</em></p><p>The labs are probably doing all that. The obvious thing is, “dhdhdhdh” should not get 100% reward. Okay, well, take “dhdhdhdh,” put it in the training set of the LLM judge, and say this is not 100%, this is 0%. You can do this, but every time you do this, you get a new LLM, and it still has adversarial examples. There’s an infinity of adversarial examples.</p><p>Probably if you iterate this a few times, it’ll probably be harder and harder to find adversarial examples, but I’m not 100% sure because this thing has a trillion parameters or whatnot. I bet you the labs are trying. I still think we need other ideas.</p><p><strong>Dwarkesh Patel </strong><em>00:48:57</em></p><p>Interesting. Do you have some shape of what the other idea could be?</p><p><strong>Andrej Karpathy </strong><em>00:49:02</em></p><p><span>This idea of a review solution encompassing </span><a href="https://en.wikipedia.org/wiki/Synthetic_data" rel>synthetic examples</a><span> such that when you train on them, you get better, and meta-learn it in some way. I think there are some papers that I’m starting to see pop out. I am only at a stage of reading abstracts because a lot of these papers are just ideas. Someone has to make it work on a frontier LLM lab scale in full generality because when you see these papers, they pop up, and it’s just a bit noisy. They’re cool ideas, but I haven’t seen anyone convincingly show that this is possible. That said, the LLM labs are fairly closed, so who knows what they’re doing now.</span></p><h3 class="header-anchor-post">00:49:38 – How do humans learn?<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§how-do-humans-learn" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/how-do-humans-learn" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>00:49:38</em></p><p>I can conceptualize how you would be able to train on synthetic examples or synthetic problems that you have made for yourself. But there seems to be another thing humans do—maybe sleep is this, maybe daydreaming is this—which is not necessarily to come up with fake problems, but just to reflect.</p><p>I’m not sure what the ML analogy is for daydreaming or sleeping, or just reflecting. I haven’t come up with a new problem. Obviously, the very basic analogy would just be fine-tuning on reflection bits, but I feel like in practice that probably wouldn’t work that well. Do you have some take on what the analogy of this thing is?</p><p><strong>Andrej Karpathy </strong><em>00:50:17</em></p><p>I do think that we’re missing some aspects there. As an example, let’s take reading a book. Currently when LLMs are reading a book, what that means is we stretch out the sequence of text, and the model is predicting the next token, and it’s getting some knowledge from that. That’s not really what humans do. When you’re reading a book, I don’t even feel like the book is exposition I’m supposed to be attending to and training on. The book is a set of prompts for me to do synthetic data generation, or for you to get to a book club and talk about it with your friends. It’s by manipulating that information that you actually gain that knowledge. We have no equivalent of that with LLMs. They don’t really do that. I’d love to see during pre-training some stage that thinks through the material and tries to reconcile it with what it already knows, and thinks through it for some amount of time and gets that to work. There’s no equivalence of any of this. This is all research.</p><p><span>There are some subtle—very subtle that I think are very hard to understand—reasons why it’s not trivial. If I can just describe one: why can’t we just synthetically generate and train on it? Because every synthetic example, if I just give synthetic generation of the model thinking about a book, you look at it and you’re like, “This looks great. Why can’t I train on it?” You could try, but the model will get much worse if you continue trying. That’s because all of the samples you get from models are silently collapsed. Silently—it is not obvious if you look at any individual example of it—they occupy a very tiny manifold of the possible space of thoughts about content. The LLMs, when they come off, they’re what we call “</span><a href="https://en.wikipedia.org/wiki/Model_collapse" rel>collapsed</a><span>.” They have a collapsed data distribution. One easy way to see it is to go to ChatGPT and ask it, “Tell me a joke.” It only has like three jokes. It’s not giving you the whole breadth of possible jokes. It knows like three jokes. They’re silently collapsed.</span></p><p>You’re not getting the richness and the diversity and the entropy from these models as you would get from humans. Humans are a lot noisier, but at least they’re not biased, in a statistical sense. They’re not silently collapsed. They maintain a huge amount of entropy. So how do you get synthetic data generation to work despite the collapse and while maintaining the entropy? That’s a research problem.</p><p><strong>Dwarkesh Patel </strong><em>00:52:20</em></p><p>Just to make sure I understood, the reason that the collapse is relevant to synthetic data generation is because you want to be able to come up with synthetic problems or reflections which are not already in your data distribution?</p><p><strong>Andrej Karpathy </strong><em>00:52:32</em></p><p>I guess what I’m saying is, say we have a chapter of a book and I ask an LLM to think about it, it will give you something that looks very reasonable. But if I ask it 10 times, you’ll notice that all of them are the same.</p><p><strong>Dwarkesh Patel </strong><em>00:52:44</em></p><p>You can’t just keep scaling “reflection” on the same amount of prompt information and then get returns from that.</p><p><strong>Andrej Karpathy </strong><em>00:52:54</em></p><p>Any individual sample will look okay, but the distribution of it is quite terrible. It’s quite terrible in such a way that if you continue training on too much of your own stuff, you actually collapse.</p><p>I think that there’s possibly no fundamental solution to this. I also think humans collapse over time. These analogies are surprisingly good. Humans collapse during the course of their lives. This is why children, they haven’t overfit yet. They will say stuff that will shock you because you can see where they’re coming from, but it’s just not the thing people say, because they’re not yet collapsed. But we’re collapsed. We end up revisiting the same thoughts. We end up saying more and more of the same stuff, and the learning rates go down, and the collapse continues to get worse, and then everything deteriorates.</p><p><strong>Dwarkesh Patel </strong><em>00:53:39</em></p><p><span>Have you seen this </span><a href="https://www.sciencedirect.com/science/article/pii/S2666389921000945?ref=pdf_download&amp;fr=RR-7&amp;rr=98fc403fefa4f8a7" rel>super interesting paper that dreaming is a way of preventing this</a><span> kind of </span><a href="https://aws.amazon.com/what-is/overfitting/" rel>overfitting</a><span> and collapse? The reason dreaming is evolutionary adaptive is to put you in weird situations that are very unlike your day-to-day reality, so as to prevent this kind of overfitting.</span></p><p><strong>Andrej Karpathy </strong><em>00:53:55</em></p><p>It’s an interesting idea. I do think that when you’re generating things in your head and then you’re attending to it, you’re training on your own samples, you’re training on your synthetic data. If you do it for too long, you go off-rails and you collapse way too much. You always have to seek entropy in your life. Talking to other people is a great source of entropy, and things like that. So maybe the brain has also built some internal mechanisms for increasing the amount of entropy in that process. That’s an interesting idea.</p><p><strong>Dwarkesh Patel </strong><em>00:54:25</em></p><p>This is a very ill-formed thought so I’ll just put it out and let you react to it. The best learners that we are aware of, which are children, are extremely bad at recollecting information. In fact, at the very earliest stages of childhood, you will forget everything. You’re just an amnesiac about everything that happens before a certain year date. But you’re extremely good at picking up new languages and learning from the world. Maybe there’s some element of being able to see the forest for the trees.</p><p>Whereas if you compare it to the opposite end of the spectrum, you have LLM pre-training, where these models will literally be able to regurgitate word-for-word what is the next thing in a Wikipedia page. But their ability to learn abstract concepts really quickly, the way a child can, is much more limited. Then adults are somewhere in between, where they don’t have the flexibility of childhood learning, but they can memorize facts and information in a way that is harder for kids. I don’t know if there’s something interesting about that spectrum.</p><p><strong>Andrej Karpathy </strong><em>00:55:19</em></p><p>I think there’s something very interesting about that, 100%. I do think that humans have a lot more of an element, compared to LLMs, of seeing the forest for the trees. We’re not actually that good at memorization, which is actually a feature. Because we’re not that good at memorization, we’re forced to find patterns in a more general sense.</p><p>LLMs in comparison are extremely good at memorization. They will recite passages from all these training sources. You can give them completely nonsensical data. You can hash some amount of text or something like that, you get a completely random sequence. If you train on it, even just for a single iteration or two, it can suddenly regurgitate the entire thing. It will memorize it. There’s no way a person can read a single sequence of random numbers and recite it to you.</p><p><span>That’s a feature, not a bug, because it forces you to only learn the generalizable components. Whereas LLMs are distracted by all the memory that they have of the pre-training documents, and it’s probably very distracting to them in a certain sense. So that’s why when I talk about the </span><a href="https://x.com/karpathy/status/1938626382248149433" rel>cognitive core</a><span>, I want to remove the memory, which is what we talked about. I’d love to have them have less memory so that they have to look things up, and they only maintain the algorithms for thought, and the idea of an experiment, and all this cognitive glue of acting.</span></p><p><strong>Dwarkesh Patel </strong><em>00:56:36</em></p><p>And this is also relevant to preventing model collapse?</p><p><strong>Andrej Karpathy </strong><em>00:56:41</em></p><p>Let me think. I’m not sure. It’s almost like a separate axis. The models are way too good at memorization, and somehow we should remove that. People are much worse, but it’s a good thing.</p><p><strong>Dwarkesh Patel </strong><em>00:56:57</em></p><p><span>What is a solution to model collapse? There are very naive things you could attempt. The distribution over </span><a href="https://en.wikipedia.org/wiki/Logit" rel>logits</a><span> should be wider or something. There are many naive things you could try. What ends up being the problem with the naive approaches?</span></p><p><strong>Andrej Karpathy </strong><em>00:57:11</em></p><p>That’s a great question. You can imagine having a regularization for entropy and things like that. I guess they just don’t work as well empirically because right now the models are collapsed. But I will say most of the tasks that we want from them don’t actually demand diversity. That’s probably the answer to what’s going on.</p><p>The frontier labs are trying to make the models useful. I feel like the diversity of the outputs is not so much... Number one, it’s much harder to work with and evaluate and all this stuff, but maybe it’s not what’s capturing most of the value.</p><p><strong>Dwarkesh Patel </strong><em>00:57:42</em></p><p>In fact, it’s actively penalized. If you’re super creative in RL, it’s not good.</p><p><strong>Andrej Karpathy </strong><em>00:57:48</em></p><p>Yeah. Or maybe if you’re doing a lot of writing, help from LLMs and stuff like that, it’s probably bad because the models will silently give you all the same stuff. They won’t explore lots of different ways of answering a question.</p><p>Maybe this diversity, not as many applications need it so the models don’t have it. But then it’s a problem at synthetic data generation time, et cetera. So we’re shooting ourselves in the foot by not allowing this entropy to maintain in the model. Possibly the labs should try harder.</p><p><strong>Dwarkesh Patel </strong><em>00:58:17</em></p><p>I think you hinted that it’s a very fundamental problem, it won’t be easy to solve. What’s your intuition for that?</p><p><strong>Andrej Karpathy </strong><em>00:58:24</em></p><p>I don’t know if it’s super fundamental. I don’t know if I intended to say that. I do think that I haven’t done these experiments, but I do think that you could probably regularize the entropy to be higher. So you’re encouraging the model to give you more and more solutions, but you don’t want it to start deviating too much from the training data. It’s going to start making up its own language. It’s going to start using words that are extremely rare, so it’s going to drift too much from the distribution.</p><p>So I think controlling the distribution is just tricky. It’s probably not trivial in that sense.</p><p><strong>Dwarkesh Patel </strong><em>00:58:58</em></p><p><span>How many bits should the optimal core of intelligence end up being if you just had to make a guess? The thing we put on the </span><a href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft" rel>von Neumann probes</a><span>, how big does it have to be?</span></p><p><strong>Andrej Karpathy </strong><em>00:59:10</em></p><p><span>It’s really interesting in the history of the field because at one point everything was very scaling-pilled in terms of like, “Oh, we’re gonna make much bigger models, trillions of parameter models.” What the models have done in size is they’ve gone up and now they’ve come down. State-of-the-art models are smaller. Even then, I think they memorized way too much. So I had a </span><a href="https://x.com/karpathy/status/1938626382248149433" rel>prediction</a><span> a while back that I almost feel like we can get cognitive cores that are very good at even a billion parameters.</span></p><p>If you talk to a billion parameter model, I think in 20 years, you can have a very productive conversation. It thinks and it’s a lot more like a human. But if you ask it some factual question, it might have to look it up, but it knows that it doesn’t know and it might have to look it up and it will just do all the reasonable things.</p><p><strong>Dwarkesh Patel </strong><em>00:59:54</em></p><p>That’s surprising that you think it’ll take a billion parameters. Because already we have billion parameter models or a couple billion parameter models that are very intelligent.</p><p><strong>Andrej Karpathy </strong><em>01:00:02</em></p><p>Well, state-of-the-art models are like a trillion parameters. But they remember so much stuff.</p><p><strong>Dwarkesh Patel </strong><em>01:00:06</em></p><p><span>Yeah, but I’m surprised that in 10 years, given the pace… We have </span><a href="https://openai.com/index/introducing-gpt-oss/" rel>gpt-oss-20b</a><span>. That’s way better than GPT-4 original, which was a trillion plus parameters. Given that trend, I’m surprised you think in 10 years the cognitive core is still a billion parameters. I’m surprised you’re not like, “Oh it’s gonna be like tens of millions or millions.”</span></p><p><strong>Andrej Karpathy </strong><em>01:00:30</em></p><p><span>Here’s the issue, the training data is the internet, which is really terrible. There’s a huge amount of gains to be made because the internet is terrible. Even the internet, when you and I think of the internet, you’re thinking of like </span><em>The Wall Street Journal</em><span>. That’s not what this is. When you’re looking at a pre-training dataset in the frontier lab and you look at a random internet document, it’s total garbage. I don’t even know how this works at all. It’s some like stock tickers, symbols, it’s a huge amount of slop and garbage from like all the corners of the internet. It’s not like your </span><em>Wall Street Journal</em><span> article, that’s extremely rare. So because the internet is so terrible, we have to build really big models to compress all that. Most of that compression is memory work instead of cognitive work.</span></p><p>But what we really want is the cognitive part, delete the memory. I guess what I’m saying is that we need intelligent models to help us refine even the pre-training set to just narrow it down to the cognitive components. Then I think you get away with a much smaller model because it’s a much better dataset and you could train it on it. But probably it’s not trained directly on it, it’s probably distilled from a much better model still.</p><p><strong>Dwarkesh Patel </strong><em>01:01:35</em></p><p>But why is the distilled version still a billion?</p><p><strong>Andrej Karpathy </strong><em>01:01:39</em></p><p>I just feel like distillation works extremely well. So almost every small model, if you have a small model, it’s almost certainly distilled.</p><p><strong>Dwarkesh Patel </strong><em>01:01:46</em></p><p>Right, but why is the distillation in 10 years not getting below 1 billion?</p><p><strong>Andrej Karpathy </strong><em>01:01:50</em></p><p>Oh, you think it should be smaller than a billion? I mean, come on, right? I don’t know. At some point it should take at least a billion knobs to do something interesting. You’re thinking it should be even smaller?</p><p><strong>Dwarkesh Patel </strong><em>01:02:01</em></p><p><span>Yeah. If you look at the trend over the last few years of just finding low-hanging fruit and going from trillion plus models to models that are literally two orders of magnitude smaller in a matter of two years and having better performance, it makes me think the sort of core of intelligence might be even way, way smaller. </span><a href="https://en.wikipedia.org/wiki/There%27s_Plenty_of_Room_at_the_Bottom" rel>Plenty of room at the bottom, to paraphrase Feynman.</a></p><p><strong>Andrej Karpathy </strong><em>01:02:22</em></p><p>I feel like I’m already contrarian by talking about a billion parameter cognitive core and you’re outdoing me. Maybe we could get a little bit smaller. I do think that practically speaking, you want the model to have some knowledge. You don’t want it to be looking up everything because then you can’t think in your head. You’re looking up way too much stuff all the time. Some basic curriculum needs to be there for knowledge, but it doesn’t have esoteric knowledge.</p><p><strong>Dwarkesh Patel </strong><em>01:02:48</em></p><p><span>We’re discussing what plausibly could be the cognitive core. There’s a separate question which is what will be the size of frontier models over time? I’m curious if you have predictions. We had increasing scale up to maybe </span><a href="https://openai.com/index/introducing-gpt-4-5/" rel>GPT 4.5</a><span> and now we’re seeing decreasing or plateauing scale. There are many reasons this could be going on. Do you have a prediction going forward? Will the biggest models be bigger, will they be smaller, will they be the same?</span></p><p><strong>Andrej Karpathy </strong><em>01:03:14</em></p><p><span>I don’t have a super strong prediction. The labs are just being practical. They have a </span><a href="https://en.wikipedia.org/wiki/Floating_point_operations_per_second" rel>flops</a><span> budget and a cost budget. It just turns out that pre-training is not where you want to put most of your flops or your cost. That’s why the models have gotten smaller. They are a bit smaller, the pre-training stage is smaller, but they make it up in reinforcement learning, mid-training, and all this stuff that follows. They’re just being practical in terms of all the stages and how you get the most bang for the buck.</span></p><p>Forecasting that trend is quite hard. I do still expect that there’s so much low-hanging fruit. That’s my basic expectation. I have a very wide distribution here.</p><p><strong>Dwarkesh Patel </strong><em>01:03:51</em></p><p><span>Do you expect the low-hanging fruit to be similar in kind to the kinds of things that have been happening over the last two to five years? If I look at nanochat versus </span><a href="https://github.com/karpathy/nanoGPT" rel>nanoGPT</a><span> and the architectural tweaks you made, is that the flavor of things you expect to continue to keep happening? You’re not expecting any giant paradigm shifts.</span></p><p><strong>Andrej Karpathy </strong><em>01:04:11</em></p><p>For the most part, yeah. I expect the datasets to get much, much better. When you look at the average datasets, they’re extremely terrible. They’re so bad that I don’t even know how anything works. Look at the average example in the training set: factual mistakes, errors, nonsensical things. Somehow when you do it at scale, the noise washes away and you’re left with some of the signal. Datasets will improve a ton.</p><p><span>Everything gets better. Our hardware, all the kernels for running the hardware and maximizing what you get with the hardware. </span><a href="https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell" rel>Nvidia is slowly tuning the hardware itself</a><span>, </span><a href="https://www.nvidia.com/en-us/data-center/tensor-cores/" rel>Tensor Cores</a><span>, all that needs to happen and will continue to happen. All the kernels will get better and utilize the chip to the max extent. All the algorithms will probably improve over optimization, architecture, and all the modeling components of how everything is done and what the algorithms are that we’re even training with. I do expect that nothing dominates. Everything plus 20%. This is roughly what I’ve seen.</span></p><h3 class="header-anchor-post">01:06:25 – AGI will blend into 2% GDP growth<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§agi-will-blend-into-gdp-growth" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/agi-will-blend-into-gdp-growth" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>01:06:25</em></p><p>People have proposed different ways of charting how much progress we’ve made towards full AGI. If you can come up with some line, then you can see where that line intersects with AGI and where that would happen on the x-axis. People have proposed it’s the education level. We had a high schooler, and then they went to college with RL, and they’re going to get a Ph.D.</p><p><strong>Andrej Karpathy </strong><em>01:06:44</em></p><p>I don’t like that one.</p><p><strong>Dwarkesh Patel </strong><em>01:06:45</em></p><p><span>Or they’ll propose </span><a href="https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks" rel>horizon length</a><span>. Maybe they can do tasks that take a minute, they can do those autonomously. Then they can autonomously do tasks that take an hour, a human an hour, a human a week. How do you think about the relevant y-axis here? How should we think about how AI is making progress?</span></p><p><strong>Andrej Karpathy </strong><em>01:07:05</em></p><p>I have two answers to that. Number one, I’m almost tempted to reject the question entirely because I see this as an extension of computing. Have we talked about how to chart progress in computing, or how do you chart progress in computing since the 1970s or whatever? What is the y-axis? The whole question is funny from that perspective a little bit.</p><p>When people talk about AI and the original AGI and how we spoke about it when OpenAI started, AGI was a system you could go to that can do any economically valuable task at human performance or better. That was the definition. I was pretty happy with that at the time. I’ve stuck to that definition forever, and then people have made up all kinds of other definitions. But I like that definition.</p><p>The first concession that people make all the time is they just take out all the physical stuff because we’re just talking about digital knowledge work. That’s a pretty major concession compared to the original definition, which was any task a human can do. I can lift things, etc. AI can’t do that, obviously, but we’ll take it. What fraction of the economy are we taking away by saying, “Oh, only knowledge work?” I don’t know the numbers. I feel about 10% to 20%, if I had to guess, is only knowledge work, someone could work from home and perform tasks, something like that. It’s still a really large market. What is the size of the economy, and what is 10% or 20%? We’re still talking about a few trillion dollars, even in the US, of market share or work. So it’s still a very massive bucket.</p><p><span>Going back to the definition, what I would be looking for is to what extent is that definition true? Are there jobs or lots of tasks? If we think of tasks as not jobs but tasks. It’s difficult because the problem is society will refactor based on the tasks that make up jobs, based on what’s automatable or not. Today, what jobs are replaceable by AI? A good example recently was Geoff Hinton’s prediction that radiologists would not be a job anymore, and </span><a href="https://www.nytimes.com/2025/05/14/technology/ai-jobs-radiologists-mayo-clinic.html" rel>this turned out to be very wrong in a bunch of ways</a><span>. Radiologists are alive and well and growing, even though </span><a href="https://en.wikipedia.org/wiki/Computer_vision" rel>computer vision</a><span> is really, really good at recognizing all the different things that they have to recognize in images. It’s just a messy, complicated job with a lot of surfaces and dealing with patients and all this stuff in the context of it.</span></p><p>I don’t know that by that definition AI has made a huge dent yet. Some of the jobs that I would be looking for have some features that make it very amenable to automation earlier than later. As an example, call center employees often come up, and I think rightly so. Call center employees have a number of simplifying properties with respect to what’s automatable today. Their jobs are pretty simple. It’s a sequence of tasks, and every task looks similar. You take a phone call with a person, it’s 10 minutes of interaction or whatever it is, probably a bit longer. In my experience, a lot longer. You complete some task in some scheme, and you change some database entries around or something like that. So you keep repeating something over and over again, and that’s your job.</p><p>You do want to bring in the task horizon—how long it takes to perform a task—and then you want to also remove context. You’re not dealing with different parts of services of companies or other customers. It’s just the database, you, and a person you’re serving. It’s more closed, it’s more understandable, it’s purely digital. So I would be looking for those things.</p><p>But even there, I’m not looking at full automation yet. I’m looking for an autonomy slider. I expect that we are not going to instantly replace people. We’re going to be swapping in AIs that do 80% of the volume. They delegate 20% of the volume to humans, and humans are supervising teams of five AIs doing the call center work that’s more rote. I would be looking for new interfaces or new companies that provide some layer that allows you to manage some of these AIs that are not yet perfect. Then I would expect that across the economy. A lot of jobs are a lot harder than a call center employee.</p><p><strong>Dwarkesh Patel </strong><em>01:11:02</em></p><p><span>With radiologists, I’m totally speculating and I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when </span><a href="https://en.wikipedia.org/wiki/Waymo" rel>Waymos</a><span> were first being rolled out, there’d be a person sitting in the front seat, and you just had to have them there to make sure that if something went really wrong, they’re there to monitor. Even today, people are still watching to make sure things are going well. </span><a href="https://www.tesla.com/robotaxi" rel>Robotaxi</a><span>, which was just deployed, still has a person inside it.</span></p><p>Now we could be in a similar situation where if you automate 99% of a job, that last 1% the human has to do is incredibly valuable because it’s bottlenecking everything else. If it were the case with radiologists, where the person sitting in the front of Waymo has to be specially trained for years in order to provide the last 1%, their wages should go up tremendously because they’re the one thing bottlenecking wide deployment. Radiologists, I think their wages have gone up for similar reasons, if you’re the last bottleneck and you’re not fungible. A Waymo driver might be fungible with others. So you might see this thing where your wages go up until you get to 99% and then fall just like that when the last 1% is gone. And I wonder if we’re seeing similar things with radiology or salaries of call center workers or anything like that.</p><p><strong>Andrej Karpathy </strong><em>01:12:17</em></p><p>That’s an interesting question. I don’t think we’re currently seeing that with radiology. I think radiology is not a good example. I don’t know why Geoff Hinton picked on radiology because I think it’s an extremely messy, complicated profession.</p><p>I would be a lot more interested in what’s happening with call center employees today, for example, because I would expect a lot of the rote stuff to be automatable today. I don’t have first-level access to it but I would be looking for trends of what’s happening with the call center employees. Some of the things I would also expect is that maybe they are swapping in AI, but then I would still wait for a year or two because I would potentially expect them to pull back and rehire some of the people.</p><p><strong>Dwarkesh Patel </strong><em>01:13:00</em></p><p>There’s been evidence that that’s already been happening generally in companies that have been adopting AI, which I think is quite surprising.</p><p>I also found what was really surprising. AGI, right? A thing which would do everything. We’ll take out physical work, but it should be able to do all knowledge work. What you would have naively anticipated is that the way this progression would happen is that you take a little task that a consultant is doing, you take that out of the bucket. You take a little task that an accountant is doing, you take that out of the bucket. Then you’re just doing this across all knowledge work.</p><p>But instead, if we do believe we’re on the path of AGI with the current paradigm, the progression is very much not like that. It does not seem like consultants and accountants are getting huge productivity improvements. It’s very much like programmers are getting more and more chiseled away at their work. If you look at the revenues of these companies, discounting normal chat revenue—which is similar to Google or something—just looking at API revenues, it’s dominated by coding. So this thing which is “general”, which should be able to do any knowledge work, is just overwhelmingly doing only coding. It’s a surprising way that you would expect the AGI to be deployed.</p><p><strong>Andrej Karpathy </strong><em>01:14:13</em></p><p>There’s an interesting point here. I do believe coding is the perfect first thing for these LLMs and agents. That’s because coding has always fundamentally worked around text. It’s computer terminals and text, and everything is based around text. LLMs, the way they’re trained on the Internet, love text. They’re perfect text processors, and there’s all this data out there. It’s a perfect fit.</p><p><span>We also have a lot of infrastructure pre-built for handling code and text. For example, we have </span><a href="https://code.visualstudio.com/" rel>Visual Studio Code</a><span> or your favorite </span><a href="https://en.wikipedia.org/wiki/Integrated_development_environment" rel>IDE</a><span> showing you code, and an agent can plug into that. If an agent has a </span><a href="https://en.wikipedia.org/wiki/Diff" rel>diff</a><span> where it made some change, we suddenly have all this code already that shows all the differences to a code base using a diff. It’s almost like we’ve pre-built a lot of the infrastructure for code.</span></p><p>Contrast that with some of the things that don’t enjoy that at all. As an example, there are people trying to build automation not for coding, but for slides. I saw a company doing slides. That’s much, much harder. The reason it’s much harder is because slides are not text. Slides are little graphics, they’re arranged spatially, and there’s a visual component to it. Slides don’t have this pre-built infrastructure. For example, if an agent is to make a change to your slides, how does a thing show you the diff? How do you see the diff? There’s nothing that shows diffs for slides. Someone has to build it. Some of these things are not amenable to AIs as they are, which are text processors, and code surprisingly is.</p><p><strong>Dwarkesh Patel </strong><em>01:15:48</em></p><p>I’m not sure that alone explains it. I personally have tried to get LLMs to be useful in domains which are just pure language-in, language-out, like rewriting transcripts, coming up with clips based on transcripts. It’s very plausible that I didn’t do every single possible thing I could do. I put a bunch of good examples in context, but maybe I should have done some kind of fine-tuning.</p><p><span>Our mutual friend, </span><a href="https://www.dwarkesh.com/p/andy-matuschak" rel>Andy Matuschak</a><span>, told me that he tried 50 billion things to try to get models to be good at writing spaced repetition prompts. Again, very much language-in, language-out tasks, the kind of thing that should be dead center in the repertoire of these LLMs. He tried in-context learning with a </span><a href="https://www.ibm.com/think/topics/few-shot-learning" rel>few-shot</a><span> examples. He tried supervised fine-tuning and retrieval. He could not get them to make cards to his satisfaction.</span></p><p>So I find it striking that even in language-out domains, it’s very hard to get a lot of economic value out of these models separate from coding. I don’t know what explains it.</p><p><strong>Andrej Karpathy </strong><em>01:16:57</em></p><p>That makes sense. I’m not saying that anything text is trivial. I do think that code is pretty structured. Text is maybe a lot more flowery, and there’s a lot more entropy in text, I would say. I don’t know how else to put it. Also code is hard, and so people feel quite empowered by LLMs, even from simple knowledge. I don’t know that I have a very good answer. Obviously, text makes it much, much easier, but it doesn’t mean that all text is trivial.</p><h3 class="header-anchor-post">01:17:36 – ASI<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§asi" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/asi" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>01:17:36</em></p><p><span>How do you think about </span><a href="https://en.wikipedia.org/wiki/Superintelligence" rel>superintelligence</a><span>? Do you expect it to feel qualitatively different from normal humans or human companies?</span></p><p><strong>Andrej Karpathy </strong><em>01:17:45</em></p><p><span>I see it as a progression of automation in society.</span><strong> </strong><span>Extrapolating the trend of computing, there will be a gradual automation of a lot of things, and superintelligence will an extrapolation of that. We expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work some amount of time later. Basically I see it as just automation, roughly speaking.</span></p><p><strong>Dwarkesh Patel </strong><em>01:18:10</em></p><p>But automation includes the things humans can already do, and superintelligence implies things humans can’t do.</p><p><strong>Andrej Karpathy </strong><em>01:18:16</em></p><p>But one of the things that people do is invent new things, which I would just put into the automation if that makes sense.</p><p><strong>Dwarkesh Patel </strong><em>01:18:20</em></p><p>But I guess, less abstractly and more qualitatively, do you expect something to feel like… Because this thing can either think so fast, or has so many copies, or the copies can merge back into themselves, or is much smarter, any number of advantages an AI might have, will the civilization in which these AIs exist just feel qualitatively different from humans?</p><p><strong>Andrej Karpathy </strong><em>01:18:51</em></p><p>I think it will. It is fundamentally automation, but it will be extremely foreign. It will look really strange. Like you mentioned, we can run all of this on a computer cluster and much faster.</p><p>Some of the scenarios that I start to get nervous about when the world looks like that is this gradual loss of control and understanding of what’s happening. I think that’s the most likely outcome, that there will be a gradual loss of understanding. We’ll gradually layer all this stuff everywhere, and there will be fewer and fewer people who understand it. Then there will be a gradual loss of control and understanding of what’s happening. That to me seems the most likely outcome of how all this stuff will go down.</p><p><strong>Dwarkesh Patel </strong><em>01:19:31</em></p><p>Let me probe on that a bit. It’s not clear to me that loss of control and loss of understanding are the same things. A board of directors at TSMC, Intel—name a random company—they’re just prestigious 80-year-olds. They have very little understanding, and maybe they don’t practically actually have control.</p><p>A better example is the President of the United States. The President has a lot of fucking power. I’m not trying to make a good statement about the current operant, or maybe I am, but the actual level of understanding is very different from the level of control.</p><p><strong>Andrej Karpathy </strong><em>01:20:06</em></p><p>I think that’s fair. That’s a good pushback. I think I expect loss of both.</p><p><strong>Dwarkesh Patel </strong><em>01:20:15</em></p><p>How come? Loss of understanding is obvious, but why loss of control?</p><p><strong>Andrej Karpathy </strong><em>01:20:20</em></p><p>We’re really far into a territory where I don’t know what this looks like, but if I were to write sci-fi novels, they would look along the lines of not even a single entity that takes over everything, but multiple competing entities that gradually become more and more autonomous. Some of them go rogue and the others fight them off. It’s this hot pot of completely autonomous activity that we’ve delegated to. I feel it would have that flavor.</p><p><strong>Dwarkesh Patel </strong><em>01:20:52</em></p><p>It is not the fact that they are smarter than us that is resulting in the loss of control. It’s the fact that they are competing with each other, and whatever arises out of that competition leads to the loss of control.</p><p><strong>Andrej Karpathy </strong><em>01:21:06</em></p><p>A lot of these things, they will be tools to people, they’re acting on behalf of people or something like that. So maybe those people are in control, but maybe it’s a loss of control overall for society in the sense of outcomes we want. You have entities acting on behalf of individuals that are still roughly seen as out of control.</p><p><strong>Dwarkesh Patel </strong><em>01:21:30</em></p><p>This is a question I should have asked earlier. We were talking about how currently it feels like when you’re doing AI engineering or AI research, these models are more in the category of compiler rather than in the category of a replacement.</p><p>At some point, if you have AGI, it should be able to do what you do. Do you feel like having a million copies of you in parallel results in some huge speed-up of AI progress? If that does happen, do you expect to see an intelligence explosion once we have a true AGI? I’m not talking about LLMs today.</p><p><strong>Andrej Karpathy </strong><em>01:22:01</em></p><p><span>I do, but it’s business as usual because we’re in an intelligence explosion already and have been for decades. It’s basically the GDP curve that is an exponential weighted sum over so many aspects of the industry. Everything is gradually being automated and has been for hundreds of years. The </span><a href="https://en.wikipedia.org/wiki/Industrial_Revolution" rel>Industrial Revolution</a><span> is automation and some of the physical components and tool building and all this stuff. Compilers are early software automation, et cetera. We’ve been recursively self-improving and exploding for a long time.</span></p><p>Another way to see it is that Earth was a pretty boring place if you don’t look at the biomechanics and so on, and looked very similar. If you look from space, we’re in the middle of this firecracker event, but we’re seeing it in slow motion. I definitely feel like this has already happened for a very long time. Again, I don’t see AI as a distinct technology with respect to what has already been happening for a long time.</p><p><strong>Dwarkesh Patel </strong><em>01:23:00</em></p><p>You think it’s continuous with this hyper-exponential trend?</p><p><strong>Andrej Karpathy </strong><em>01:23:03</em></p><p>Yes. That’s why this was very interesting to me, because I was trying to find AI in the GDP for a while. I thought that GDP should go up. But then I looked at some of the other technologies that I thought were very transformative, like computers or mobile phones or et cetera. You can’t find them in GDP. GDP is the same exponential.</p><p>Even the early iPhone didn’t have the App Store, and it didn’t have a lot of the bells and whistles that the modern iPhone has. So even though we think of 2008, when the iPhone came out, as this major seismic change, it’s actually not. Everything is so spread out and it so slowly diffuses that everything ends up being averaged up into the same exponential. It’s the exact same thing with computers. You can’t find them in the GDP like, “Oh, we have computers now.” That’s not what happened, because it’s such slow progression.</p><p>With AI we’re going to see the exact same thing. It’s just more automation. It allows us to write different kinds of programs that we couldn’t write before, but AI is still fundamentally a program. It’s a new kind of computer and a new kind of computing system. But it has all these problems, it’s going to diffuse over time, and it’s still going to add up to the same exponential. We’re still going to have an exponential that’s going to get extremely vertical. It’s going to be very foreign to live in that kind of an environment.</p><p><strong>Dwarkesh Patel </strong><em>01:24:10</em></p><p>Are you saying that, if you look at the trend before the Industrial Revolution to now, you have a hyper-exponential where you go from 0% growth to then 10,000 years ago, 0.02% growth, and to now when we’re at 2% growth. That’s a hyper-exponential. Are you saying if you’re charting AI on there, then AI takes you to 20% growth or 200% growth?</p><p>Or are you saying that if you look at the last 300 years, what you’ve been seeing is that you have technology after technology—computers, electrification, steam engines, railways, et cetera—but the rate of growth is the exact same, it’s 2%. Are you saying the rate of growth will go up?</p><p><strong>Andrej Karpathy </strong><em>01:24:46</em></p><p>The rate of growth has also stayed roughly constant, right?</p><p><strong>Dwarkesh Patel </strong><em>01:24:49</em></p><p>Only over the last 200, 300 years. But over the course of human history it’s exploded. It’s gone from 0% to faster, faster, faster. Industrial explosion, 2%.</p><p><strong>Andrej Karpathy </strong><em>01:25:01</em></p><p>For a while I tried to find AI or look for AI in the GDP curve, and I’ve convinced myself that this is false. Even when people talk about recursive self-improvement and labs and stuff like that, this is business as usual. Of course it’s going to recursively self-improve, and it’s been recursively self-improving.</p><p>LLMs allow the engineers to work much more efficiently to build the next round of LLM, and a lot more of the components are being automated and tuned and et cetera. All the engineers having access to Google Search is part of it. All the engineers having an IDE, all of them having autocomplete or having Claude code, et cetera, it’s all just part of the same speed-up of the whole thing. It’s just so smooth.</p><p><strong>Dwarkesh Patel </strong><em>01:25:41</em></p><p>Just to clarify, you’re saying that the rate of growth will not change. The intelligence explosion will show up as it just enabled us to continue staying on the 2% growth trajectory, just as the Internet helped us stay on the 2% growth trajectory.</p><p><strong>Andrej Karpathy </strong><em>01:25:53</em></p><p>Yes, my expectation is that it stays in the same pattern.</p><p><strong>Dwarkesh Patel </strong><em>01:25:58</em></p><p>Just to throw the opposite argument against you, my expectation is that it blows up because I think true AGI—and I’m not talking about LLM coding bots, I’m talking about actual replacement of a human in a server—is qualitatively different from these other productivity-improving technologies because it’s labor itself.</p><p>I think we live in a very labor-constrained world. If you talk to any startup founder or any person, you can be like, what do you need more of? You need really talented people. And if you have billions of extra people who are inventing stuff, integrating themselves, making companies bottom start to finish, that feels qualitatively different from a single technology. It’s as if you get 10 billion extra people on the planet.</p><p><strong>Andrej Karpathy </strong><em>01:26:44</em></p><p>Maybe a counterpoint. I’m pretty willing to be convinced one way or another on this point. But I will say, for example, computing is labor. Computing was labor. Computers, a lot of jobs disappeared because computers are automating a bunch of digital information processing that you now don’t need a human for. So computers are labor, and that has played out.</p><p>Self-driving as an example is also computers doing labor. That’s already been playing out. It’s still business as usual.</p><p><strong>Dwarkesh Patel </strong><em>01:27:13</em></p><p>You have a machine which is spitting out more things like that at potentially faster pace. Historically, we have examples of the growth regime changing where you went from 0.2% growth to 2% growth. It seems very plausible to me that a machine which is then spitting out the next self-driving car and the next Internet and whatever…</p><p><strong>Andrej Karpathy </strong><em>01:27:33</em></p><p>I see where it’s coming from. At the same time, I do feel like people make this assumption of, “We have God in a box, and now it can do everything,” and it just won’t look like that. It’s going to be able to do some of the things. It’s going to fail at some other things. It’s going to be gradually put into society, and we’ll end up with the same pattern. That is my prediction.</p><p>This assumption of suddenly having a completely intelligent, fully flexible, fully general human in a box, and we can dispense it at arbitrary problems in society, I don’t think that we will have this discrete change. I think we’ll arrive at the same kind of gradual diffusion of this across the industry.</p><p><strong>Dwarkesh Patel </strong><em>01:28:14</em></p><p>It often ends up being misleading in these conversations. I don’t like to use the word intelligence in this context because intelligence implies you think there’ll be a single superintelligence sitting in a server and it’ll divine how to come up with new technologies and inventions that cause this explosion. That’s not what I’m imagining when I’m imagining 20% growth. I’m imagining that there are billions of very smart human-like minds, potentially, or that’s all that’s required.</p><p>But the fact that there’s hundreds of millions of them, billions of them, each individually making new products, figuring out how to integrate themselves into the economy. If a highly experienced smart immigrant came to the country, you wouldn’t need to figure out how we integrate them in the economy. They figure it out. They could start a company, they could make inventions, or increase productivity in the world.</p><p>We have examples, even in the current regime, of places that have had 10-20% economic growth. If you just have a lot of people and less capital in comparison to the people, you can have Hong Kong or Shenzhen or whatever with decades of 10% plus growth. There’s a lot of really smart people who are ready to make use of the resources and do this period of catch-up because we’ve had this discontinuity, and I think AI might be similar.</p><p><strong>Andrej Karpathy </strong><em>01:29:33</em></p><p>I understand, but I still think that you’re presupposing some discrete jump. There’s some unlock that we’re waiting to claim. And suddenly we’re going to have geniuses in data centers. I still think you’re presupposing some discrete jump that has no historical precedent that I can’t find in any of the statistics and that I think probably won’t happen.</p><p><strong>Dwarkesh Patel </strong><em>01:29:52</em></p><p>I mean, the Industrial Revolution is such a jump. You went from 0.2% growth to 2% growth. I’m just saying you’ll see another jump like that.</p><p><strong>Andrej Karpathy </strong><em>01:30:00</em></p><p>I’m a little bit suspicious, I would have to take a look. For example, some of the logs are not very good from before the Industrial Revolution. I’m a bit suspicious of it but I don’t have strong opinions. You’re saying that this was a singular event that was extremely magical. You’re saying that maybe there’s going to be another event that’s going to be just like that, extremely magical. It will break the paradigm, and so on.</p><p><strong>Dwarkesh Patel </strong><em>01:30:23</em></p><p>I actually don’t think… The crucial thing with the Industrial Revolution was that it was not magical. If you just zoomed in, what you would see in 1770 or 1870 is not that there was some key invention. But at the same time, you did move the economy to a regime where the progress was much faster and the exponential 10x’d. I expect a similar thing from AI where it’s not like there’s going to be a single moment where we’ve made the crucial invention.</p><p><strong>Andrej Karpathy </strong><em>01:30:51</em></p><p>It’s an overhang that’s being unlocked. Like maybe there’s a new energy source. There’s some unlock—in this case, some kind of a cognitive capacity—and there’s an overhang of cognitive work to do.</p><p><strong>Dwarkesh Patel </strong><em>01:31:02</em></p><p>That’s right.</p><p><strong>Andrej Karpathy </strong><em>01:31:03</em></p><p>You’re expecting that overhang to be filled by this new technology when it crosses the threshold.</p><p><strong>Dwarkesh Patel </strong><em>01:31:06</em></p><p>Maybe one way to think about it is throughout history, a lot of growth comes because people come up with ideas, and then people are out there doing stuff to execute those ideas and make valuable output. Through most of this time, the population has been exploding. That has been driving growth.</p><p>For the last 50 years, people have argued that growth has stagnated. The population in frontier countries has also stagnated. I think we go back to the exponential growth in population that causes hyper-exponential growth in output.</p><p><strong>Andrej Karpathy </strong><em>01:31:37</em></p><p>It’s really hard to tell. I understand that viewpoint. I don’t intuitively feel that viewpoint.</p><h3 class="header-anchor-post">01:32:50 – Evolution of intelligence &amp; culture<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§evolution-of-intelligence-and-culture" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/evolution-of-intelligence-and-culture" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>01:32:50</em></p><p><span>You recommended </span><a href="https://www.dwarkesh.com/p/nick-lane" rel>Nick Lane’s</a><span> </span><a href="https://amzn.to/42KeI4w" rel>book</a><span> to me. On that basis, I also found it super interesting and I interviewed him. I have some questions about thinking about intelligence and evolutionary history.</span></p><p>Now that you, over the last 20 years of doing AI research, you maybe have a more tangible sense of what intelligence is, what it takes to develop it. Are you more or less surprised as a result that evolution just spontaneously stumbled upon it?</p><p><strong>Andrej Karpathy </strong><em>01:33:19</em></p><p>I love Nick Lane’s books. I was just listening to his podcast on the way up here. With respect to intelligence and its evolution, it’s very, very recent. I am surprised that it evolved.</p><p>I find it fascinating to think about all the worlds out there. Say there’s a thousand planets like Earth and what they look like. I think Nick Lane was here talking about some of the earliest parts. He expects very similar life forms, roughly speaking, and bacteria-like things in most of them. There are a few breaks in there. The evolution of intelligence intuitively feels to me like it should be a fairly rare event.</p><p>Maybe you should base it on how long something has existed. If bacteria were around for 2 billion years and nothing happened, then going to eukaryote is probably pretty hard because bacteria came up quite early in Earth’s evolution or history. How long have we had animals? Maybe a couple hundred million years, multicellular animals that run around, crawl, et cetera. That’s maybe 10% of Earth’s lifespan. Maybe on that timescale it’s not too tricky. It’s still surprising to me, intuitively, that it developed.  I would maybe expect just a lot of animal-like life forms doing animal-like things. The fact that you can get something that creates culture and knowledge and accumulates it is surprising to me.</p><p><strong>Dwarkesh Patel </strong><em>01:34:42</em></p><p>There’s a couple of interesting follow-ups. If you buy the Sutton perspective that the crux of intelligence is animal intelligence… The quote he said is “If you got to the squirrel, you’d be most of the way to AGI.”</p><p><span>We got to squirrel intelligence right after the </span><a href="https://en.wikipedia.org/wiki/Cambrian_explosion" rel>Cambrian explosion</a><span> 600 million years ago. It seems like what instigated that was the </span><a href="https://en.wikipedia.org/wiki/Great_Oxidation_Event" rel>oxygenation event</a><span> 600 million years ago. But immediately the intelligence algorithm was there to make the squirrel intelligence. It’s suggestive that animal intelligence was like </span><em>that</em><span>. As soon as you had the oxygen in the environment, you had the eukaryote, you could just get the algorithm. Maybe it was an accident that evolution stumbled upon it so fast, but I don’t know if that suggests that at the end it’s going to be quite simple.</span></p><p><strong>Andrej Karpathy </strong><em>01:35:31</em></p><p>It’s so hard to tell with any of this stuff. You can base it a bit on how long something has existed or how long it feels like something has been bottlenecked. Nick Lane is very good about describing this very apparent bottleneck in bacteria and archaea. For two billion years, nothing happened. There’s extreme diversity of biochemistry, and yet nothing grows to become animals. Two billion years.</p><p>I don’t know that we’ve seen exactly that kind of an equivalent with animals and intelligence, to your point. We could also look at it with respect to how many times we think certain intelligence has individually sprung up.</p><p><strong>Dwarkesh Patel </strong><em>01:36:07</em></p><p>That’s a really good thing to investigate.</p><p><strong>Andrej Karpathy </strong><em>01:36:09</em></p><p>One thought on that. There’s hominid intelligence, and then there’s bird intelligence. Ravens, etc., are extremely clever, but their brain parts are quite distinct, and we don’t have that much in common. That’s a slight indication of maybe intelligence springing up a few times. In that case, you’d expect it more frequently.</p><p><strong>Dwarkesh Patel </strong><em>01:36:32</em></p><p><span>A former guest, </span><a href="https://www.dwarkesh.com/p/gwern-branwen" rel>Gwern</a><span>, and </span><a href="https://www.dwarkesh.com/p/carl-shulman" rel>Carl Shulman</a><span>, they’ve made a really interesting point about that. Their perspective is that the scalable algorithm which humans have and primates have, arose in birds as well, and maybe other times as well. But humans found an evolutionary niche which rewarded marginal increases in intelligence and also had a scalable brain algorithm that could achieve those increases in intelligence.</span></p><p>For example, if a bird had a bigger brain, it would just collapse out of the air. It’s very smart for the size of its brain, but it’s not in a niche which rewards the brain getting bigger. It’s maybe similar to some really smart…</p><p><strong>Andrej Karpathy</strong></p><p>Like dolphins?</p><p><strong>Dwarkesh Patel</strong></p><p>Exaclty, humans, we have hands that reward being able to learn how to do tool use. We can externalize digestion, more energy to the brain, and that kicks off the flywheel.</p><p><strong>Andrej Karpathy </strong><em>01:37:28</em></p><p>Also stuff to work with. I’m guessing it would be harder if I were a dolphin. How do you have fire? The universe of things you can do in water, inside water, is probably lower than what you can do on land, just chemically.</p><p>I do agree with this viewpoint of these niches and what’s being incentivized. I still find it miraculous. I would have expected things to get stuck on animals with bigger muscles. Going through intelligence is a really fascinating breaking point.</p><p><strong>Dwarkesh Patel </strong><em>01:38:02</em></p><p>The way Gwern put it is the reason it was so hard is that it’s a very tight line between being in a situation where something is so important to learn that it’s not worth distilling the exact right circuits directly back into your DNA, versus it’s not important enough to learn at all. It has to be something that incentivizes building the algorithm to learn in a lifetime.</p><p><strong>Andrej Karpathy </strong><em>01:38:28</em></p><p>You have to incentivize some kind of adaptability. You want environments that are unpredictable so evolution can’t bake your algorithms into your weights. A lot of animals are pre-baked in this sense. Humans have to figure it out at test time when they get born. You want these environments that change really rapidly, where you can’t foresee what will work well. You create intelligence to figure it out at test time.</p><p><strong>Dwarkesh Patel </strong><em>01:38:55</em></p><p><a href="https://www.alignmentforum.org/users/quintin-pope" rel>Quintin Pope</a><span> had </span><a href="https://www.openphilanthropy.org/wp-content/uploads/Evolution-provides-no-evidence-for-the-sharp-left-turn-LessWrong-Quintin-Pope.pdf" rel>this interesting blog post</a><span> where he’s saying the reason he doesn’t expect a sharp takeoff is that humans had the sharp takeoff where 60,000 years ago we seem to have had the cognitive architectures that we have today. 10,000 years ago, </span><a href="https://en.wikipedia.org/wiki/Neolithic_Revolution" rel>agricultural revolution</a><span>, modernity. What was happening in that 50,000 years? You had to build this cultural scaffold where you can accumulate knowledge over generations.</span></p><p>This is an ability that exists for free in the way we do AI training. In many cases they are literally distilled. If you retrain a model, they can be trained on each other, they can be trained on the same pre-training corpus, they don’t literally have to start from scratch. There’s a sense in which it took humans a long time to get this cultural loop going, but it just comes for free with the way we do LLM training.</p><p><strong>Andrej Karpathy </strong><em>01:39:45</em></p><p>Yes and no. Because LLMs don’t really have the equivalent of culture. Maybe we’re giving them way too much and incentivizing not to create it or something like that. But the invention of culture and of written record and of passing down notes between each other, I don’t think there’s an equivalent of that with LLMs right now. LLMs don’t really have culture right now and it’s one of the impediments I would say.</p><p><strong>Dwarkesh Patel </strong><em>01:40:05</em></p><p>Can you give me some sense of what LLM culture might look like?</p><p><strong>Andrej Karpathy </strong><em>01:40:09</em></p><p>In the simplest case it would be a giant scratchpad that the LLM can edit and as it’s reading stuff or as it’s helping out with work, it’s editing the scratchpad for itself. Why can’t an LLM write a book for the other LLMs? That would be cool. Why can’t other LLMs read this LLM’s book and be inspired by it or shocked by it or something like that? There’s no equivalence for any of this stuff.</p><p><strong>Dwarkesh Patel </strong><em>01:40:29</em></p><p><span>Interesting. When would you expect that kind of thing to start happening? Also, </span><a href="https://en.wikipedia.org/wiki/Multi-agent_system" rel>multi-agent systems</a><span> and a sort of independent AI civilization and culture?</span></p><p><strong>Andrej Karpathy </strong><em>01:40:40</em></p><p>There are two powerful ideas in the realm of multi-agent that have both not been really claimed or so on. The first one I would say is culture and LLMs having a growing repertoire of knowledge for their own purposes.</p><p><span>The second one looks a lot more like the powerful idea of </span><a href="https://en.wikipedia.org/wiki/Self-play" rel>self-play</a><span>. In my mind it’s extremely powerful. Evolution has a lot of competition driving intelligence and evolution. In </span><a href="https://en.wikipedia.org/wiki/AlphaGo" rel>AlphaGo</a><span> more algorithmically, AlphaGo is playing against itself and that’s how it learns to get really good at Go. There’s no equivalent of self-playing LLMs, but I would expect that to also exist. No one has done it yet. Why can’t an LLM for example, create a bunch of problems that another LLM is learning to solve? Then the LLM is always trying to serve more and more difficult problems, stuff like that.</span></p><p>There’s a bunch of ways to organize it. It’s a realm of research, but I haven’t seen anything that convincingly claims both of those multi-agent improvements. We’re mostly in the realm of a single individual agent, but that will change. In the realm of culture also, I would also bucket organizations. We haven’t seen anything like that convincingly either. That’s why we’re still early.</p><p><strong>Dwarkesh Patel </strong><em>01:41:53</em></p><p>Can you identify the key bottleneck that’s preventing this kind of collaboration between LLMs?</p><p><strong>Andrej Karpathy </strong><em>01:41:59</em></p><p>Maybe the way I would put it is, some of these analogies work and they shouldn’t, but somehow, remarkably, they do. A lot of the smaller models, or the dumber models, remarkably resemble a kindergarten student, or an elementary school student or high school student. Somehow, we still haven’t graduated enough where this stuff can take over. My Claude Code or Codex, they still feel like this elementary-grade student. I know that they can take PhD quizzes, but they still cognitively feel like a kindergarten or an elementary school student.</p><p>I don’t think they can create culture because they’re still kids. They’re savant kids. They have perfect memory of all this stuff. They can convincingly create all kinds of slop that looks really good. But I still think they don’t really know what they’re doing and they don’t really have the cognition across all these little checkboxes that we still have to collect.</p><h3 class="header-anchor-post">01:42:55 - Why self driving took so long<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§why-self-driving-took-so-long" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/why-self-driving-took-so-long" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel</strong><span> </span><em>01:42:55</em></p><p>You’ve talked about how you were at Tesla leading self-driving from 2017 to 2022. And you firsthand saw this progress from cool demos to now thousands of cars out there actually autonomously doing drives. Why did that take a decade? What was happening through that time?</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:43:11</em></p><p><span>One thing I will almost instantly push back on is that this is not even near done, in a bunch of ways that I’m going to get to. Self-driving is very interesting because it’s definitely where I get a lot of my intuitions because I spent five years on it. It has this </span><a href="https://en.wikipedia.org/wiki/Self-driving_car#History" rel>entire history</a><span> where the first demos of self-driving go all the way to the 1980s. You can see a </span><a href="https://en.wikipedia.org/wiki/Navlab" rel>demo from CMU in 1986</a><span>. There’s a truck that’s driving itself on roads.</span></p><p>Fast forward. When I was joining Tesla, I had a very early demo of Waymo. It basically gave me a perfect drive in 2014 or something like that, so a perfect Waymo drive a decade ago. It took us around Palo Alto and so on because I had a friend who worked there. I thought it was very close and then it still took a long time.</p><p>For some kinds of tasks and jobs and so on, there’s a very large demo-to-product gap where the demo is very easy, but the product is very hard. It’s especially the case in cases like self-driving where the cost of failure is too high. Many industries, tasks, and jobs maybe don’t have that property, but when you do have that property, that definitely increases the timelines.</p><p>For example, in software engineering, I do think that property does exist. For a lot of vibe coding, it doesn’t. But if you’re writing actual production-grade code, that property should exist, because any kind of mistake leads to a security vulnerability or something like that. Millions and hundreds of millions of people’s personal Social Security numbers get leaked or something like that. So in software, people should be careful, kind of like in self-driving. In self-driving, if things go wrong, you might get injured. There are worse outcomes. But in software, it’s almost unbounded how terrible something could be.</p><p><span>I do think that they share that property. What takes the long amount of time and the way to think about it is that it’s a march of </span><a href="https://en.wikipedia.org/wiki/High_availability#%22Nines%22" rel>nines</a><span>. Every single nine is a constant amount of work. Every single nine is the same amount of work. When you get a demo and something works 90% of the time, that’s just the first nine. Then you need the second nine, a third nine, a fourth nine, a fifth nine. While I was at Tesla for five years or so, we went through maybe three nines or two nines. I don’t know what it is, but multiple nines of iteration. There are still more nines to go.</span></p><p>That’s why these things take so long. It’s definitely formative for me, seeing something that was a demo. I’m very unimpressed by demos. Whenever I see demos of anything, I’m extremely unimpressed by that. If it’s a demo that someone cooked up just to show you, it’s worse. If you can interact with it, it’s a bit better. But even then, you’re not done. You need the actual product. It’s going to face all these challenges when it comes in contact with reality and all these different pockets of behavior that need patching.</p><p>We’re going to see all this stuff play out. It’s a march of nines. Each nine is constant. Demos are encouraging. It’s still a huge amount of work to do. It is a critical safety domain, unless you’re doing vibe coding, which is all nice and fun and so on. That’s why this also enforced my timelines from that perspective.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:46:25</em></p><p>It’s very interesting to hear you say that, that the safety guarantees you need from software are not dissimilar to self-driving. What people will often say is that self-driving took so long because the cost of failure is so high. A human makes a mistake on average every 400,000 miles or every seven years. If you had to release a coding agent that couldn’t make a mistake for at least seven years, it would be much harder to deploy.</p><p>But your point is that if you made a catastrophic coding mistake, like breaking some important system every seven years...</p><p><strong>Andrej Karpathy </strong><em>01:46:56</em></p><p>Very easy to do.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:46:57</em></p><p>In fact, in terms of wall clock time, it would be much less than seven years because you’re constantly outputting code like that. In terms of tokens, it would be seven years. But in terms of wall clock time...</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:47:09</em></p><p>In some ways, it’s a much harder problem. Self-driving is just one of thousands of things that people do. It’s almost like a single vertical, I suppose. Whereas when we’re talking about general software engineering, it’s even more... There’s more surface area.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:47:20</em></p><p>There’s another objection people make to that analogy, which is that with self-driving, what took a big fraction of that time was solving the problem of having basic perception that’s robust, building representations, and having a model that has some common sense so it can generalize to when it sees something that’s slightly out of distribution. If somebody’s waving down the road this way, you don’t need to train for it. The thing will have some understanding of how to respond to something like that.</p><p><span>These are things we’re getting for free with LLMs or </span><a href="https://www.nvidia.com/en-us/glossary/vision-language-models/" rel>VLMs</a><span> today, so we don’t have to solve these very basic representation problems. So now deploying AIs across different domains will sort of be like deploying a self-driving car with current models to a different city, which is hard but not like a 10-year-long task.</span></p><p><strong>Andrej Karpathy</strong><span> </span><em>01:48:07</em></p><p>I’m not 100% sure if I fully agree with that. I don’t know how much we’re getting for free. There’s still a lot of gaps in understanding what we are getting. We’re definitely getting more generalizable intelligence in a single entity, whereas self-driving is a very special-purpose task that requires. In some sense building a special-purpose task is maybe even harder in a certain sense because it doesn’t fall out from a more general thing that you’re doing at scale, if that makes sense.</p><p>But the analogy still doesn’t fully resonate because the LLMs are still pretty fallible and they have a lot of gaps that still need to be filled in. I don’t think that we’re getting magical generalization completely out of the box, in a certain sense.</p><p>The other aspect that I wanted to return to is that self-driving cars are nowhere near done still. The deployments are pretty minimal. Even Waymo and so on has very few cars. They’re doing that roughly speaking because they’re not economical. They’ve built something that lives in the future. They’ve had to pull back the future, but they had to make it uneconomical. There are all these costs, not just marginal costs for those cars and their operation and maintenance, but also the capex of the entire thing. Making it economical is still going to be a slog for them.</p><p>Also, when you look at these cars and there’s no one driving, I actually think it’s a little bit deceiving because there are very elaborate teleoperation centers of people kind of in a loop with these cars. I don’t have the full extent of it, but there’s more human-in-the-loop than you might expect. There are people somewhere out there beaming in from the sky. I don’t know if they’re fully in the loop with the driving. Some of the time they are, but they’re certainly involved and there are people. In some sense, we haven’t actually removed the person, we’ve moved them to somewhere where you can’t see them.</p><p>I still think there will be some work, as you mentioned, going from environment to environment. There are still challenges to make self-driving real. But I do agree that it’s definitely crossed a threshold where it kind of feels real, unless it’s really teleoperated. For example, Waymo can’t go to all the different parts of the city. My suspicion is that it’s parts of the city where you don’t get good signal. Anyway, I don’t know anything about the stack. I’m just making stuff up.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:50:23</em></p><p>You led self-driving for five years at Tesla.</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:50:27</em></p><p>Sorry, I don’t know anything about the specifics of Waymo. By the way, I love Waymo and I take it all the time. I just think that people are sometimes a little bit too naive about some of the progress and there’s still a huge amount of work. Tesla took in my mind a much more scalable approach and the team is doing extremely well. I’m kind of on the record for predicting how this thing will go. Waymo had an early start because you can package up so many sensors. But I do think Tesla is taking the more scalable strategy and it’s going to look a lot more like that. So this will still have to play out and hasn’t. But I don’t want to talk about self-driving as something that took a decade because it didn’t take it yet, if that makes sense.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:51:08</em></p><p>Because one, the start is at 1980 and not 10 years ago, and then two, the end is not here yet.</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:51:14</em></p><p>The end is not near yet because when we’re talking about self-driving, usually in my mind it’s self-driving at scale. People don’t have to get a driver’s license, etc.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:51:22</em></p><p>I’m curious to bounce two other ways in which the analogy might be different. The reason I’m especially curious about this is because the question of how fast AI is deployed, how valuable it is when it’s early on is potentially the most important question in the world right now. If you’re trying to model what the year 2030 looks like, this is the question you ought to have some understanding of.</p><p>Another thing you might think is one, you have this latency requirement with self-driving. I have no idea what the actual models are, but I assume it’s like tens of millions of parameters or something, which is not the necessary constraint for knowledge work with LLMs. Maybe it might be with computer use and stuff.</p><p>But the other big one is, maybe more importantly, on this capex question. Yes, there is additional cost to serving up an additional copy of a model, but the opex of a session is quite low and you can amortize the cost of AI into the training run itself, depending on how inference scaling goes and stuff. But it’s certainly not as much as building a whole new car to serve another instance of a model. So the economics of deploying more widely are much more favorable.</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:52:37</em></p><p>I think that’s right. If you’re sticking to the realm of bits, bits are a million times easier than anything that touches the physical world. I definitely grant that. Bits are completely changeable, arbitrarily reshuffleable at a very rapid speed. You would expect a much faster adaptation also in the industry and so on. What was the first one?</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:52:59</em></p><p>The latency requirements and its implications for model size?</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:53:02</em></p><p>I think that’s roughly right. I also think that if we are talking about knowledge work at scale, there will be some latency requirements, practically speaking, because we’re going to have to create a huge amount of compute and serve that.</p><p>The last aspect that I very briefly want to also talk about is all the rest of it. What does society think about it? What are the legal ramifications? How is it working legally? How is it working insurance-wise? What are those layers of it and aspects of it? What is the equivalent of people putting a cone on a Waymo? There are going to be equivalents of all that. So I feel like self-driving is a very nice analogy that you can borrow things from. What is the equivalent of a cone in the car? What is the equivalent of a teleoperating worker who’s hidden away and all the aspects of it.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:53:53</em></p><p>Do you have any opinions on what this implies about the current AI buildout, which would 10x the amount of available compute in the world in a year or two and maybe more than 100x it by the end of the decade. If the use of AI will be lower than some people naively predict, does that mean that we’re overbuilding compute or is that a separate question?</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:54:15</em></p><p><span>Kind of like what happened with </span><a href="https://www.focus-economics.com/blog/railway-mania-the-largest-speculative-bubble-you-never-heard-of/" rel>railroads</a><span>.</span></p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:54:18</em></p><p>With what, sorry?</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:54:19</em></p><p>Was it railroads or?</p><p><strong>Dwarkesh Patel</strong><span> </span><em>01:54:20</em></p><p>Yeah, it was.</p><p><strong>Andrej Karpathy</strong><span> </span><em>01:54:21</em></p><p><span>Yeah. There’s historical precedent. Or was it with the telecommunication industry? Pre-paving the internet that only came a decade later and creating a </span><a href="https://www.fabricatedknowledge.com/p/lessons-from-history-the-rise-and" rel>whole bubble in the telecommunications industry</a><span> in the late ‘90s.</span></p><p>I understand I’m sounding very pessimistic here. I’m actually optimistic. I think this will work. I think it’s tractable. I’m only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me. There’s a lot of reasons for why that exists. A lot of it is honestly just fundraising. It’s just incentive structures. A lot of it may be fundraising. A lot of it is just attention, converting attention to money on the internet, stuff like that. There’s a lot of that going on, and I’m only reacting to that.</p><p>But I’m still overall very bullish on technology. We’re going to work through all this stuff. There’s been a rapid amount of progress. I don’t know that there’s overbuilding. I think we’re going to be able to gobble up what, in my understanding, is being built. For example, Claude Code or OpenAI Codex and stuff like that didn’t even exist a year ago. Is that right? This is a miraculous technology that didn’t exist. There’s going to be a huge amount of demand, as we see the demand in ChatGPT already and so on.</p><p>So I don’t know that there’s overbuilding. I’m just reacting to some of the very fast timelines that people continue to say incorrectly. I’ve heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. I want this to be properly calibrated, and some of this also has geopolitical ramifications and things like that with some of these questions. I don’t want people to make mistakes in that sphere of things. I do want us to be grounded in the reality of what technology is and isn’t.</p><h3 class="header-anchor-post">01:56:20 - Future of education<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§future-of-education" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://www.dwarkesh.com/i/176425744/future-of-education" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p><strong>Dwarkesh Patel </strong><em>01:56:20</em></p><p><span>Let’s talk about education and </span><a href="https://eurekalabs.ai/" rel>Eureka</a><span>. One thing you could do is start another AI lab and then try to solve those problems. I’m curious what you’re up to now, and why not AI research itself?</span></p><p><strong>Andrej Karpathy </strong><em>01:56:33</em></p><p><span>I guess the way I would put it is I feel some amount of determinism around the things that AI labs are doing. I feel like I could help out there, but I don’t know that I would uniquely improve it. My personal big fear is that a lot of this stuff happens on the side of humanity, and that humanity gets disempowered by it. I care not just about all the </span><a href="https://en.wikipedia.org/wiki/Dyson_sphere" rel>Dyson spheres</a><span> that we’re going to build and that AI is going to build in a fully autonomous way, I care about what happens to humans. I want humans to be well off in the future.</span></p><p><span>I feel like that’s where I can a lot more uniquely add value than an incremental improvement in the frontier lab. I’m most afraid of something depicted in movies like </span><em>WALL-E</em><span> or </span><em>Idiocracy</em><span> or something like that, where humanity is on the side of this stuff. I want humans to be much, much better in this future. To me, this is through education that you can achieve this.</span></p><p><strong>Dwarkesh Patel </strong><em>01:57:35</em></p><p>So what are you working on there?</p><p><strong>Andrej Karpathy </strong><em>01:57:36</em></p><p><span>The easiest way I can describe it is we’re trying to build the </span><a href="https://memory-alpha.fandom.com/wiki/Starfleet_Academy" rel>Starfleet Academy</a><span>. I don’t know if you’ve watched </span><em>Star Trek</em><span>.</span></p><p><strong>Dwarkesh Patel </strong><em>01:57:44</em></p><p>I haven’t.</p><p><strong>Andrej Karpathy </strong><em>01:57:44</em></p><p>Starfleet Academy is this elite institution for frontier technology, building spaceships, and graduating cadets to be the pilots of these spaceships and whatnot. So I just imagine an elite institution for technical knowledge and a kind of school that’s very up-to-date and a premier institution.</p><p><strong>Dwarkesh Patel </strong><em>01:58:05</em></p><p><span>A category of questions I have for you is explaining how one teaches technical or scientific content well, because you are one of the world masters at it. I’m curious both about how you think about it for content you’ve already put out there on </span><a href="https://www.youtube.com/andrejkarpathy" rel>YouTube</a><span>, but also, to the extent it’s any different, how you think about it for Eureka.</span></p><p><strong>Andrej Karpathy </strong><em>01:58:25</em></p><p>With respect to Eureka, one thing that is very fascinating to me about education is that I do think education will pretty fundamentally change with AIs on the side. It has to be rewired and changed to some extent.</p><p>I still think that we’re pretty early. There’s going to be a lot of people who are going to try to do the obvious things. Have an LLM and ask it questions. Do all the basic things that you would do via prompting right now. It’s helpful, but it still feels to me a bit like slop. I’d like to do it properly, and I think the capability is not there for what I would want. What I’d want is an actual tutor experience.</p><p>A prominent example in my mind is I was recently learning Korean, so language learning. I went through a phase where I was learning Korean by myself on the internet. I went through a phase where I was part of a small class in Korea taking Korean with a bunch of other people, which was really funny. We had a teacher and 10 people or so taking Korean. Then I switched to a one-on-one tutor.</p><p>I guess what was fascinating to me was, I think I had a really good tutor, but just thinking through what this tutor was doing for me and how incredible that experience was and how high the bar is for what I want to build eventually. Instantly from a very short conversation, she understood where I am as a student, what I know and don’t know. She was able to probe exactly the kinds of questions or things to understand my world model. No LLM will do that for you 100% right now, not even close. But a tutor will do that if they’re good. Once she understands, she really served me all the things that I needed at my current sliver of capability. I need to be always appropriately challenged. I can’t be faced with something too hard or too trivial, and a tutor is really good at serving you just the right stuff.</p><p>I felt like I was the only constraint to learning. I was always given the perfect information. I’m the only constraint. I felt good because I’m the only impediment that exists. It’s not that I can’t find knowledge or that it’s not properly explained or etc. It’s just my ability to memorize and so on. This is what I want for people.</p><p><strong>Dwarkesh Patel </strong><em>02:00:27</em></p><p>How do you automate that?</p><p><strong>Andrej Karpathy </strong><em>02:00:29</em></p><p>Very good question. At the current capability, you don’t. That’s why I think it’s not actually the right time to build this kind of an AI tutor. I still think it’s a useful product, and lots of people will build it, but the bar is so high and the capability is not there. Even today, I would say ChatGPT is an extremely valuable educational product. But for me, it was so fascinating to see how high the bar is. When I was with her, I almost felt like there’s no way I can build this.</p><p><strong>Dwarkesh Patel </strong><em>02:01:02</em></p><p>But you are building it, right?</p><p><strong>Andrej Karpathy </strong><em>02:01:03</em></p><p>Anyone who’s had a really good tutor is like, “How are you going to build this?” I’m waiting for that capability.</p><p>I did some AI consulting for computer vision. A lot of times, the value that I brought to the company was telling them not to use AI. I was the AI expert, and they described the problem, and I said, “Don’t use AI.” This is my value add. I feel like it’s the same in education right now, where I feel like for what I have in mind, it’s not yet the time, but the time will come. For now, I’m building something that looks maybe a bit more conventional that has a physical and digital component and so on. But it’s obvious how this should look in the future.</p><p><strong>Dwarkesh Patel </strong><em>02:01:43</em></p><p>To the extent you’re willing to say, what is the thing you hope will be released this year or next year?</p><p><strong>Andrej Karpathy </strong><em>02:01:49</em></p><p><span>I’m building the first course. I want to have a really, really good course, the obvious state-of-the-art destination you go to to learn, AI in this case. That’s just what I’m familiar with, so it’s a really good first product to get to be really good at it. So that’s what I’m building. Nanochat, which you briefly mentioned, is a capstone project of </span><a href="https://github.com/karpathy/LLM101n" rel>LLM101N</a><span>, which is a class that I’m building. That’s a really big piece of it. But now I have to build out a lot of the intermediates, and then I have to hire a small team of TAs and so on and build the entire course.</span></p><p>One more thing that I would say is that many times, when people think about education, they think more about what I would say is a softer component of diffusing knowledge. I have something very hard and technical in mind. In my mind, education is the very difficult technical process of building ramps to knowledge. In my mind, nanochat is a ramp to knowledge because it’s very simple. It’s the super simplified full-stack thing. If you give this artifact to someone and they look through it, they’re learning a ton of stuff. It’s giving you a lot of what I call eurekas per second, which is understanding per second. That’s what I want, lots of eurekas per second. So to me, this is a technical problem of how do we build these ramps to knowledge.</p><p>So I almost think of Eureka as maybe not that different from some of the frontier labs or some of the work that’s going on there. I want to figure out how to build these ramps very efficiently so that people are never stuck and everything is always not too hard or not too trivial, and you have just the right material to progress.</p><p><strong>Dwarkesh Patel </strong><em>02:03:25</em></p><p>You’re imagining in the short term that instead of a tutor being able to probe your understanding, if you have enough self-awareness to be able to probe yourself, you’re never going to be stuck. You can find the right answer between talking to the TA or talking to an LLM and looking at the reference implementation. It sounds like automation or AI is not a significant part. So far, the big alpha here is your ability to explain AI codified in the source material of the class. That’s fundamentally what the course is.</p><p><strong>Andrej Karpathy </strong><em>02:04:00</em></p><p>You always have to be calibrated to what capability exists in the industry. A lot of people are going to pursue just asking ChatGPT, etc. But I think right now, for example, if you go to ChatGPT and you say, teach me AI, there’s no way. It’s going to give you some slop. AI is never going to write nanochat right now. But nanochat is a really useful intermediate point. I’m collaborating with AI to create all this material, so AI is still fundamentally very helpful.</p><p><span>Earlier on, I built </span><a href="https://cs231n.stanford.edu/" rel>CS231n</a><span> at Stanford, which I think was the first deep learning class at Stanford, which became very popular. The difference in building out 231n then and LLM101N now is quite stark. I feel really empowered by the LLMs as they exist right now, but I’m very much in the loop. They’re helping me build the materials, I go much faster. They’re doing a lot of the boring stuff, etc. I feel like I’m developing the course much faster, and it’s LLM-infused, but it’s not yet at a place where it can creatively create the content. I’m still there to do that. The trickiness is always calibrating yourself to what exists.</span></p><p><strong>Dwarkesh Patel </strong><em>02:05:04</em></p><p>When you imagine what is available through Eureka in a couple of years, it seems like the big bottleneck is going to be finding Karpathys in field after field who can convert their understanding into these ramps.</p><p><strong>Andrej Karpathy </strong><em>02:05:18</em></p><p>It would change over time. Right now, it would be hiring faculty to help work hand-in-hand with AI and a team of people probably to build state-of-the-art courses. Over time maybe some of the TAs can become AIs. You just take all the course materials and then I think you could serve a very good automated TA for the student when they have more basic questions or something like that. But I think you’ll need faculty for the overall architecture of a course and making sure that it fits. So I see a progression of how this will evolve. Maybe at some future point I’m not even that useful and AI is doing most of the design much better than I could. But I still think that’s going to take some time to play out.</p><p><strong>Dwarkesh Patel </strong><em>02:05:59</em></p><p><span>Are you imagining that people who have expertise in other fields are then contributing courses, or do you feel like it’s quite essential to the vision that you, given your understanding of how you want to teach, are the one designing the content? </span><a href="https://en.wikipedia.org/wiki/Sal_Khan" rel>Sal Khan</a><span> is narrating all the videos on </span><a href="http://khanacademy.org/" rel>Khan Academy</a><span>. Are you imagining something like that?</span></p><p><strong>Andrej Karpathy </strong><em>02:06:20</em></p><p>No, I will hire faculty because there are domains in which I’m not an expert. That’s the only way to offer the state-of-the-art experience for the student ultimately. I do expect that I would hire faculty, but I will probably stick around in AI for some time. I do have something more conventional in mind for the current capability than what people would probably anticipate.</p><p>When I’m building Starfleet Academy, I do probably imagine a physical institution, and maybe a tier below that a digital offering that is not the state-of-the-art experience you would get when someone comes in physically full-time and we work through material from start to end and make sure you understand it. That’s the physical offering. The digital offering is a bunch of stuff on the internet and maybe some LLM assistant. It’s a bit more gimmicky in a tier below, but at least it’s accessible to 8 billion people.</p><p><strong>Dwarkesh Patel </strong><em>02:07:08</em></p><p>I think you’re basically inventing college from first principles for the tools that are available today and just selecting for people who have the motivation and the interest of really engaging with material.</p><p><strong>Andrej Karpathy </strong><em>02:07:26</em></p><p>There’s going to have to be a lot of not just education but also re-education. I would love to help out there because the jobs will probably change quite a bit. For example, today a lot of people are trying to upskill in AI specifically. I think it’s a really good course to teach in this respect. Motivation-wise, before AGI motivation is very simple to solve because people want to make money. This is how you make money in the industry today. Post-AGI is a lot more interesting possibly because if everything is automated and there’s nothing to do for anyone, why would anyone go to a school?</p><p>I often say that pre-AGI education is useful. Post-AGI education is fun. In a similar way, people go to the gym today. We don’t need their physical strength to manipulate heavy objects because we have machines that do that. They still go to the gym. Why do they go to the gym? Because it’s fun, it’s healthy, and you look hot when you have a six-pack. It’s attractive for people to do that in a very deep, psychological, evolutionary sense for humanity. Education will play out in the same way. You’ll go to school like you go to the gym.</p><p>Right now, not that many people learn because learning is hard. You bounce from material. Some people overcome that barrier, but for most people, it’s hard. It’s a technical problem to solve. It’s a technical problem to do what my tutor did for me when I was learning Korean. It’s tractable and buildable, and someone should build it. It’s going to make learning anything trivial and desirable, and people will do it for fun because it’s trivial. If I had a tutor like that for any arbitrary piece of knowledge, it’s going to be so much easier to learn anything, and people will do it. They’ll do it for the same reasons they go to the gym.</p><p><strong>Dwarkesh Patel </strong><em>02:09:17</em></p><p>That sounds different from using… So post-AGI, you’re using this as entertainment or as self-betterment. But it sounded like you had a vision also that this education is relevant to keeping humanity in control of AI. That sounds different. Is it entertaining for some people, but then empowerment for some others? How do you think about that?</p><p><strong>Andrej Karpathy </strong><em>02:09:41</em></p><p>I do think eventually it’s a bit of a losing game, if that makes sense. It is in the long term. In the long term, which is longer than maybe most people in the industry think about, it’s a losing game. I do think people can go so far and we’ve barely scratched the surface of how much a person can go. That’s just because people are bouncing off of material that’s too easy or too hard. People will be able to go much further. Anyone will speak five languages because why not? Because it’s so trivial. Anyone will know all the basic curriculum of undergrad, et cetera.</p><p><strong>Dwarkesh Patel </strong><em>02:10:18</em></p><p>Now that I’m understanding the vision, that’s very interesting. It has a perfect analog in gym culture. I don’t think 100 years ago anybody would be ripped. Nobody would have been able to just spontaneously bench two plates or three plates or something. It’s very common now because of this idea of systematically training and lifting weights in the gym, or systematically training to be able to run a marathon, which is a capability most humans would not spontaneously have. You’re imagining similar things for learning across many different domains, much more intensely, deeply, faster.</p><p><strong>Andrej Karpathy </strong><em>02:10:54</em></p><p>Exactly. I am betting a bit implicitly on some of the timelessness of human nature. It will be desirable to do all these things, and I think people will look up to it as they have for millennia. This will continue to be true. There’s some evidence of that historically. If you look at, for example, aristocrats, or you look at ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, people have spent a lot of their time flourishing in a certain way, either physically or cognitively. I feel okay about the prospects of that.</p><p><span>If this is false and I’m wrong and we end up in a </span><em>WALL-E</em><span> or </span><em>Idiocracy</em><span> future, then I don’t even care if there are Dyson spheres. This is a terrible outcome. I really do care about humanity. Everyone has to just be superhuman in a certain sense.</span></p><p><strong>Dwarkesh Patel </strong><em>02:11:52</em></p><p>It’s still a world in which that is not enabling us to… It’s like the culture world, right? You’re not fundamentally going to be able to transform the trajectory of technology or influence decisions by your own labor or cognition alone. Maybe you can influence decisions because the AI is asking for your approval, but it’s not because I’ve invented something or I’ve come up with a new design that I’m really influencing the future.</p><p><strong>Andrej Karpathy </strong><em>02:12:21</em></p><p>Maybe. I think there will be a transitional period where we are going to be able to be in the loop and advance things if we understand a lot of stuff. In the long-term, that probably goes away. It might even become a sport. Right now you have powerlifters who go extreme in this direction. What is powerlifting in a cognitive era? Maybe it’s people who are really trying to make Olympics out of knowing stuff. If you have a perfect AI tutor, maybe you can get extremely far. I feel that the geniuses of today are barely scratching the surface of what a human mind can do, I think.</p><p><strong>Dwarkesh Patel </strong><em>02:12:59</em></p><p>I love this vision. I also feel like the person you have the most product-market fit with is me because my job involves having to learn different subjects every week, and I am very excited.</p><p><strong>Andrej Karpathy </strong><em>02:13:17</em></p><p>I’m similar, for that matter. A lot of people, for example, hate school and want to get out of it. I really liked school. I loved learning things, et cetera. I wanted to stay in school. I stayed all the way until Ph.D. and then they wouldn’t let me stay longer, so I went to the industry. Roughly speaking, I love learning, even for the sake of learning, but I also love learning because it’s a form of empowerment and being useful and productive.</p><p><strong>Dwarkesh Patel </strong><em>02:13:39</em></p><p>You also made a point that was subtle and I want to spell it out. With what’s happened so far with online courses, why haven’t they already enabled us to enable every single human to know everything? They’re just so motivation-laden because there are no obvious on-ramps and it’s so easy to get stuck. If you had this thing instead—like a really good human tutor—it would just be such an unlock from a motivation perspective.</p><p><strong>Andrej Karpathy </strong><em>02:14:10</em></p><p>I think so. It feels bad to bounce from material. It feels bad. You get negative reward from sinking an amount of time in something and it doesn’t pan out, or being completely bored because what you’re getting is too easy or too hard. When you do it properly, learning feels good. It’s a technical problem to get there. For a while, it’s going to be AI plus human collab, and at some point, maybe it’s just AI.</p><p><strong>Dwarkesh Patel </strong><em>02:14:36</em></p><p>Can I ask some questions about teaching well? If you had to give advice to another educator in another field that you’re curious about to make the kinds of YouTube tutorials you’ve made. Maybe it might be especially interesting to talk about domains where you can’t test someone’s technical understanding by having them code something up or something. What advice would you give them?</p><p><strong>Andrej Karpathy </strong><em>02:14:58</em></p><p>That’s a pretty broad topic. There are 10–20 tips and tricks that I semi-consciously do probably. But a lot of this comes from my physics background. I really, really did enjoy my physics background. I have a whole rant on how everyone should learn physics in early school education because early school education is not about accumulating knowledge or memory for tasks later in the industry. It’s about booting up a brain. Physics uniquely boots up the brain the best because some of the things that they get you to do in your brain during physics is extremely valuable later.</p><p>The idea of building models and abstractions and understanding that there’s a first-order approximation that describes most of the system, but then there’re second-order, third-order, fourth-order terms that may or may not be present. The idea that you’re observing a very noisy system, but there are these fundamental frequencies that you can abstract away. When a physicist walks into the class and they say, “Assume there’s a spherical cow,” everyone laughs at that, but this is brilliant. It’s brilliant thinking that’s very generalizable across the industry because a cow can be approximated as a sphere in a bunch of ways.</p><p><span>There’s a really good book, for example, </span><em><a href="https://amzn.to/47ilBeP" rel>Scale</a></em><span>. It’s from a </span><a href="https://en.wikipedia.org/wiki/Geoffrey_West" rel>physicist</a><span> talking about biology. Maybe this is also a book I would recommend reading. You can get a lot of really interesting approximations and chart scaling laws of animals. You can look at their </span><a href="https://johnmjennings.com/animal-size-heartbeats-and-longevity/" rel>heartbeats</a><span> and things like that, and they line up with the size of the animal and things like that. You can talk about an animal as a volume. You can talk about the heat dissipation of that, because your heat dissipation grows as the surface area, which is growing as a square. But your heat creation or generation is growing as a cube. So I just feel like physicists have all the right cognitive tools to approach problem solving in the world.</span></p><p>So because of that training, I always try to find the first-order terms or the second-order terms of everything. When I’m observing a system or a thing, I have a tangle of a web of ideas or knowledge in my mind. I’m trying to find, what is the thing that matters? What is the first-order component? How can I simplify it? How can I have a simplest thing that shows that thing, shows it in action, and then I can tack on the other terms?</p><p><span>Maybe an example from one of my repos that I think illustrates it well is called </span><a href="https://github.com/karpathy/micrograd" rel>micrograd</a><span>. I don’t know if you’re familiar with this. So micrograd is 100 lines of code that shows backpropagation. You can create neural networks out of simple operations like plus and times, et cetera. Lego blocks of neural networks. You build up a computational graph and you do a forward pass and a backward pass to get the gradients. Now, this is at the heart of all neural network learning.</span></p><p><span>So micrograd is a 100 lines of pretty interpretable Python code, and it can do forward and backward arbitrary neural networks, but not efficiently. So micrograd, these 100 lines of Python, are everything you need to understand how neural networks train. Everything else is just efficiency. Everything else is efficiency. There’s a huge amount of work to get efficiency. You need your </span><a href="https://en.wikipedia.org/wiki/Tensor_(machine_learning)" rel>tensors</a><span>, you lay them out, you stride them, you make sure your kernels, orchestrating memory movement correctly, et cetera. It’s all just efficiency, roughly speaking. But the core intellectual piece of neural network training is micrograd. It’s 100 lines. You can easily understand it. It’s a recursive application of </span><a href="https://en.wikipedia.org/wiki/Chain_rule" rel>chain rule</a><span> to derive the gradient, which allows you to optimize any arbitrary differentiable function.</span></p><p>So I love finding these small-order terms and serving them on a platter and discovering them. I feel like education is the most intellectually interesting thing because you have a tangle of understanding and you’re trying to lay it out in a way that creates a ramp where everything only depends on the thing before it. I find that this untangling of knowledge is just so intellectually interesting as a cognitive task. I love doing it personally, but I just have a fascination with trying to lay things out in a certain way. Maybe that helps me.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:18:41</em></p><p><span>It also makes the learning experience so much more motivated. Your tutorial on the transformer begins with </span><a href="https://en.wikipedia.org/wiki/Bigram" rel>bigrams</a><span>, literally a lookup table from, “Here’s the word right now, or here’s the previous word, here’s the next word.” It’s literally just a lookup table.</span></p><p><strong>Andrej Karpathy</strong><span> </span><em>02:18:58</em></p><p>That’s the essence of it, yeah.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:18:59</em></p><p>It’s such a brilliant way, starting with a lookup table and then going to a transformer. Each piece is motivated. Why would you add that? Why would you add the next thing? You could memorize the attention formula, but having an understanding of why every single piece is relevant, what problem it solves.</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:19:13</em></p><p>You’re presenting the pain before you present a solution, and how clever is that? You want to take the student through that progression. There are a lot of other small things that make it nice and engaging and interesting. Always prompting the student.</p><p> There’s a lot of small things like that are important and a lot of good educators will do this. How would you solve this? I’m not going to present the solution before you guess. That would be wasteful. That’s a little bit of a…I don’t want to swear but it’s a dick move towards you to present you with the solution before I give you a shot to try to come up with it yourself.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:19:51</em></p><p>Because if you try to come up with it yourself, you get a better understanding of what the action space is, what the objective is, and then why only this action fulfills that objective.</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:20:03</em></p><p>You have a chance to try it yourself, and you have an appreciation when I give you the solution. It maximizes the amount of knowledge per new fact added.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:20:11</em></p><p>Why do you think, by default, people who are genuine experts in their field are often bad at explaining it to somebody ramping up?</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:20:24</em></p><p><span>It’s the </span><a href="https://en.wikipedia.org/wiki/Curse_of_knowledge" rel>curse of knowledge</a><span> and expertise. This is a real phenomenon, and I suffered from it myself as much as I try not to. But you take certain things for granted, and you can’t put yourself in the shoes of new people who are just starting out. This is pervasive and happens to me as well.</span></p><p>One thing that’s extremely helpful. As an example, someone was trying to show me a paper in biology recently, and I just instantly had so many terrible questions. What I did was I used ChatGPT to ask the questions with the paper in the context window. It worked through some of the simple things. Then I shared the thread to the person who wrote that paper or worked on that work. I felt like if they could see the dumb questions I had, it might help them explain better in the future.</p><p>For my material, I would love it if people shared their dumb conversations with ChatGPT about the stuff that I’ve created because it really helps me put myself again in the shoes of someone who’s starting out.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:21:19</em></p><p>Another trick that just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in 100% of cases that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there’s something about communicating one-on-one with a person which compels you to just say the thing.</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:22:07</em></p><p>Just say the thing. I saw that tweet, I thought it was really good. I shared it with a bunch of people. I noticed this many, many times.</p><p>The most prominent example is that I remember back in my PhD days doing research. You read someone’s paper, and you work to understand what it’s doing. Then you catch them, you’re having beers at the conference later, and you ask them, “So this paper, what were you doing? What is the paper about?”</p><p>They will just tell you these three sentences that perfectly captured the essence of that paper and totally give you the idea. And you didn’t have to read the paper. It’s only when you’re sitting at the table with a beer or something, and they’re like, “Oh yeah, the paper is just, you take this idea, you take that idea and try this experiment and you try out this thing.” They have a way of just putting it conversationally just perfectly. Why isn’t that the abstract?</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:22:51</em></p><p>Exactly. This is coming from the perspective of how somebody who’s trying to explain an idea should formulate it better. What is your advice as a student to other students, if you don’t have a Karpathy who is doing the exposition of an idea? If you’re reading a paper from somebody or reading a book, what strategies do you employ to learn material you’re interested in in fields you’re not an expert at?</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:23:20</em></p><p><span>I don’t know that I have unique tips and tricks, to be honest. It’s a painful process. One thing that has always helped me quite a bit is—</span><a href="https://x.com/karpathy/status/1325154823856033793?lang=en" rel>I had a small tweet about this</a><span>—learning things on demand is pretty nice. Learning depth-wise. I do feel you need a bit of alternation of learning depth-wise, on demand—you’re trying to achieve a certain project that you’re going to get a reward from—and learning breadth-wise, which is just, “Oh, let’s do whatever 101, and here’s all the things you might need.” Which is a lot of school—does breadth-wise learning, like, “Oh, trust me, you’ll need this later,” that kind of stuff. Okay, I trust you. I’ll learn it because I guess I need it. But I love the kind of learning where you’ll get a reward out of doing something, and you’re learning on demand.</span></p><p>The other thing that I’ve found extremely helpful. This is an aspect where education is a bit more selfless, but explaining things to people is a beautiful way to learn something more deeply. This happens to me all the time. It probably happens to other people too because I realize if I don’t really understand something, I can’t explain it. I’m trying and I’m like, “Oh, I don’t understand this.” It’s so annoying to come to terms with that. You can go back and make sure you understood it. It fills these gaps of your understanding. It forces you to come to terms with them and to reconcile them.</p><p>I love to re-explain things and people should be doing that more as well. That forces you to manipulate the knowledge and make sure that you know what you’re talking about when you’re explaining it.</p><p><strong>Dwarkesh Patel</strong><span> </span><em>02:24:48</em></p><p>That’s an excellent note to close on. Andrej, that was great.</p><p><strong>Andrej Karpathy</strong><span> </span><em>02:24:51</em></p><p>Thank you.</p></div></div><div class="visibility-check"></div><div class="visibility-check"></div><div id="discussion" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-paddingTop-32 pc-paddingBottom-32 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-reset"><h4 class="pencraft pc-reset line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">Discussion about this video</h4><div class="pencraft pc-alignSelf-flex-start pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-minWidth-0 pc-reset bg-primary-zk6FDl pc-borderRadius-sm overflow-hidden-WdpwT6"><button type="button" aria-hidden="true" style="position:fixed;top:1px;left:1px;width:1px;height:0px;padding:0px;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;"></button><div aria-label="Select discussion type" role="tablist" aria-orientation="horizontal" class="pencraft pc-display-flex pc-gap-4 pc-padding-4 pc-position-relative pc-reset cursor-default-flE2S1 outline-detail-vcQLyr pc-borderRadius-sm overflow-auto-7WTsTi scrollBar-hidden-HcAIpI"><button tabindex="0" type="button" id="headlessui-tabs-tab-P0-42" role="tab" aria-selected="true" data-headlessui-state="selected" class="pencraft pc-reset flex-auto-j3S2WA pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_tertiary-rlke8z size_sm-G3LciD">Comments</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-43" role="tab" aria-selected="false" data-headlessui-state class="pencraft pc-reset flex-auto-j3S2WA pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_quaternary-kpMibu size_sm-G3LciD">Restacks</button><div class="pencraft pc-position-absolute pc-height-32 pc-reset bg-secondary-UUD3_J pc-borderRadius-xs sizing-border-box-DggLA4 highlight-U002IP"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH left-Tg8vqp"><div class="overlay-zrMCxn primary-lv_sOW"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH right-i3oWGi"><div class="overlay-zrMCxn primary-lv_sOW"></div></div></div></div></div><div id="comments-for-scroll"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div class="comments-page"><div class="container"><div data-test-id="comment-input" class="pencraft pc-display-flex pc-reset flex-grow-rzmknG"><form class="form-CkZ7Kt"><div style="--scale:32px;" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj"><div style="--scale:32px;" title="User" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!TnFC!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 32w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 64w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 96w" sizes="32px"/><img src="https://substackcdn.com/image/fetch/$s_!TnFC!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png" sizes="32px" alt="User's avatar" srcset="https://substackcdn.com/image/fetch/$s_!TnFC!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 32w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 64w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"/></picture></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset flex-grow-rzmknG"><textarea name="body" placeholder="Write a comment..." aria-label="Write a comment..." rows="4" class="pencraft input-qHk4bN autogrowing-_ipn9Y textarea-GbEjRX inputText-pV_yWb"></textarea><div style="height:0px;transition:height 250ms var(--animation-smooth), opacity 250ms var(--animation-smooth);display:block;flex-direction:column;opacity:0;"><div style="padding-top:0.05px;padding-bottom:0.05px;display:block;flex:0 0 auto;flex-direction:column;transition:transform 250ms var(--animation-smooth);"></div></div></div></form></div></div></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset border-left-detail-themed-TuEvbU sidebar-RUDMha"><div aria-label="Sidebar content" role="complementary" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-24 pc-paddingLeft-16 pc-paddingRight-16 pc-paddingTop-24 pc-paddingBottom-24 pc-boxShadow-lg pc-reset border-detail-themed-ofWgVp pc-borderRadius-md container-MuztBl"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-alignItems-center pc-reset"><a href="https://www.dwarkesh.com" native style="width:48px;height:48px;"><img src="https://substackcdn.com/image/fetch/$s_!hRiK!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png" width="48" height="48" alt="Dwarkesh Podcast" name="Dwarkesh Podcast" class="pencraft pc-reset outline-detail-vcQLyr pub-logo-m0IFm6 static-XUAQjT"/></a><div class="pencraft pc-reset color-pub-primary-text-NyXPlw align-center-y7ZD4w line-height-24-jnGwiv font-pub-headings-FE5byy size-17-JHHggF weight-bold-DmI9lw reset-IxiVJZ">Dwarkesh Podcast</div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ description-ij0plp">Deeply researched interviews</div><span hidden data-testid="podcast-description">Deeply researched interviews</span></div></div><div class="subscribe-container-_pmqCv"><div data-component-name="SubscribeWidget" class="subscribe-widget"><div class="pencraft pc-display-flex pc-justifyContent-center pc-reset"><div class="container-IpPqBD"><form action="/api/v1/free?nojs=true" method="post" novalidate class="form form-M5sC90"><input type="hidden" name="first_url" value/><input type="hidden" name="first_referrer" value/><input type="hidden" name="current_url"/><input type="hidden" name="current_referrer"/><input type="hidden" name="first_session_url" value/><input type="hidden" name="first_session_referrer" value/><input type="hidden" name="referral_code"/><input type="hidden" name="source" value="episode-page-sidebar"/><input type="hidden" name="referring_pub_id"/><input type="hidden" name="additional_referring_pub_ids"/><div class="sideBySideWrap-vGXrwP"><div class="emailInputWrapper-QlA86j emailInputWrapperExpectTruncation-w6Fd5V"><div class="pencraft pc-display-flex pc-minWidth-0 pc-position-relative pc-reset flex-auto-j3S2WA"><input name="email" placeholder="Type your email..." type="email" class="pencraft emailInput-OkIMeB emailInputExpectTruncation-VNj17x input-y4v6N4 inputText-pV_yWb"/></div></div><button tabindex="0" type="submit" disabled class="pencraft pc-reset pencraft rightButton primary subscribe-btn button-VFSdkv buttonBase-GK1x3M"><span class="button-text ">Subscribe</span></button></div><div id="error-container"></div></form></div></div></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-reset"><div data-testid="shows-listen-on" role="region" aria-label="Listen on" aria-hidden="true" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-pub-headings-FE5byy size-17-JHHggF weight-bold-DmI9lw reset-IxiVJZ">Listen on</div><div class="pencraft pc-display-flex pc-flexWrap-wrap pc-gap-8 pc-reset"><div class="pencraft pc-display-flex pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-alignItems-center pc-reset bg-secondary-UUD3_J pc-borderRadius-sm listen-on-item-yiZDJR"><div class="pencraft pc-display-flex pc-opacity-100 pc-reset"><img src="/img/shows_app_icons/substack.svg?v=1" class="icon-20-rna120"/></div><div class="pencraft pc-opacity-100 pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV nowrap-QngyoB reset-IxiVJZ">Substack App</div></div><div class="pencraft pc-display-flex pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-alignItems-center pc-reset bg-secondary-UUD3_J pc-borderRadius-sm listen-on-item-yiZDJR"><div class="pencraft pc-display-flex pc-opacity-100 pc-reset"><img src="/img/shows_app_icons/apple_podcasts.svg?v=1" class="icon-20-rna120"/></div><div class="pencraft pc-opacity-100 pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV nowrap-QngyoB reset-IxiVJZ">Apple Podcasts</div></div><div class="pencraft pc-display-flex pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-alignItems-center pc-reset bg-secondary-UUD3_J pc-borderRadius-sm listen-on-item-yiZDJR"><div class="pencraft pc-display-flex pc-opacity-100 pc-reset"><img src="/img/shows_app_icons/spotify.svg?v=1" class="icon-20-rna120"/></div><div class="pencraft pc-opacity-100 pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV nowrap-QngyoB reset-IxiVJZ">Spotify</div></div><div class="pencraft pc-display-flex pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-alignItems-center pc-reset bg-secondary-UUD3_J pc-borderRadius-sm listen-on-item-yiZDJR"><div class="pencraft pc-display-flex pc-opacity-100 pc-reset"><img src="/img/shows_app_icons/youtube.svg?v=1" class="icon-20-rna120"/></div><div class="pencraft pc-opacity-100 pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV nowrap-QngyoB reset-IxiVJZ">YouTube</div></div><div class="pencraft pc-display-flex pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-alignItems-center pc-reset bg-secondary-UUD3_J pc-borderRadius-sm listen-on-item-yiZDJR"><div class="pencraft pc-display-flex pc-opacity-100 pc-reset"><img src="/img/shows_app_icons/rss.svg?v=1" class="icon-20-rna120"/></div><div class="pencraft pc-opacity-100 pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-semibold-uqA4FV nowrap-QngyoB reset-IxiVJZ">RSS Feed</div></div></div></div><div aria-label="Appears in episode" role="region" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-pub-headings-FE5byy size-17-JHHggF weight-bold-DmI9lw reset-IxiVJZ">Appears in episode</div><div class="pencraft pc-display-flex pc-flexDirection-row pc-flexWrap-wrap pc-gap-12 pc-reset"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset"><div class="pencraft pc-display-flex pc-position-relative pc-reset"><a href="https://substack.com/@dwarkesh" aria-label="View Dwarkesh Patel's profile" utmSource="author-byline-face-podcast" class="pencraft pc-display-contents pc-reset"><div style="--scale:32px;" tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6"><div style="--scale:32px;" title="Dwarkesh Patel" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5eJb!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 32w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 64w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 96w" sizes="32px"/><img src="https://substackcdn.com/image/fetch/$s_!5eJb!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg" sizes="32px" alt="Dwarkesh Patel's avatar" srcset="https://substackcdn.com/image/fetch/$s_!5eJb!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 32w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 64w, https://substackcdn.com/image/fetch/$s_!5eJb!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"/></picture></div></div></a><div class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-reset userSelect-none-oDUy26 pc-borderRadius-full sizing-border-box-DggLA4 badge-jGMz0j placement_bottom-right-hoG7Xz shape_circle-Q9w9_R theme_accent-Y2sqZY priority_primary-uPxff5 singleElement-ve4D6v empty-w6FjPW"><div data-testid="user-badge" class="pencraft pc-reset cursor-pointer-LYORKw container-ORd8N2"><div data-headlessui-state><div aria-expanded="false" data-headlessui-state class="pencraft pc-reset"><svg role="img" style="height:16px;width:16px;" width="16" height="16" viewBox="0 0 40 40" fill="none" stroke-width="1.8" stroke="#000" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M17.8212 2.19198C19.5364 1.48152 21.4636 1.48152 23.1788 2.19198L31.198 5.51364C32.9132 6.2241 34.2759 7.58682 34.9864 9.30201L38.308 17.3212C39.0185 19.0364 39.0185 20.9636 38.308 22.6788L34.9864 30.698C34.2759 32.4132 32.9132 33.7759 31.198 34.4864L23.1788 37.808C21.4636 38.5185 19.5364 38.5185 17.8212 37.808L9.80201 34.4864C8.08682 33.7759 6.7241 32.4132 6.01364 30.698L2.69198 22.6788C1.98152 20.9636 1.98152 19.0364 2.69198 17.3212L6.01364 9.30201C6.7241 7.58682 8.08682 6.2241 9.80202 5.51364L17.8212 2.19198Z" stroke="#FF6719" stroke-width="4" fill="transparent"></path><path d="M27.1666 15L17.9999 24.1667L13.8333 20" stroke="#FF6719" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"></path></g></svg></div></div><div hidden style="position:fixed;top:1px;left:1px;width:1px;height:0px;padding:0px;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none;"></div></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ">Dwarkesh Patel</div></div></div></div></div><div aria-label="Recent episodes" role="region" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-pub-headings-FE5byy size-17-JHHggF weight-bold-DmI9lw reset-IxiVJZ">Recent Episodes</div><div style="margin:0 -8px;" aria-label="Recent episodes" role="region" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-12 pc-reset"><div aria-label="Post preview for Nick Lane – Life as we know it is chemically inevitable" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XUQW!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F175802370%2Fc3eaa143-65d8-48aa-9bcb-992642d8cc70%2Ftranscoded-1760110067.png"/><img src="https://substackcdn.com/image/fetch/$s_!XUQW!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F175802370%2Fc3eaa143-65d8-48aa-9bcb-992642d8cc70%2Ftranscoded-1760110067.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/nick-lane" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Nick Lane – Life as we know it is chemically inevitable</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-10-10T15:29:40.052Z" class="date-rtYe1v">Oct 10</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for Some thoughts on the Sutton interview" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LS8X!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F175283310%2F7e44846b-49c5-4241-86a4-c57622c6a7b8%2Ftranscoded-1759599175.png"/><img src="https://substackcdn.com/image/fetch/$s_!LS8X!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F175283310%2F7e44846b-49c5-4241-86a4-c57622c6a7b8%2Ftranscoded-1759599175.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/thoughts-on-sutton" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Some thoughts on the Sutton interview</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-10-04T17:45:00.586Z" class="date-rtYe1v">Oct 4</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for Richard Sutton – Father of RL thinks LLMs are a dead end" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!zTfZ!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F174609513%2Fbbf5d079-6be9-4aaf-82d2-57d8584f22d4%2Ftranscoded-1758896358.png"/><img src="https://substackcdn.com/image/fetch/$s_!zTfZ!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F174609513%2Fbbf5d079-6be9-4aaf-82d2-57d8584f22d4%2Ftranscoded-1758896358.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/richard-sutton" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Richard Sutton – Father of RL thinks LLMs are a dead end</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-09-26T14:48:59.697Z" class="date-rtYe1v">Sep 26</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for Fully autonomous robots are much closer than you think – Sergey Levine" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8wSC!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F173427890%2Fbb23b2a7-219d-463e-9f5d-164721ef56d3%2Ftranscoded-1758898981.png"/><img src="https://substackcdn.com/image/fetch/$s_!8wSC!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F173427890%2Fbb23b2a7-219d-463e-9f5d-164721ef56d3%2Ftranscoded-1758898981.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/sergey-levine" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Fully autonomous robots are much closer than you think – Sergey Levine</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-09-12T15:03:02.106Z" class="date-rtYe1v">Sep 12</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for How Hitler almost starved Britain – Sarah Paine" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XURO!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F172873013%2F0798040c-0395-47a1-a1c1-7b97cda60b1e%2Ftranscoded-1757084500.png"/><img src="https://substackcdn.com/image/fetch/$s_!XURO!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F172873013%2F0798040c-0395-47a1-a1c1-7b97cda60b1e%2Ftranscoded-1757084500.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/sarah-paine-ww2" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">How Hitler almost starved Britain – Sarah Paine</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-09-05T15:05:14.631Z" class="date-rtYe1v">Sep 5</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for Evolution designed us to die fast; we can change that — Jacob Kimmel" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!GYpT!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F171561010%2F6b6c7dda-3309-4f82-a4e4-788e85444986%2Ftranscoded-1755793692.png"/><img src="https://substackcdn.com/image/fetch/$s_!GYpT!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F171561010%2F6b6c7dda-3309-4f82-a4e4-788e85444986%2Ftranscoded-1755793692.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/jacob-kimmel" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Evolution designed us to die fast; we can change that — Jacob Kimmel</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-08-21T16:29:18.613Z" class="date-rtYe1v">Aug 21</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div><div aria-label="Post preview for China is killing the US on energy. Does that mean they’ll win AGI? — Casey Handmer" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><div style="aspect-ratio: 1" class="image-UySkN_ container-XxSyR3"><div style="transition:opacity var(--animation-timing-fast) var(--animation-smooth);" class="pencraft pc-display-flex pc-opacity-100 pc-justifyContent-center pc-alignItems-center pc-position-absolute pc-inset-0 pc-reset"><button style="width:32px;height:32px;" tabindex="0" type="button" aria-label="n" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz"><svg role="img" style="stroke:none;" width="10" height="10" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!yWjv!,w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F171017457%2F8f59424c-5a86-421d-8d62-fc66c40ff4e8%2Ftranscoded-1755268166.png"/><img src="https://substackcdn.com/image/fetch/$s_!yWjv!,w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F171017457%2F8f59424c-5a86-421d-8d62-fc66c40ff4e8%2Ftranscoded-1755268166.png" sizes="(min-width:768px) 50vw, 100vw" alt width="150" height="150" style="aspect-ratio:1;" class="img-OACg1c image-nBNbRY pencraft pc-reset"/></picture></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-4 pc-reset"><a style="font-size:15px;line-height:20px;" href="https://www.dwarkesh.com/p/casey-handmer" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">China is killing the US on energy. Does that mean they’ll win AGI? — Casey Handmer</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq clamp-y7pNm8 clamp-1-eSN73l reset-IxiVJZ meta-EgzBVA"><time datetime="2025-08-15T14:54:54.243Z" class="date-rtYe1v">Aug 15</time> <span class="dividerChar-SbAJEi">•</span> <span class="pencraft pc-reset reset-IxiVJZ"><a href="https://substack.com/@dwarkesh" class="link-HFGLqU">Dwarkesh Patel</a></span></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div class="visibility-check"></div><div class="subscribe-footer"><div class="container"><p>Ready for more?</p><div class="pencraft pc-display-flex pc-justifyContent-center pc-reset"><div><div class="container-IpPqBD"><form action="/api/v1/free?nojs=true" method="post" novalidate class="form form-M5sC90"><input type="hidden" name="first_url" value/><input type="hidden" name="first_referrer" value/><input type="hidden" name="current_url"/><input type="hidden" name="current_referrer"/><input type="hidden" name="first_session_url" value/><input type="hidden" name="first_session_referrer" value/><input type="hidden" name="referral_code"/><input type="hidden" name="source" value="subscribe_footer"/><input type="hidden" name="referring_pub_id"/><input type="hidden" name="additional_referring_pub_ids"/><div class="sideBySideWrap-vGXrwP"><div class="emailInputWrapper-QlA86j"><div class="pencraft pc-display-flex pc-minWidth-0 pc-position-relative pc-reset flex-auto-j3S2WA"><input name="email" placeholder="Type your email..." type="email" class="pencraft emailInput-OkIMeB emailInputOnAccentBackground-TfaCGr input-y4v6N4 inputText-pV_yWb"/></div></div><button tabindex="0" type="submit" disabled class="pencraft pc-reset pencraft rightButton primary subscribe-btn button-VFSdkv buttonOnAccentBackground-vmEt94 buttonBase-GK1x3M"><span class="button-text ">Subscribe</span></button></div><div id="error-container"></div></form></div></div></div></div></div></div></div></div><div class="footer-wrap publication-footer"><div class="visibility-check"></div><div class="footer themed-background"><div class="container"><div class="footer-blurbs"><div class="footer-copyright-blurb">© 2025 Dwarkesh Patel</div><div class="footer-terms-blurb"><a href="https://substack.com/privacy" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Privacy</a><span> ∙ </span><a href="https://substack.com/tos" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Terms</a><span> ∙ </span><a href="https://substack.com/ccpa#personal-data-collected" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Collection notice</a></div></div><div class="footer-buttons"><a native href="https://substack.com/signup?utm_source=substack&amp;utm_medium=web&amp;utm_content=footer" class="footer-substack-cta start-publishing"><svg role="img" width="1000" height="1000" viewBox="0 0 1000 1000" fill="#ff6719" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M764.166 348.371H236.319V419.402H764.166V348.371Z"></path><path d="M236.319 483.752V813.999L500.231 666.512L764.19 813.999V483.752H236.319Z"></path><path d="M764.166 213H236.319V284.019H764.166V213Z"></path></g></svg> Start your Substack</a><a native href="https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&amp;utm_content=web-footer-button" class="footer-substack-cta get-the-app no-icon">Get the app</a></div><div translated class="pencraft pc-reset reset-IxiVJZ footer-slogan-blurb"><a href="https://substack.com" native>Substack</a> is the home for great culture</div></div></div></div></div><div style="left:auto;right:16px;bottom:16px;z-index:1001;transform:translateY(0px);" role="region" aria-label="Notification" class="pencraft pc-position-fixed pc-reset sizing-border-box-DggLA4"></div><div></div>
        </div>

        
            <script src="https://js.sentry-cdn.com/6c2ff3e3828e4017b7faf7b63e24cdf8.min.js" crossorigin="anonymous"></script>
            <script>
                window.Sentry && window.Sentry.onLoad(function() {
                    window.Sentry.init({
                        environment: window._preloads.sentry_environment,
                        dsn: window._preloads.sentry_dsn,
                    })
                })
            </script>
        


        
        
        
        <script>window._preloads        = JSON.parse("{\"isEU\":false,\"language\":\"en\",\"country\":\"JP\",\"userLocale\":{\"language\":\"en\",\"region\":\"US\",\"source\":\"accept-language\"},\"base_url\":\"https://www.dwarkesh.com\",\"stripe_publishable_key\":\"pk_live_51QfnARLDSWi1i85FBpvw6YxfQHljOpWXw8IKi5qFWEzvW8HvoD8cqTulR9UWguYbYweLvA16P7LN6WZsGdZKrNkE00uGbFaOE3\",\"captcha_site_key\":\"6LdYbsYZAAAAAIFIRh8X_16GoFRLIReh-e-q6qSa\",\"pub\":{\"apple_pay_disabled\":false,\"apex_domain\":\"dwarkesh.com\",\"author_id\":4281466,\"byline_images_enabled\":true,\"bylines_enabled\":true,\"chartable_token\":null,\"community_enabled\":true,\"copyright\":\"Dwarkesh Patel\",\"cover_photo_url\":null,\"created_at\":\"2020-07-18T16:36:25.723Z\",\"custom_domain_optional\":false,\"custom_domain\":\"www.dwarkesh.com\",\"default_comment_sort\":\"best_first\",\"default_coupon\":null,\"default_group_coupon\":null,\"default_show_guest_bios\":true,\"email_banner_url\":null,\"email_from_name\":\"Dwarkesh Patel\",\"email_from\":null,\"embed_tracking_disabled\":false,\"explicit\":false,\"expose_paywall_content_to_search_engines\":true,\"fb_pixel_id\":null,\"fb_site_verification_token\":null,\"flagged_as_spam\":false,\"founding_subscription_benefits\":[],\"free_subscription_benefits\":[\"Episodes and posts\"],\"ga_pixel_id\":null,\"google_site_verification_token\":null,\"google_tag_manager_token\":null,\"hero_image\":null,\"hero_text\":\"Deeply researched interviews\",\"hide_intro_subtitle\":null,\"hide_intro_title\":null,\"hide_podcast_feed_link\":false,\"homepage_type\":\"newspaper\",\"id\":69345,\"image_thumbnails_always_enabled\":false,\"invite_only\":false,\"hide_podcast_from_pub_listings\":false,\"language\":\"en\",\"logo_url_wide\":null,\"logo_url\":\"https://substackcdn.com/image/fetch/$s_!QEPJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"minimum_group_size\":2,\"moderation_enabled\":true,\"name\":\"Dwarkesh Podcast\",\"paid_subscription_benefits\":[\"My sincere gratitude\"],\"parsely_pixel_id\":null,\"payments_state\":\"enabled\",\"paywall_free_trial_enabled\":false,\"podcast_art_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"paid_podcast_episode_art_url\":null,\"podcast_byline\":\"Dwarkesh Patel\",\"podcast_description\":\"Deeply researched interviews\",\"podcast_enabled\":true,\"podcast_feed_url\":null,\"podcast_title\":\"Dwarkesh Podcast\",\"post_preview_limit\":0,\"primary_user_id\":4281466,\"require_clickthrough\":false,\"show_pub_podcast_tab\":true,\"show_recs_on_homepage\":false,\"subdomain\":\"dwarkesh\",\"subscriber_invites\":0,\"support_email\":null,\"theme_var_background_pop\":\"#D10000\",\"theme_var_color_links\":false,\"theme_var_cover_bg_color\":null,\"trial_end_override\":null,\"twitter_pixel_id\":null,\"type\":\"newsletter\",\"post_reaction_faces_enabled\":true,\"is_personal_mode\":false,\"plans\":[{\"id\":\"yearly200usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":20000,\"amount_decimal\":\"20000\",\"billing_scheme\":\"per_unit\",\"created\":1724217851,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$200 a year\",\"product\":\"prod_QhW3iGq7tP5cbs\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":30500,\"unit_amount_decimal\":\"30500\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":106500,\"unit_amount_decimal\":\"106500\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":28000,\"unit_amount_decimal\":\"28000\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":16000,\"unit_amount_decimal\":\"16000\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":127500,\"unit_amount_decimal\":\"127500\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":17500,\"unit_amount_decimal\":\"17500\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":15000,\"unit_amount_decimal\":\"15000\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":366500,\"unit_amount_decimal\":\"366500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":200000,\"unit_amount_decimal\":\"200000\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":35000,\"unit_amount_decimal\":\"35000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":73000,\"unit_amount_decimal\":\"73000\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":188500,\"unit_amount_decimal\":\"188500\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":20000,\"unit_amount_decimal\":\"20000\"}}},{\"id\":\"monthly20usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":2000,\"amount_decimal\":\"2000\",\"billing_scheme\":\"per_unit\",\"created\":1724217850,\"currency\":\"usd\",\"interval\":\"month\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$20 a month\",\"product\":\"prod_QhW3eK6WoLEKhl\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3100,\"unit_amount_decimal\":\"3100\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":11000,\"unit_amount_decimal\":\"11000\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2800,\"unit_amount_decimal\":\"2800\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1600,\"unit_amount_decimal\":\"1600\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":13000,\"unit_amount_decimal\":\"13000\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1800,\"unit_amount_decimal\":\"1800\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1500,\"unit_amount_decimal\":\"1500\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":37000,\"unit_amount_decimal\":\"37000\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":20000,\"unit_amount_decimal\":\"20000\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3500,\"unit_amount_decimal\":\"3500\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":7500,\"unit_amount_decimal\":\"7500\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":19000,\"unit_amount_decimal\":\"19000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2000,\"unit_amount_decimal\":\"2000\"}}},{\"id\":\"founding50000usd\",\"name\":\"founding50000usd\",\"nickname\":\"founding50000usd\",\"active\":true,\"amount\":50000,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"metadata\":{\"substack\":\"yes\",\"founding\":\"yes\",\"no_coupons\":\"yes\",\"short_description\":\"Founding Member\",\"short_description_english\":\"Founding Member\",\"minimum\":\"50000\",\"minimum_local\":{\"aud\":77000,\"brl\":269500,\"cad\":70000,\"chf\":40000,\"dkk\":321500,\"eur\":43500,\"gbp\":38000,\"mxn\":920000,\"nok\":499000,\"nzd\":87000,\"pln\":182500,\"sek\":469500,\"usd\":50000}},\"currency_options\":{\"aud\":{\"unit_amount\":77000,\"tax_behavior\":\"unspecified\"},\"brl\":{\"unit_amount\":269500,\"tax_behavior\":\"unspecified\"},\"cad\":{\"unit_amount\":70000,\"tax_behavior\":\"unspecified\"},\"chf\":{\"unit_amount\":40000,\"tax_behavior\":\"unspecified\"},\"dkk\":{\"unit_amount\":321500,\"tax_behavior\":\"unspecified\"},\"eur\":{\"unit_amount\":43500,\"tax_behavior\":\"unspecified\"},\"gbp\":{\"unit_amount\":38000,\"tax_behavior\":\"unspecified\"},\"mxn\":{\"unit_amount\":920000,\"tax_behavior\":\"unspecified\"},\"nok\":{\"unit_amount\":499000,\"tax_behavior\":\"unspecified\"},\"nzd\":{\"unit_amount\":87000,\"tax_behavior\":\"unspecified\"},\"pln\":{\"unit_amount\":182500,\"tax_behavior\":\"unspecified\"},\"sek\":{\"unit_amount\":469500,\"tax_behavior\":\"unspecified\"},\"usd\":{\"unit_amount\":50000,\"tax_behavior\":\"unspecified\"}}}],\"stripe_user_id\":\"acct_1MUypKIfrO6rsO2w\",\"stripe_country\":\"US\",\"stripe_publishable_key\":\"pk_live_51MUypKIfrO6rsO2wJTQofVEA9VhSG1QGqcbo2tsaZgNIMTpre6rJW4dlBrzGdR4OOqnEtj6wJVCblEd9HTj1bH4O00lMVFfS3k\",\"stripe_platform_account\":\"US\",\"automatic_tax_enabled\":false,\"author_name\":\"Dwarkesh Patel\",\"author_handle\":\"dwarkesh\",\"author_photo_url\":\"https://substackcdn.com/image/fetch/$s_!5eJb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"author_bio\":\"Host of Dwarkesh Podcast\",\"twitter_screen_name\":\"dwarkesh_sp\",\"has_custom_tos\":false,\"has_custom_privacy\":false,\"theme\":{\"background_pop_color\":\"#f3c016\",\"web_bg_color\":\"#ffffff\",\"cover_bg_color\":\"#ffffff\",\"publication_id\":69345,\"color_links\":null,\"font_preset_heading\":\"fancy_serif\",\"font_preset_body\":null,\"font_family_headings\":null,\"font_family_body\":null,\"font_family_ui\":null,\"font_size_body_desktop\":null,\"print_secondary\":null,\"custom_css_web\":null,\"custom_css_email\":null,\"home_hero\":\"podcast\",\"home_posts\":\"custom\",\"home_show_top_posts\":true,\"hide_images_from_list\":false,\"home_hero_alignment\":\"left\",\"home_hero_show_podcast_links\":true,\"default_post_header_variant\":null},\"threads_v2_settings\":{\"photo_replies_enabled\":true,\"first_thread_email_sent_at\":null,\"create_thread_minimum_role\":\"contributor\",\"activated_at\":\"2024-10-11T19:28:14.299+00:00\",\"reader_thread_notifications_enabled\":false,\"boost_free_subscriber_chat_preview_enabled\":false,\"push_suppression_enabled\":false},\"default_group_coupon_percent_off\":null,\"pause_return_date\":null,\"has_posts\":true,\"has_recommendations\":true,\"first_post_date\":\"2020-05-22T18:31:47.000Z\",\"has_podcast\":true,\"has_free_podcast\":true,\"has_subscriber_only_podcast\":false,\"has_community_content\":true,\"rankingDetail\":\"Hundreds of paid subscribers\",\"rankingDetailFreeIncluded\":\"Tens of thousands of subscribers\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Over 49,000 subscribers\",\"rankingDetailByLanguage\":{\"de\":{\"rankingDetail\":\"Hunderte von Paid-Abonnenten\",\"rankingDetailFreeIncluded\":\"Zehntausende von Abonnenten\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"\u00DCber 49,000 Abonnenten\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"es\":{\"rankingDetail\":\"Cientos de suscriptores de pago\",\"rankingDetailFreeIncluded\":\"Decenas de miles de suscriptores\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"M\u00E1s de 49,000 suscriptores\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"fr\":{\"rankingDetail\":\"Des centaines d'abonn\u00E9s payants\",\"rankingDetailFreeIncluded\":\"Des dizaines de milliers d'abonn\u00E9s\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Plus de 49,000 abonn\u00E9s\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"pt\":{\"rankingDetail\":\"Centenas de subscritores pagos\",\"rankingDetailFreeIncluded\":\"Dezenas de milhares de subscritores\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Mais de 49,000 subscritores\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"pt-br\":{\"rankingDetail\":\"Centenas de assinantes pagantes\",\"rankingDetailFreeIncluded\":\"Dezenas de milhares de assinantes\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Mais de 49,000 assinantes\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"it\":{\"rankingDetail\":\"Centinaia di abbonati a pagamento\",\"rankingDetailFreeIncluded\":\"Decine di migliaia di abbonati\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Oltre 49,000 abbonati\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"},\"en\":{\"rankingDetail\":\"Hundreds of paid subscribers\",\"rankingDetailFreeIncluded\":\"Tens of thousands of subscribers\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":10000,\"rankingDetailFreeSubscriberCount\":\"Over 49,000 subscribers\",\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\"}},\"freeSubscriberCount\":\"49,000\",\"freeSubscriberCountOrderOfMagnitude\":\"49K+\",\"author_bestseller_tier\":100,\"disable_monthly_subscriptions\":false,\"disable_annual_subscriptions\":false,\"hide_post_restacks\":false,\"notes_feed_enabled\":false,\"showIntroModule\":false,\"last_chat_post_at\":\"2024-10-11T19:28:13.762Z\",\"primary_profile_name\":\"Dwarkesh Patel\",\"primary_profile_photo_url\":\"https://substackcdn.com/image/fetch/$s_!5eJb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"no_follow\":false,\"paywall_chat\":\"paid\",\"sections\":[{\"id\":21898,\"created_at\":\"2022-07-05T04:51:23.844Z\",\"updated_at\":\"2025-07-02T05:27:37.542Z\",\"publication_id\":69345,\"name\":\"Blog\",\"description\":\"All my posts\",\"slug\":\"blog\",\"is_podcast\":false,\"is_live\":true,\"is_default_on\":true,\"sibling_rank\":0,\"port_status\":\"success\",\"logo_url\":null,\"hide_from_navbar\":false,\"email_from_name\":\"\",\"hide_posts_from_pub_listings\":false,\"email_banner_url\":null,\"cover_photo_url\":null,\"hide_intro_title\":false,\"hide_intro_subtitle\":false,\"ignore_publication_email_settings\":false,\"podcastSettings\":null,\"showLinks\":[],\"spotifyPodcastSettings\":null,\"pageTheme\":{\"id\":7565,\"publication_id\":69345,\"section_id\":21898,\"page\":null,\"page_hero\":\"default\",\"page_posts\":\"list\",\"show_podcast_links\":true,\"hero_alignment\":\"left\"},\"podcastPalette\":{\"DarkMuted\":{\"population\":72,\"rgb\":[73,153,137]},\"DarkVibrant\":{\"population\":6013,\"rgb\":[4,100,84]},\"LightMuted\":{\"population\":7,\"rgb\":[142,198,186]},\"LightVibrant\":{\"population\":3,\"rgb\":[166,214,206]},\"Muted\":{\"population\":6,\"rgb\":[92,164,156]},\"Vibrant\":{\"population\":5,\"rgb\":[76,164,146]}},\"spotify_podcast_settings\":null}],\"multipub_migration\":null,\"navigationBarItems\":[{\"id\":\"7485e42b-b8b7-4970-bd90-a1a98990a9df\",\"publication_id\":69345,\"sibling_rank\":0,\"link_title\":null,\"link_url\":null,\"section_id\":21898,\"post_id\":null,\"is_hidden\":null,\"standard_key\":null,\"post_tag_id\":null,\"post\":null,\"postTag\":null,\"section\":{\"id\":21898,\"created_at\":\"2022-07-05T04:51:23.844Z\",\"updated_at\":\"2025-07-02T05:27:37.542Z\",\"publication_id\":69345,\"name\":\"Blog\",\"description\":\"All my posts\",\"slug\":\"blog\",\"is_podcast\":false,\"is_live\":true,\"is_default_on\":true,\"sibling_rank\":0,\"port_status\":\"success\",\"logo_url\":null,\"hide_from_navbar\":false,\"email_from_name\":\"\",\"hide_posts_from_pub_listings\":false,\"email_banner_url\":null,\"cover_photo_url\":null,\"hide_intro_title\":false,\"hide_intro_subtitle\":false,\"ignore_publication_email_settings\":false}},{\"id\":\"3ac94479-116c-485e-ad36-f045a158319f\",\"publication_id\":69345,\"sibling_rank\":1,\"link_title\":null,\"link_url\":null,\"section_id\":null,\"post_id\":null,\"is_hidden\":false,\"standard_key\":\"about\",\"post_tag_id\":null,\"post\":null,\"postTag\":null,\"section\":null},{\"id\":\"0129216d-f181-4572-a505-dc4cc754eadf\",\"publication_id\":69345,\"sibling_rank\":2,\"link_title\":null,\"link_url\":null,\"section_id\":null,\"post_id\":null,\"is_hidden\":true,\"standard_key\":\"archive\",\"post_tag_id\":null,\"post\":null,\"postTag\":null,\"section\":null},{\"id\":\"7be81402-e9fa-4252-a988-6dad7fb280b2\",\"publication_id\":69345,\"sibling_rank\":5,\"link_title\":\"Sponsors\",\"link_url\":\"/advertise\",\"section_id\":null,\"post_id\":null,\"is_hidden\":null,\"standard_key\":null,\"post_tag_id\":null,\"post\":null,\"postTag\":null,\"section\":null}],\"contributors\":[{\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"role\":\"admin\",\"owner\":true,\"user_id\":4281466,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\"}],\"threads_v2_enabled\":true,\"viralGiftsConfig\":{\"id\":\"75ad850a-cbea-4ee2-8dd4-a28184c02f58\",\"publication_id\":69345,\"enabled\":false,\"gifts_per_user\":5,\"gift_length_months\":1,\"send_extra_gifts\":true,\"message\":\"I interview scientists, historians, economists, & founders. I also blog.\",\"created_at\":\"2023-01-06T02:21:54.369933+00:00\",\"updated_at\":\"2023-01-06T02:21:54.369933+00:00\",\"days_til_invite\":14,\"send_emails\":true,\"show_link\":null,\"grant_email_body\":null,\"grant_email_subject\":null},\"tier\":2,\"no_index\":false,\"can_set_google_site_verification\":true,\"can_have_sitemap\":true,\"draft_iap_advanced_plans\":[{\"sku\":\"1Y5KRKQzQMn9AAGssw\",\"publication_id\":\"69345\",\"is_active\":true,\"price_base_units\":2800,\"currency_alpha3\":\"usd\",\"period\":\"month\",\"created_at\":\"2025-08-18T15:10:52.804Z\",\"updated_at\":\"2025-08-18T15:10:52.804Z\",\"id\":\"238455\",\"payout_amount_base_units\":200,\"alternate_currencies\":{\"aud\":4400,\"brl\":15500,\"cad\":3900,\"chf\":2300,\"dkk\":18000,\"eur\":2400,\"gbp\":2100,\"mxn\":52500,\"nok\":28500,\"nzd\":4800,\"pln\":10500,\"sek\":27000},\"display_name\":\"Dwarkesh Podcast (Monthly)\",\"display_price\":\"$28\"},{\"sku\":\"kJQXuRZhO7a0EqZrzU\",\"publication_id\":\"69345\",\"is_active\":true,\"price_base_units\":27500,\"currency_alpha3\":\"usd\",\"period\":\"year\",\"created_at\":\"2025-08-18T15:10:52.816Z\",\"updated_at\":\"2025-08-18T15:10:52.816Z\",\"id\":\"238456\",\"payout_amount_base_units\":2000,\"alternate_currencies\":{\"aud\":42500,\"brl\":149000,\"cad\":38000,\"chf\":22500,\"dkk\":175500,\"eur\":23500,\"gbp\":20500,\"mxn\":515000,\"nok\":280000,\"nzd\":46500,\"pln\":100000,\"sek\":262500},\"display_name\":\"Dwarkesh Podcast (Yearly)\",\"display_price\":\"$275\"}],\"iap_advanced_plans\":[{\"sku\":\"1Y5KRKQzQMn9AAGssw\",\"publication_id\":\"69345\",\"is_active\":true,\"price_base_units\":2800,\"currency_alpha3\":\"usd\",\"period\":\"month\",\"created_at\":\"2025-08-18T15:10:52.804Z\",\"updated_at\":\"2025-08-18T15:10:52.804Z\",\"id\":\"238455\",\"payout_amount_base_units\":200,\"alternate_currencies\":{\"aud\":4400,\"brl\":15500,\"cad\":3900,\"chf\":2300,\"dkk\":18000,\"eur\":2400,\"gbp\":2100,\"mxn\":52500,\"nok\":28500,\"nzd\":4800,\"pln\":10500,\"sek\":27000},\"display_name\":\"Dwarkesh Podcast (Monthly)\",\"display_price\":\"$28\"},{\"sku\":\"kJQXuRZhO7a0EqZrzU\",\"publication_id\":\"69345\",\"is_active\":true,\"price_base_units\":27500,\"currency_alpha3\":\"usd\",\"period\":\"year\",\"created_at\":\"2025-08-18T15:10:52.816Z\",\"updated_at\":\"2025-08-18T15:10:52.816Z\",\"id\":\"238456\",\"payout_amount_base_units\":2000,\"alternate_currencies\":{\"aud\":42500,\"brl\":149000,\"cad\":38000,\"chf\":22500,\"dkk\":175500,\"eur\":23500,\"gbp\":20500,\"mxn\":515000,\"nok\":280000,\"nzd\":46500,\"pln\":100000,\"sek\":262500},\"display_name\":\"Dwarkesh Podcast (Yearly)\",\"display_price\":\"$275\"}],\"founding_plan_name_english\":\"Founding Member\",\"draft_plans\":[{\"id\":\"yearly200usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":20000,\"amount_decimal\":\"20000\",\"billing_scheme\":\"per_unit\",\"created\":1724217851,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$200 a year\",\"product\":\"prod_QhW3iGq7tP5cbs\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":30500,\"unit_amount_decimal\":\"30500\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":106500,\"unit_amount_decimal\":\"106500\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":28000,\"unit_amount_decimal\":\"28000\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":16000,\"unit_amount_decimal\":\"16000\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":127500,\"unit_amount_decimal\":\"127500\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":17500,\"unit_amount_decimal\":\"17500\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":15000,\"unit_amount_decimal\":\"15000\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":366500,\"unit_amount_decimal\":\"366500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":200000,\"unit_amount_decimal\":\"200000\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":35000,\"unit_amount_decimal\":\"35000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":73000,\"unit_amount_decimal\":\"73000\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":188500,\"unit_amount_decimal\":\"188500\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":20000,\"unit_amount_decimal\":\"20000\"}}},{\"id\":\"monthly20usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":2000,\"amount_decimal\":\"2000\",\"billing_scheme\":\"per_unit\",\"created\":1724217850,\"currency\":\"usd\",\"interval\":\"month\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$20 a month\",\"product\":\"prod_QhW3eK6WoLEKhl\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3100,\"unit_amount_decimal\":\"3100\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":11000,\"unit_amount_decimal\":\"11000\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2800,\"unit_amount_decimal\":\"2800\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1600,\"unit_amount_decimal\":\"1600\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":13000,\"unit_amount_decimal\":\"13000\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1800,\"unit_amount_decimal\":\"1800\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1500,\"unit_amount_decimal\":\"1500\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":37000,\"unit_amount_decimal\":\"37000\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":20000,\"unit_amount_decimal\":\"20000\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3500,\"unit_amount_decimal\":\"3500\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":7500,\"unit_amount_decimal\":\"7500\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":19000,\"unit_amount_decimal\":\"19000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2000,\"unit_amount_decimal\":\"2000\"}}},{\"id\":\"founding50000usd\",\"name\":\"founding50000usd\",\"nickname\":\"founding50000usd\",\"active\":true,\"amount\":50000,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"metadata\":{\"substack\":\"yes\",\"founding\":\"yes\",\"no_coupons\":\"yes\",\"short_description\":\"Founding Member\",\"short_description_english\":\"Founding Member\",\"minimum\":\"50000\",\"minimum_local\":{\"aud\":77000,\"brl\":269500,\"cad\":70000,\"chf\":40000,\"dkk\":321500,\"eur\":43500,\"gbp\":38000,\"mxn\":920000,\"nok\":499000,\"nzd\":87000,\"pln\":182500,\"sek\":469500,\"usd\":50000}},\"currency_options\":{\"aud\":{\"unit_amount\":77000,\"tax_behavior\":\"unspecified\"},\"brl\":{\"unit_amount\":269500,\"tax_behavior\":\"unspecified\"},\"cad\":{\"unit_amount\":70000,\"tax_behavior\":\"unspecified\"},\"chf\":{\"unit_amount\":40000,\"tax_behavior\":\"unspecified\"},\"dkk\":{\"unit_amount\":321500,\"tax_behavior\":\"unspecified\"},\"eur\":{\"unit_amount\":43500,\"tax_behavior\":\"unspecified\"},\"gbp\":{\"unit_amount\":38000,\"tax_behavior\":\"unspecified\"},\"mxn\":{\"unit_amount\":920000,\"tax_behavior\":\"unspecified\"},\"nok\":{\"unit_amount\":499000,\"tax_behavior\":\"unspecified\"},\"nzd\":{\"unit_amount\":87000,\"tax_behavior\":\"unspecified\"},\"pln\":{\"unit_amount\":182500,\"tax_behavior\":\"unspecified\"},\"sek\":{\"unit_amount\":469500,\"tax_behavior\":\"unspecified\"},\"usd\":{\"unit_amount\":50000,\"tax_behavior\":\"unspecified\"}}}],\"base_url\":\"https://www.dwarkesh.com\",\"hostname\":\"www.dwarkesh.com\",\"is_on_substack\":false,\"show_links\":[{\"id\":15250,\"publication_id\":69345,\"section_id\":null,\"url\":\"https://podcasts.apple.com/us/podcast/dwarkesh-podcast/id1516093381\",\"platform\":\"apple_podcasts\"},{\"id\":18109,\"publication_id\":69345,\"section_id\":null,\"url\":\"https://open.spotify.com/show/4JH4tybY1zX6e5hjCwU6gF\",\"platform\":\"spotify\"},{\"id\":15251,\"publication_id\":69345,\"section_id\":null,\"url\":\"https://www.youtube.com/playlist?list=PLd7-bHaQwnthaNDpZ32TtYONGVk95-fhF\",\"platform\":\"youtube\"},{\"id\":18110,\"publication_id\":69345,\"section_id\":null,\"url\":\"https://open.spotify.com/show/4JH4tybY1zX6e5hjCwU6gF\",\"platform\":\"spotify_for_paid_users\"}],\"spotify_podcast_settings\":{\"id\":2638,\"publication_id\":69345,\"section_id\":null,\"spotify_access_token\":\"a1cf8c47-901a-4209-9566-4d5bb473fe52\",\"spotify_uri\":\"spotify:show:5ikFm5iDfpalhbBdxSukTy\",\"spotify_podcast_title\":null,\"created_at\":\"2024-03-24T22:31:36.587Z\",\"updated_at\":\"2024-04-14T01:46:43.445Z\",\"currently_published_on_spotify\":true,\"feed_url_for_spotify\":\"https://api.substack.com/feed/podcast/spotify/a1cf8c47-901a-4209-9566-4d5bb473fe52/69345.rss\",\"spotify_show_url\":\"https://open.spotify.com/show/5ikFm5iDfpalhbBdxSukTy\"},\"podcastPalette\":{\"Vibrant\":{\"rgb\":[148,92,36],\"population\":2},\"DarkVibrant\":{\"rgb\":[106,67,24],\"population\":75},\"LightVibrant\":{\"rgb\":[229.05652173913046,188.7,148.34347826086955],\"population\":0},\"Muted\":{\"rgb\":[157,115,77],\"population\":8},\"DarkMuted\":{\"rgb\":[68,58,55],\"population\":404},\"LightMuted\":{\"rgb\":[220,228,204],\"population\":1}},\"pageThemes\":{\"podcast\":{\"id\":3678,\"publication_id\":69345,\"section_id\":null,\"page\":\"podcast\",\"page_hero\":\"feature-media\",\"page_posts\":\"list\",\"show_podcast_links\":true,\"hero_alignment\":\"left\"}},\"appTheme\":{\"colors\":{\"accent\":{\"name\":\"#f3c016\",\"primary\":{\"r\":225,\"g\":176,\"b\":0,\"a\":1},\"primary_hover\":{\"r\":204,\"g\":157,\"b\":0,\"a\":1},\"primary_elevated\":{\"r\":204,\"g\":157,\"b\":0,\"a\":1},\"secondary\":{\"r\":225,\"g\":176,\"b\":0,\"a\":0.2},\"contrast\":{\"r\":255,\"g\":255,\"b\":255,\"a\":1},\"bg\":{\"r\":225,\"g\":176,\"b\":0,\"a\":0.2},\"bg_hover\":{\"r\":225,\"g\":176,\"b\":0,\"a\":0.3},\"dark\":{\"primary\":{\"r\":243,\"g\":192,\"b\":22,\"a\":1},\"primary_hover\":{\"r\":255,\"g\":211,\"b\":51,\"a\":1},\"primary_elevated\":{\"r\":255,\"g\":211,\"b\":51,\"a\":1},\"secondary\":{\"r\":243,\"g\":192,\"b\":22,\"a\":0.2},\"contrast\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.8},\"bg\":{\"r\":243,\"g\":192,\"b\":22,\"a\":0.2},\"bg_hover\":{\"r\":243,\"g\":192,\"b\":22,\"a\":0.3}}},\"fg\":{\"primary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.8},\"secondary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.6},\"tertiary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.4},\"accent\":{\"r\":150,\"g\":111,\"b\":0,\"a\":1},\"dark\":{\"primary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.9},\"secondary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.6},\"tertiary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.4},\"accent\":{\"r\":243,\"g\":192,\"b\":22,\"a\":1}}},\"bg\":{\"name\":\"#ffffff\",\"hue\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0},\"tint\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0},\"primary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":1},\"primary_hover\":{\"r\":250,\"g\":250,\"b\":250,\"a\":1},\"primary_elevated\":{\"r\":250,\"g\":250,\"b\":250,\"a\":1},\"secondary\":{\"r\":238,\"g\":238,\"b\":238,\"a\":1},\"secondary_elevated\":{\"r\":206.90096477355226,\"g\":206.90096477355175,\"b\":206.9009647735519,\"a\":1},\"tertiary\":{\"r\":219,\"g\":219,\"b\":219,\"a\":1},\"quaternary\":{\"r\":182,\"g\":182,\"b\":182,\"a\":1},\"dark\":{\"primary\":{\"r\":22,\"g\":23,\"b\":24,\"a\":1},\"primary_hover\":{\"r\":27,\"g\":28,\"b\":29,\"a\":1},\"primary_elevated\":{\"r\":27,\"g\":28,\"b\":29,\"a\":1},\"secondary\":{\"r\":35,\"g\":37,\"b\":37,\"a\":1},\"secondary_elevated\":{\"r\":41.35899397549579,\"g\":43.405356429195315,\"b\":43.40489285041963,\"a\":1},\"tertiary\":{\"r\":54,\"g\":55,\"b\":55,\"a\":1},\"quaternary\":{\"r\":90,\"g\":91,\"b\":91,\"a\":1}}}},\"cover_image\":{\"url\":\"https://substackcdn.com/image/fetch/$s_!QEPJ!,w_1200,h_400,c_pad,f_auto,q_auto:best,fl_progressive:steep,b_auto:border,b_rgb:ffffff/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"height\":1080,\"width\":3240}},\"live_subscriber_counts\":false,\"supports_ip_content_unlock\":false,\"logoPalette\":{\"Vibrant\":{\"rgb\":[172,116,81],\"population\":3},\"DarkVibrant\":{\"rgb\":[100,57,34],\"population\":15},\"LightVibrant\":{\"rgb\":[212.5470355731225,183.19683794466403,164.8529644268775],\"population\":0},\"Muted\":{\"rgb\":[172,124,84],\"population\":2},\"DarkMuted\":{\"rgb\":[78,69,59],\"population\":401},\"LightMuted\":{\"rgb\":[190,184,177],\"population\":414}}},\"confirmedLogin\":false,\"hide_intro_popup\":true,\"block_auto_login\":false,\"domainInfo\":{\"isSubstack\":false,\"customDomain\":\"www.dwarkesh.com\"},\"experimentFeatures\":{},\"experimentExposures\":{},\"siteConfigs\":{\"score_upsell_email\":\"control\",\"first_chat_email_enabled\":true,\"notes_video_max_duration_minutes\":5,\"reader-onboarding-promoted-pub\":737237,\"new_commenter_approval\":false,\"pub_update_opennode_api_key\":false,\"ios_trending_topic_note_badge\":\"control\",\"enable_user_report_review\":true,\"zendesk_automation_cancellations\":false,\"hide_book_a_meeting_button\":false,\"mfa_action_box_enabled\":false,\"publication_max_bylines\":35,\"no_contest_charge_disputes\":false,\"feed_posts_previously_seen_weight\":0.1,\"publication_tabs_reorder\":false,\"comp_expiry_email_new_copy\":\"NONE\",\"free_unlock_required\":false,\"traffic_rule_check_enabled\":false,\"amp_emails_enabled\":false,\"enable_post_summarization\":false,\"live_stream_host_warning_message\":\"\",\"bitcoin_enabled\":false,\"minimum_ios_os_version\":\"17.0.0\",\"show_entire_square_image\":false,\"hide_subscriber_count\":false,\"publication_author_display_override\":\"\",\"ios_webview_payments_enabled\":\"control\",\"generate_pdf_tax_report\":false,\"show_generic_post_importer\":false,\"enable_pledges_modal\":true,\"include_pdf_invoice\":false,\"app_upsell_after_posting_notes\":\"experiment\",\"notes_weight_watch_video\":5,\"use_post_podcast_import_batching\":true,\"enable_react_dashboard\":false,\"meetings_v1\":false,\"enable_videos_page\":false,\"exempt_from_gtm_filter\":false,\"group_sections_and_podcasts_in_menu\":false,\"boost_optin_modal_enabled\":true,\"standards_and_enforcement_features_enabled\":false,\"pub_creation_captcha_behavior\":\"risky_pubs_or_rate_limit\",\"post_blogspot_importer\":false,\"suggested_search_metadata_web_ui\":false,\"notes_weight_short_item_boost\":0.15,\"pub_tts_override\":\"default\",\"disable_monthly_subscriptions\":false,\"skip_welcome_email\":false,\"chat_reader_thread_notification_default\":false,\"scheduled_pinned_posts\":false,\"disable_redirect_outbound_utm_params\":false,\"reader_gift_referrals_enabled\":true,\"dont_show_guest_byline\":false,\"like_comments_enabled\":true,\"subscription_bar_all_debug_enabled\":false,\"temporal_livestream_ended_draft\":true,\"enable_author_note_email_toggle\":false,\"meetings_embed_publication_name\":false,\"fallback_to_archive_search_on_section_pages\":false,\"livekit_track_egress_custom_base_url\":\"http://livekit-egress-custom-recorder-participant-test.s3-website-us-east-1.amazonaws.com\",\"people_you_may_know_algorithm\":\"experiment\",\"welcome_screen_blurb_override\":\"\",\"live_stream_guest_overlay\":\"control\",\"like_posts_enabled\":true,\"ios_gutterless_feed\":\"control\",\"dpn_weight_disable\":15,\"twitter_player_card_enabled\":true,\"feed_promoted_user\":false,\"writer_beta_android_enable_post_editor_v2\":false,\"show_note_stats_for_all_notes\":false,\"section_specific_csv_imports_enabled\":false,\"disable_podcast_feed_description_cta\":false,\"bypass_profile_substack_logo_detection\":false,\"use_preloaded_player_sources\":false,\"enable_tiktok_oauth\":false,\"list_pruning_enabled\":false,\"facebook_connect\":false,\"opt_in_to_sections_during_subscribe\":false,\"dpn_weight_share\":2,\"underlined_colored_links\":false,\"unified_presskit_enabled\":false,\"extract_stripe_receipt_url\":false,\"enable_aligned_images\":false,\"max_image_upload_mb\":64,\"enable_android_dms_writer_beta\":false,\"threads_suggested_ios_version\":null,\"pledges_disabled\":false,\"threads_minimum_ios_version\":812,\"hide_podcast_email_setup_link\":false,\"subscribe_captcha_behavior\":\"default\",\"publication_ban_sample_rate\":0,\"grant_viral_gifts_to_gift_recipients\":\"experiment\",\"ios_enable_publication_activity_tab\":false,\"custom_themes_substack_subscribe_modal\":false,\"share_viral_gift_as_link\":\"experiment\",\"opt_in_to_sections_during_subscribe_include_main_pub_newsletter\":false,\"continue_support_cta_in_newsletter_emails\":false,\"bloomberg_syndication_enabled\":false,\"lists_enabled\":false,\"ios_feed_media_content_mode\":\"fit\",\"generated_database_maintenance_mode\":false,\"allow_document_freeze\":false,\"subscription_bar_all_debug_subdomains\":null,\"podcast_main_feed_is_firehose\":false,\"pub_app_incentive_gift\":\"\",\"no_embed_redirect\":false,\"translate_mobile_app\":false,\"customized_email_from_name_for_new_follow_emails\":\"treatment\",\"spotify_open_access_sandbox_mode\":false,\"publication_ranking_v18\":\"experiment\",\"fullstory_enabled\":false,\"chat_reply_poll_interval\":3,\"dpn_weight_follow_or_subscribe\":3,\"speaker_focus_group_shot\":\"experiment\",\"updated_note_sharing_assets_enabled\":false,\"enable_reader_marketing_page\":false,\"force_pub_links_to_use_subdomain\":false,\"always_show_cookie_banner\":false,\"hide_media_download_option\":false,\"hide_post_restacks\":false,\"feed_item_source_debug_mode\":false,\"writer_beta_android_enable_post_editor\":false,\"thefp_enable_account_menu\":false,\"enable_user_status_ui\":false,\"publication_homepage_title_display_override\":\"\",\"pub_banned_word_list\":\"\",\"post_preview_highlight_byline\":false,\"4k_video\":false,\"enable_islands_section_intent_screen\":false,\"tfp_free_week_reg_wall\":false,\"post_metering_enabled\":false,\"notifications_disabled\":\"\",\"cross_post_notification_threshold\":1000,\"facebook_connect_prod_app\":true,\"feed_enable_live_streams\":false,\"force_into_pymk_ranking\":false,\"minimum_android_version\":756,\"live_stream_krisp_noise_suppression_enabled\":false,\"enable_transcription_translations\":false,\"ios_post_video_pager_alpha_enabled\":false,\"use_og_image_as_twitter_image_for_post_previews\":false,\"always_use_podcast_channel_art_as_episode_art_in_rss\":false,\"cookie_preference_middleware_enabled\":false,\"seo_tier_override\":\"NONE\",\"no_follow_links\":false,\"publisher_api_enabled\":false,\"zendesk_support_priority\":\"default\",\"enable_post_clips_stats\":false,\"enable_subscriber_referrals_awards\":true,\"ios_profile_themes_feed_permalink_enabled\":false,\"use_publication_language_for_transcription\":false,\"show_substack_funded_gifts_tooltip\":true,\"disable_ai_transcription\":false,\"thread_permalink_preview_min_ios_version\":4192,\"android_toggle_on_website_enabled\":false,\"internal_android_enable_post_editor\":false,\"edit_profile_feed_item\":false,\"updated_inbox_ui\":false,\"web_reader_podcasts_tab\":false,\"use_temporal_thumbnail_selection_workflow\":false,\"live_stream_creation_enabled\":false,\"disable_card_element_in_europe\":false,\"web_growth_item_promotion_threshold\":0,\"enable_web_typing_indicators\":false,\"web_vitals_sample_rate\":0,\"allow_live_stream_auto_takedown\":\"true\",\"search_ranker_variant\":\"control\",\"enable_progressive_speaker_focus_clips\":false,\"post_advanced_search\":\"control\",\"ai_image_generation_enabled\":true,\"disable_personal_substack_initialization\":false,\"section_specific_welcome_pages\":false,\"local_payment_methods\":\"control\",\"enable_linkedin_oauth\":true,\"posts_in_rss_feed\":20,\"post_rec_endpoint\":\"\",\"publisher_dashboard_section_selector\":false,\"reader_surveys_platform_question_order\":\"36,1,4,2,3,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35\",\"ios_toggle_on_website_enabled\":false,\"login_guard_app_link_in_email\":true,\"community_moderators_enabled\":false,\"monthly_sub_is_one_off\":false,\"unread_notes_activity_digest\":\"control\",\"display_cookie_settings\":false,\"welcome_page_query_params\":false,\"enable_free_podcast_urls\":false,\"comp_expiry_emails_disabled\":false,\"enable_description_on_polls\":false,\"use_microlink_for_instagram_embeds\":false,\"post_notification_batch_delay_ms\":30000,\"free_signup_confirmation_behavior\":\"with_email_validation\",\"ios_post_stats_for_admins\":false,\"live_stream_concurrent_viewer_count_drawer\":false,\"use_livestream_post_media_composition\":true,\"section_specific_preambles\":false,\"android_live_stream_multihost_enabled\":false,\"show_menu_on_posts\":false,\"app_upsell_follow_prompt\":\"control\",\"ios_post_subscribe_web_routing\":true,\"opt_into_all_trending_topics\":false,\"ios_writer_stats_public_launch_v2\":false,\"min_size_for_phishing_check\":1,\"enable_android_post_stats\":false,\"ios_chat_revamp_enabled\":false,\"app_onboarding_survey_email\":false,\"post_notification_batch_chunk_size\":100,\"thefp_enable_pullquote_alignment\":false,\"thefp_enable_pullquote_color\":false,\"republishing_enabled\":false,\"app_mode\":false,\"show_phone_banner\":true,\"live_stream_video_enhancer\":\"internal\",\"minimum_ios_version\":2200,\"enable_author_pages\":false,\"enable_decagon_chat\":true,\"first_month_upsell\":\"control\",\"enable_fedcm\":false,\"new_user_checklist_enabled\":\"use_follower_count\",\"android_enable_auto_gain_control\":false,\"enable_updated_webview_checkout\":false,\"show_attached_profile_for_pub_setting\":false,\"welcome_page_update_desktop_visuals_limited\":\"experiment\",\"rss_verification_code\":\"\",\"notification_post_emails\":\"experiment\",\"ios_profile_subdomain_chips\":true,\"chat_suppress_contributor_push_option_enabled\":false,\"live_stream_invite_ttl_seconds\":600,\"feed_ranking_per_post_clip_cap\":2,\"export_hooks_enabled\":false,\"audio_encoding_bitrate\":null,\"bestseller_pub_override\":false,\"extra_seats_coupon_type\":false,\"post_subdomain_universal_links\":false,\"post_import_max_file_size\":26214400,\"notes_weight_follow\":4,\"enable_post_to_post_link_data_event\":true,\"livekit_reconnect_slate_url\":\"https://mux-livestream-assets.s3.us-east-1.amazonaws.com/custom-disconnect-slate-tall.png\",\"exclude_from_pymk_suggestions\":false,\"web_enable_pays_for_you_tag\":false,\"disable_annual_subscriptions\":false,\"enable_android_dms\":false,\"feed_ranker_use_user_comment_reaction_cache\":true,\"pub_auto_moderation_enabled\":false,\"disable_live_stream_ai_trimming_by_default\":false,\"recipes_enabled\":false,\"disable_deletion\":false,\"ios_default_coupon_enabled\":false,\"notes_weight_read_post\":5,\"notes_weight_reply\":3,\"livekit_egress_custom_base_url\":\"http://livekit-egress-custom-recorder.s3-website-us-east-1.amazonaws.com\",\"clip_focused_video_upload_flow\":false,\"live_stream_max_guest_users\":2,\"enable_video_seo_data\":false,\"can_reimport_unsubscribed_users_with_2x_optin\":false,\"feed_posts_weight_subscribed\":0,\"included_in_demo_feed\":false,\"live_event_mixin\":\"\",\"review_incoming_email\":\"default\",\"app_install_prompts\":\"native_banner_if_supported\",\"enable_founding_gifts\":false,\"ios_chat_uikit\":false,\"enable_sponsorship_campaigns\":false,\"thread_permalink_preview_min_android_version\":2037,\"enable_fp_new_events_page\":true,\"direct_to_app_sources_enabled\":false,\"default_thumbnail_time\":10,\"pub_ranking_weight_immediate_engagement\":1,\"pub_ranking_weight_retained_engagement\":1,\"load_test_unichat\":false,\"notes_read_post_baseline\":0,\"live_stream_head_alignment_guide\":false,\"show_open_post_as_pdf_button\":false,\"free_press_combo_subscribe_flow_enabled\":false,\"restack_with_image\":false,\"free_press_tabbed_subscribe_flow\":\"control\",\"gift_from_substack_modal\":\"experiment\",\"gifts_from_substack_feature_available\":true,\"disable_ai_clips\":false,\"thefp_enable_web_livestream_kicking\":false,\"enable_elevenlabs_voiceovers\":false,\"growth_sources_all_time\":false,\"android_upgrade_alert_dialog\":true,\"headline_testing_enabled\":true,\"translated_notifications_enabled\":false,\"show_simple_post_editor\":false,\"desktop_live_streaming_enabled\":false,\"search_ranker_query_augmentation\":\"enabled\",\"enable_publication_podcasts_page\":false,\"ios_payment_connection_enabled\":true,\"app_install_reminder_email\":\"experiment\",\"ios_enable_pays_for_you_tag\":false,\"thefp_enable_dynamic_toaster\":false,\"thefp_enable_america_250\":true,\"ios_note_composer_settings_enabled\":false,\"android_v2_post_video_player_enabled\":false,\"enable_direct_message_request_bypass\":false,\"enable_apple_news_sync\":false,\"postsById_batch_size\":20,\"free_press_newsletter_promo_enabled\":false,\"enable_ios_livestream_stats\":false,\"disable_live_stream_reactions\":false,\"enable_high_follower_dm\":true,\"ios_welcome_video_profile_prompt\":false,\"clip_generation_3rd_party_vendor\":\"internal\",\"ios_notification_settings_enabled\":false,\"tone_down_sidebar_livestreams\":false,\"notes_weight_negative\":1,\"ios_discover_tab_min_installed_date\":\"2025-06-09T16:56:58+0000\",\"notes_weight_click_see_more\":2,\"enable_publish_youtube_connect_repeat_upsell\":false,\"edit_profile_theme_colors\":false,\"backend_enable_subscription_bar\":true,\"disable_clipping_for_readers\":false,\"android_enable_subscription_bar\":false,\"apple_fee_percent\":15,\"allow_anonymous_personal_pub_creation\":false,\"feed_posts_weight_reply\":3,\"feed_posts_weight_negative\":5,\"feed_posts_weight_like\":1.5,\"feed_posts_weight_share\":3,\"feed_posts_weight_save\":3,\"enable_press_kit_preview_modal\":false,\"dpn_weight_tap_clickbait_penalty\":0.5,\"feed_posts_weight_sign_up\":4,\"live_stream_video_degradation_preference\":\"maintainFramerate\",\"pause_app_badges\":false,\"android_enable_publication_activity_tab\":false,\"thefp_paywall_with_plans\":\"experiment\",\"notes_weight_like\":2,\"profile_feed_expanded_inventory\":false,\"phone_verification_fallback_to_twilio\":false,\"livekit_mux_latency_mode\":\"low\",\"feed_posts_weight_long_click\":1,\"feed_juiced_user\":0,\"vertical_video_player_in_feed_1\":\"experiment\",\"show_branded_intro_setting\":true,\"free_press_single_screen_subscribe_flow_enabled\":false,\"notes_click_see_more_baseline\":0.35,\"android_edit_user_links\":true,\"android_move_feed_tabs\":false,\"ios_inline_replies\":\"control\",\"android_enable_user_status_ui\":false,\"use_advanced_commerce_api_for_iap\":false,\"skip_free_preview_language_in_podcast_notes\":false,\"larger_wordmark_on_publication_homepage\":false,\"video_editor_full_screen\":false,\"enable_mobile_stats_for_admins\":false,\"ios_profile_themes_note_composer_enabled\":false,\"reduce_post_search_fuzziness\":\"treatment\",\"related_posts_web\":\"experiment\",\"notes_weight_click_item\":3,\"notes_weight_long_visit\":1,\"bypass_single_unlock_token_limit\":false,\"notes_watch_video_baseline\":0.08,\"add_section_and_tag_metadata\":false,\"daily_promoted_notes_enabled\":true,\"feed_ranker_use_user_feed_restack_comment_cache\":true,\"enable_islands_cms\":false,\"enable_livestream_combined_stats\":false,\"ios_social_subgroups_enabled\":false,\"enable_drip_campaigns\":false,\"ios_offline_mode_enabled\":false,\"post_management_search_engine\":\"elasticsearch\",\"new_bestseller_leaderboard_feed_item_enabled\":false,\"feed_main_disabled\":false,\"enable_account_settings_revamp\":false,\"allowed_email_domains\":\"one\",\"thefp_enable_fp_recirc_block\":false,\"ios_web_subscription_payments\":\"experiment\",\"ios_full_search_results\":\"control\",\"enable_debug_logs_ios\":false,\"show_pub_content_on_profile_for_pub_id\":0,\"web_badge_popover_treatment\":\"lottie\",\"show_pub_content_on_profile\":false,\"livekit_track_egress\":true,\"video_tab_mixture_pattern\":\"npnnnn\",\"enable_theme_contexts\":false,\"onboarding_suggestions_search\":\"experiment\",\"feed_tuner_enabled\":false,\"livekit_mux_latency_mode_rtmp\":\"low\",\"notes_weight_follow_boost\":3,\"fcm_high_priority\":false,\"subscription_bar_top_selection_strategy_v2\":\"destination_wau_pub_score\",\"search_ranker_load_test_pct\":0,\"iap_announcement_blog_url\":\"\",\"android_onboarding_progress_persistence\":\"control\",\"dpn_weight_tap_bonus_subscribed\":3,\"thefp_email_paywall_with_plans\":\"experiment\",\"ios_custom_buttons_enabled\":true,\"ios_livestream_feedback\":false,\"founding_plan_upgrade_warning\":false,\"suggested_search_ranking_v1\":\"control\",\"ios_iap_opt_out_enabled\":false,\"skip_kafka_retry_messages\":false,\"related_notes_variations\":\"control\",\"android_view_post_share_assets_employees_only\":false,\"thefp_show_fixed_footer_paywall\":false,\"android_subscription_queue_experiment\":\"experiment\",\"ios_viral_gift_entry_points\":\"treatment\",\"ios_post_video_pager_enabled_v2\":\"experiment\",\"render_high_quality_clips\":true,\"ios_subscription_pogs\":\"experiment\",\"android_enable_pays_for_you_tag\":false,\"dpn_weight_like\":2.5,\"use_elasticsearch_for_category_tabs\":\"control\",\"dpn_weight_reply\":2,\"age_verification_uk_rollout_percentage\":0,\"android_enable_edit_profile_theme\":false,\"android_enable_view_profile_theme\":false,\"enable_refresh_token_deduplication\":true,\"dpn_weight_follow\":3,\"live_stream_audio_enhancer_v2\":\"auphonic\",\"ios_user_status_sheet_subscribe_button\":\"control\",\"age_verification_uk_enabled\":false,\"enable_speaker_focus_clips\":true,\"notes_ranking_v90\":\"experiment\",\"search_ranker_load_test_ranking_window\":2000,\"ios_new_post_sharing_flow_enabled\":false,\"profile_feed_expanded_inventory_experiment_driven\":\"control\",\"ignore_video_in_notes_length_limit\":false,\"web_show_scores_on_sports_tab\":false,\"notes_weight_click_share\":3,\"direct_device_push_notifications\":false,\"allow_long_videos\":true,\"enable_livestream_rtmp_invites\":true,\"dpn_score_threshold\":0,\"thefp_enable_follow_module\":false,\"publication_user_invite_tier_2_only\":false,\"dpn_weight_follow_bonus\":0.5,\"ios_post_subscribe_follow_related\":\"control\",\"use_intro_clip_and_branded_intro_by_default\":false,\"ios_reader_post_sharing_flow_v2\":\"control\",\"community_profile_activity_feed\":false,\"new_user_subscribe_follow_prompt_override\":\"none\",\"ios_subscription_pogs_new_users\":\"experiment\",\"ios_subscription_pogs_old_users\":\"experiment\",\"android_subscription_queue_experiment_2\":\"experiment\",\"enable_viewing_all_livestream_viewers\":false,\"tabbed_notes_search\":\"control\",\"enable_clip_prompt_variant_filtering\":true,\"dpn_ranking_enabled\":true,\"sequential_retrieval_model_pct\":100,\"android_vertical_post_player_3\":\"experiment\",\"dpn_model_variant\":\"experiment\",\"add_byline_by_user_id_tier_2_only\":false,\"enable_outlier_subscriber_activity_item\":true,\"fake_pays_for_you_badge\":false,\"enable_apple_podcast_auto_publish\":false,\"dpn_weight_long_session\":2,\"dpn_weight_open\":1,\"enable_dashboard_data_callout\":true,\"speaker_focus_hls_usage_enabled\":true,\"ios_reader_post_sharing_flow\":\"control\",\"dpn_suggested_content_title\":\"control\",\"live_stream_in_trending_topic_overrides\":\"\",\"android_vertical_post_player\":\"control\",\"enable_notes_admins\":false,\"ios_post_embed_card_enabled\":true,\"enable_suggested_searches\":true,\"saved_post_reactivation_push_notification\":\"experiment\",\"android_synchronous_push_notif_handling\":\"control\",\"disable_user_status_for_user\":false,\"a24_redemption_link\":\"\",\"dpn_weight_negative\":10,\"suggested_search_metadata_web_market_ui\":false,\"dpn_weight_restack\":2,\"dpn_weight_tap\":2,\"podcast_subscribe_flow_app_upsell\":\"experiment\",\"session_version_invalidation_enabled\":false,\"publisher_banner\":\"\",\"ios_enable_subscription_stories\":false,\"direct_device_push_notifications_ios\":\"experiment\",\"forced_featured_topic_id\":\"\",\"android_vertical_post_player_2\":\"control\",\"web_notes_trending_topics_enabled\":\"control\",\"ios_live_stream_auto_gain_enabled\":false,\"search_retrieval_variant\":\"experiment\",\"android_rank_share_destinations_experiment\":\"control\",\"android_reader_share_assets\":\"control\",\"android_reader_share_assets_2\":\"control\",\"community_profile_activity_feed_recommendation_ratio\":0.5,\"get_app_pill_welcome_page_v2\":\"experiment\",\"ios_live_stream_pip_dismiss\":\"control\",\"feed_permalink_referred_modal\":\"experiment\",\"platform_search_space_insensitive_matches\":\"treatment\"},\"publicationSettings\":{\"block_ai_crawlers\":false,\"credit_token_enabled\":false,\"custom_tos_and_privacy\":false,\"did_identity\":null,\"disable_optimistic_bank_payments\":false,\"display_welcome_page_details\":true,\"enable_meetings\":false,\"payment_pledges_enabled\":true,\"enable_post_page_conversion\":true,\"enable_prev_next_nav\":false,\"enable_restacking\":true,\"gifts_from_substack_disabled\":false,\"google_analytics_4_token\":null,\"group_sections_and_podcasts_in_menu_enabled\":false,\"live_stream_homepage_visibility\":\"contributorsAndAdmins\",\"live_stream_homepage_style\":\"autoPlay\",\"medium_length_description\":\"\",\"notes_feed_enabled\":false,\"paywall_unlock_tokens\":false,\"post_preview_crop_gravity\":\"center\",\"reader_referrals_enabled\":false,\"reader_referrals_leaderboard_enabled\":false,\"seen_coming_soon_explainer\":false,\"seen_google_analytics_migration_modal\":false,\"local_currency_modal_seen\":true,\"local_payment_methods_modal_seen\":true,\"twitter_pixel_signup_event_id\":null,\"twitter_pixel_subscribe_event_id\":null,\"use_local_currency\":true,\"welcome_page_opt_out_text\":\"No thanks\",\"cookie_settings\":\"\",\"show_restacks_below_posts\":true,\"holiday_gifting_post_header\":false,\"homepage_message_text\":\"\",\"homepage_message_link\":\"\",\"about_us_author_ids\":\"\",\"archived_section_ids\":\"\",\"column_section_ids\":\"\",\"fp_primary_column_section_ids\":\"\",\"event_section_ids\":\"\",\"podcasts_metadata\":\"\",\"video_section_ids\":\"\",\"post_metering_enabled\":false},\"publicationUserSettings\":null,\"userSettings\":{\"user_id\":null,\"activity_likes_enabled\":true,\"artist_mode_enabled\":false,\"dashboard_nav_refresh_enabled\":false,\"hasDismissedSectionToNewsletterRename\":false,\"is_guest_post_enabled\":true,\"feed_web_nux_seen_at\":null,\"has_seen_select_to_restack_tooltip_nux\":false,\"invite_friends_nux_dismissed_at\":null,\"suggestions_feed_item_last_shown_at\":null,\"has_seen_select_to_restack_modal\":false,\"last_home_tab\":null,\"last_notification_alert_shown_at\":null,\"disable_reply_hiding\":false,\"newest_seen_chat_item_published_at\":null,\"explicitContentEnabled\":false,\"contactMatchingEnabled\":false,\"messageRequestLevel\":\"everyone\",\"liveStreamAcceptableInviteLevel\":\"everyone\",\"liveStreamAcceptableChatLevel\":\"everyone\",\"creditTokensTreatmentExposed\":false,\"appBadgeIncludesChat\":false,\"autoPlayVideo\":true,\"smart_delivery_enabled\":false,\"chatbotTermsLastAcceptedAt\":null,\"has_seen_notes_post_app_upsell\":false,\"substack_summer_nux_dismissed_at\":null,\"first_note_id\":null,\"show_concurrent_live_stream_viewers\":false,\"has_dismissed_fp_download_pdf_nux\":false,\"edit_profile_feed_item_dismissed_at\":null,\"mobile_permalink_app_upsell_seen_at\":null,\"new_user_checklist_enabled\":false,\"new_user_follow_subscribe_prompt_dismissed_at\":null,\"has_seen_youtube_shorts_auto_publish_announcement\":false,\"has_seen_publish_youtube_connect_upsell\":false,\"notificationQualityFilterEnabled\":true,\"hasSeenOnboardingNewslettersScreen\":false},\"subscriberCountDetails\":\"tens of thousands of subscribers\",\"mux_env_key\":\"u42pci814i6011qg3segrcpp9\",\"sentry_environment\":\"production\",\"launchWelcomePage\":false,\"pendingInviteForActiveLiveStream\":null,\"noIndex\":false,\"post\":{\"audience\":\"everyone\",\"audience_before_archived\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/andrej-karpathy\",\"default_comment_sort\":null,\"editor_v2\":false,\"exempt_from_archive_paywall\":false,\"free_unlock_required\":false,\"id\":176425744,\"podcast_art_url\":null,\"podcast_duration\":8719.281,\"podcast_preview_upload_id\":null,\"podcast_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/59851d57-8ed7-4904-9da0-a1c611dc620c/src\",\"post_date\":\"2025-10-17T16:54:33.052Z\",\"updated_at\":\"2025-10-23T18:53:10.664Z\",\"publication_id\":69345,\"search_engine_description\":null,\"search_engine_title\":null,\"section_id\":null,\"should_send_free_preview\":false,\"show_guest_bios\":true,\"slug\":\"andrej-karpathy\",\"social_title\":\"Andrej Karpathy \u2014 AGI is still a decade away\",\"subtitle\":\"\\\"The problems are tractable, but they're still difficult\u201D\",\"teaser_post_eligible\":true,\"title\":\"Andrej Karpathy \u2014 AGI is still a decade away\",\"type\":\"podcast\",\"video_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"write_comment_permissions\":\"everyone\",\"meter_type\":\"none\",\"live_stream_id\":null,\"is_published\":true,\"restacks\":51,\"reactions\":{\"\u2764\":224},\"top_exclusions\":[],\"pins\":[69345],\"section_pins\":[],\"has_shareable_clips\":true,\"previous_post_slug\":\"nick-lane\",\"next_post_slug\":null,\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/176425744/248940bd-cafb-4de4-bf60-6b24fae0fa2e/transcoded-1760720000.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"videoUpload\":{\"id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"name\":\"SUB0 Karpathy.mov\",\"created_at\":\"2025-10-17T15:45:15.207Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.336,\"height\":1080,\"width\":1920,\"thumbnail_id\":1760720000,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":12874298785,\"is_mux\":true,\"mux_asset_id\":\"l1NV65oRNv3QqVIoePiv01zgAygo9naA1E02c2etMnLa4\",\"mux_playback_id\":\"00jalP00100HXe4zcvoOVKJnUXYKFIsHb8TkVSwfClJeDE\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"name\":null,\"created_at\":\"2025-10-17T16:02:24.631Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.281,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":139509059,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"created_at\":\"2025-10-17T16:06:35.002Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K7SERJ8AEM97B0P3D4BZ0GXW\",\"approved_at\":\"2025-10-17T16:15:41.370Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=FHVsNF9PIV1VN-tjNAjsl4dsgk5rc1SXnUbS5PZcVYlmkOFhlznNxYlYgQ43ZekS5UIu0DAlY9Dq9wnmdKDpyHlDFIr9595Krl1risOCUGzBtHrs2sdDFf-w5xtnJOwwl8X-R0lW8aRYUIffq8vSAc9jYLu6iXmNBVD2pRgHMUrlqj449R7WaBmbDFcLKx3CoOu8h2NIr7UEtuY4RG19GWtdoZ6RdtaUijN0pnkbuQwsNyDHPJ4S-V3hH4oAFDFCZlEcimtxhe3O0cRfq~8QZLSm5o4TTJza2HZ4hcybMR2JxaWAGSUYh76pRZFJWDcEMiIGy~kX14H22Sg8qgoesA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/unaligned_transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=WiPDp-MMfScttlAjJ-E41Fi9kZxl7ozKUjo6dB0aoDv6Ce2nnLbiprCtIP1Xvvj5iUxwQhji06dYVUHC6JBdhcDkAGzCWk7NLuxIE2bmdAQrKJF6M8Cq-qb8X1gMvZbDYm1HQd-4Nf39CinyzwUOoQoaisGe2hcgcnv9LjvoGE~-yorw8n~h0UIl4xU8OcEVIc8gWEYpO0kxC4~-Of5pBaGeV0AwRPcxF7cZITDmy2gyZhctzXAcN5ro9tB8O7gyPCgrzBr03KxzGvfYbNXNKvs34AeS1nFAW6fF2qdcmlW4A2JOkfSgluP2gTVrXUYJeoKFZwF9A6vlD6Tcmiy5yQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hIEGBRT~Wivmj-J7hxBQqP3vVk8dc~AxD3li-ZuNrYrqr3NWuf~3X21JuQ~X-hPoiq3KCGPJ50~tcu8cDmTHx2qM~CFwOWM6s3yeL~1JfRFYtQl~~IcNhCUx7rrfd-G2uxWVZMNKTshFaWD0wA5SYgLNFmKsirU8lwAlSOeCL6JVAxqxOGqKYJYVZ8aoXm7GB8FYENtXf58RRpFOHNq7EfU9ffw~7nTFpLCzHcXVh5rttIYh3PifWAyS7mHaHglz8ig2AV7BywQDNgsf8rwoTvZjYucWMWI21iBQOPhhnrJE6ZZ2BLFAywZ2YYB-zZk8xkujzABYTAZHb720QCy3qQ__\",\"original\":true}]}}},\"podcastFields\":{\"post_id\":176425744,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcastUpload\":{\"id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"name\":null,\"created_at\":\"2025-10-17T16:02:24.631Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.281,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":139509059,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"created_at\":\"2025-10-17T16:06:35.002Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K7SERJ8AEM97B0P3D4BZ0GXW\",\"approved_at\":\"2025-10-17T16:15:41.370Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=FHVsNF9PIV1VN-tjNAjsl4dsgk5rc1SXnUbS5PZcVYlmkOFhlznNxYlYgQ43ZekS5UIu0DAlY9Dq9wnmdKDpyHlDFIr9595Krl1risOCUGzBtHrs2sdDFf-w5xtnJOwwl8X-R0lW8aRYUIffq8vSAc9jYLu6iXmNBVD2pRgHMUrlqj449R7WaBmbDFcLKx3CoOu8h2NIr7UEtuY4RG19GWtdoZ6RdtaUijN0pnkbuQwsNyDHPJ4S-V3hH4oAFDFCZlEcimtxhe3O0cRfq~8QZLSm5o4TTJza2HZ4hcybMR2JxaWAGSUYh76pRZFJWDcEMiIGy~kX14H22Sg8qgoesA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/unaligned_transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=WiPDp-MMfScttlAjJ-E41Fi9kZxl7ozKUjo6dB0aoDv6Ce2nnLbiprCtIP1Xvvj5iUxwQhji06dYVUHC6JBdhcDkAGzCWk7NLuxIE2bmdAQrKJF6M8Cq-qb8X1gMvZbDYm1HQd-4Nf39CinyzwUOoQoaisGe2hcgcnv9LjvoGE~-yorw8n~h0UIl4xU8OcEVIc8gWEYpO0kxC4~-Of5pBaGeV0AwRPcxF7cZITDmy2gyZhctzXAcN5ro9tB8O7gyPCgrzBr03KxzGvfYbNXNKvs34AeS1nFAW6fF2qdcmlW4A2JOkfSgluP2gTVrXUYJeoKFZwF9A6vlD6Tcmiy5yQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hIEGBRT~Wivmj-J7hxBQqP3vVk8dc~AxD3li-ZuNrYrqr3NWuf~3X21JuQ~X-hPoiq3KCGPJ50~tcu8cDmTHx2qM~CFwOWM6s3yeL~1JfRFYtQl~~IcNhCUx7rrfd-G2uxWVZMNKTshFaWD0wA5SYgLNFmKsirU8lwAlSOeCL6JVAxqxOGqKYJYVZ8aoXm7GB8FYENtXf58RRpFOHNq7EfU9ffw~7nTFpLCzHcXVh5rttIYh3PifWAyS7mHaHglz8ig2AV7BywQDNgsf8rwoTvZjYucWMWI21iBQOPhhnrJE6ZZ2BLFAywZ2YYB-zZk8xkujzABYTAZHb720QCy3qQ__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"\\\"The problems are tractable, but they're still difficult\u201D\",\"body_html\":\"<p>The <a href=\\\"https://x.com/karpathy\\\">Andrej Karpathy</a> episode.</p><p>Andrej explains why reinforcement learning is terrible (but everything else is much worse), why model collapse prevents LLMs from learning the way humans do, why AGI will just blend into the previous ~2.5 centuries of 2% GDP growth, why self driving took so long to crack, and what he sees as the future of education.</p><p>Watch on <a href=\\\"https://youtu.be/lXUZvyajciY\\\">YouTube</a>; listen on <a href=\\\"https://podcasts.apple.com/us/podcast/andrej-karpathy-agi-is-still-a-decade-away/id1516093381?i=1000732326311\\\">Apple Podcasts</a> or <a href=\\\"https://open.spotify.com/episode/3iIYVmmhXwh3fOumypWVpC?si=33d37708b2b44e2f\\\">Spotify</a>.</p><div id=\\\"youtube2-lXUZvyajciY\\\" class=\\\"youtube-wrap\\\" data-attrs=\\\"{&quot;videoId&quot;:&quot;lXUZvyajciY&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}\\\" data-component-name=\\\"Youtube2ToDOM\\\"><div class=\\\"youtube-inner\\\"><iframe src=\\\"https://www.youtube-nocookie.com/embed/lXUZvyajciY?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0\\\" frameborder=\\\"0\\\" loading=\\\"lazy\\\" gesture=\\\"media\\\" allow=\\\"autoplay; fullscreen\\\" allowautoplay=\\\"true\\\" allowfullscreen=\\\"true\\\" width=\\\"728\\\" height=\\\"409\\\"></iframe></div></div><h3>Sponsors</h3><ul><li><p><a href=\\\"https://labelbox.com/dwarkesh\\\">Labelbox</a> helps you get data that is more detailed, more accurate, and higher signal than you could get by default, no matter your domain or training paradigm. Reach out today at <a href=\\\"https://labelbox.com/dwarkesh\\\">labelbox.com/dwarkesh</a></p></li><li><p><a href=\\\"https://mercury.com\\\">Mercury</a> helps you run your business better. It\u2019s the banking platform we use for the podcast \u2014 we love that we can see our accounts, cash flows, AR, and AP all in one place. Apply online in minutes at <a href=\\\"https://mercury.com\\\">mercury.com</a></p></li><li><p><a href=\\\"https://blog.google/technology/ai/veo-updates-flow/\\\">Google\u2019s Veo 3.1</a> update is a notable improvement to an already great model. Veo 3.1\u2019s generations are more coherent and the audio is even higher-quality. If you have a Google AI Pro or Ultra plan, you can try it in Gemini today by visiting <a href=\\\"https://gemini.google\\\">https://gemini.google</a></p></li></ul><h3>Timestamps</h3><p><a href=\\\"https://www.dwarkesh.com/i/176425744/agi-is-still-a-decade-away\\\">(00:00:00) \u2013 AGI is still a decade away</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/llm-cognitive-deficits\\\">(00:29:45) \u2013 LLM cognitive deficits</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/rl-is-terrible\\\">(00:40:05) \u2013 RL is terrible</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/how-do-humans-learn\\\">(00:49:38) \u2013 How do humans learn?</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/agi-will-blend-into-gdp-growth\\\">(01:06:25) \u2013 AGI will blend into 2% GDP growth</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/asi\\\">(01:17:36) \u2013 ASI</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/evolution-of-intelligence-and-culture\\\">(01:32:50) \u2013 Evolution of intelligence &amp; culture</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/why-self-driving-took-so-long\\\">(01:42:55) - Why self driving took so long</a></p><p><a href=\\\"https://www.dwarkesh.com/i/176425744/future-of-education\\\">(01:56:20) - Future of education</a></p><h3>Transcript</h3><h3>00:00:00 \u2013 AGI is still a decade away</h3><p><strong>Dwarkesh Patel </strong><em>00:00:00</em></p><p>Today I\u2019m speaking with <a href=\\\"https://karpathy.ai/\\\">Andrej Karpathy</a>. Andrej, why do you say that this will be the decade of <a href=\\\"https://en.wikipedia.org/wiki/Agentic_AI\\\">agents</a> and not the year of agents?</p><p><strong>Andrej Karpathy </strong><em>00:00:07</em></p><p>First of all, thank you for having me here. I\u2019m excited to be here.</p><p>The quote you\u2019ve just mentioned, <a href=\\\"https://x.com/karpathy/status/1882544526033924438?lang=en\\\">\u201CIt\u2019s the decade of agents,\u201D</a> is actually a reaction to a pre-existing quote. I\u2019m not actually sure who said this but they were alluding to this being the year of agents with respect to <a href=\\\"https://en.wikipedia.org/wiki/Large_language_model\\\">LLMs</a> and how they were going to evolve. I was triggered by that because there\u2019s some over-prediction going on in the industry. In my mind, this is more accurately described as the decade of agents.</p><p>We have some very early agents that are extremely impressive and that I use daily\u2014Claude and <a href=\\\"https://en.wikipedia.org/wiki/OpenAI_Codex\\\">Codex</a> and so on\u2014but I still feel there\u2019s so much work to be done. My reaction is we\u2019ll be working with these things for a decade. They\u2019re going to get better, and it\u2019s going to be wonderful. I was just reacting to the timelines of the implication.</p><p><strong>Dwarkesh Patel </strong><em>00:00:58</em></p><p>What do you think will take a decade to accomplish? What are the bottlenecks?</p><p><strong>Andrej Karpathy </strong><em>00:01:02</em></p><p>Actually making it work. When you\u2019re talking about an agent, or what the labs have in mind and maybe what I have in mind as well, you should think of it almost like an employee or an intern that you would hire to work with you. For example, you work with some employees here. When would you prefer to have an agent like Claude or Codex do that work?</p><p>Currently, of course they can\u2019t. What would it take for them to be able to do that? Why don\u2019t you do it today? The reason you don\u2019t do it today is because they just don\u2019t work. They don\u2019t have enough intelligence, they\u2019re not <a href=\\\"https://en.wikipedia.org/wiki/Multimodal_learning\\\">multimodal</a> enough, they can\u2019t do <a href=\\\"https://a16z.com/the-rise-of-computer-use-and-agentic-coworkers/\\\">computer use</a> and all this stuff.</p><p>They don\u2019t do a lot of the things you\u2019ve alluded to earlier. They don\u2019t have <a href=\\\"https://en.wikipedia.org/wiki/Incremental_learning\\\">continual learning</a>. You can\u2019t just tell them something and they\u2019ll remember it. They\u2019re cognitively lacking and it\u2019s just not working. It will take about a decade to work through all of those issues.</p><p><strong>Dwarkesh Patel </strong><em>00:01:44</em></p><p>Interesting. As a professional podcaster and a viewer of AI from afar, it\u2019s easy for me to identify what\u2019s lacking: continual learning is lacking, or multimodality is lacking. But I don\u2019t really have a good way of trying to put a timeline on it. If somebody asks how long continual learning will take, I have no prior about whether this is a project that should take 5 years, 10 years, or 50 years. Why a decade? Why not one year? Why not 50 years?</p><p><strong>Andrej Karpathy </strong><em>00:02:16</em></p><p>This is where you get into a bit of my own intuition, and doing a bit of an extrapolation with respect to my own experience in the field. I\u2019ve been in AI for almost two decades. It\u2019s going to be 15 years or so, not that long. You had <a href=\\\"https://www.dwarkesh.com/p/richard-sutton\\\">Richard Sutton</a> here, who was around for much longer. I do have about 15 years of experience of people making predictions, of seeing how they turned out. Also I was in the industry for a while, I was in research, and I\u2019ve worked in the industry for a while. I have a general intuition that I have left from that.</p><p>I feel like the problems are tractable, they\u2019re surmountable, but they\u2019re still difficult. If I just average it out, it just feels like a decade to me.</p><p><strong>Dwarkesh Patel </strong><em>00:02:57</em></p><p>This is quite interesting. I want to hear not only the history, but what people in the room felt was about to happen at various different breakthrough moments. What were the ways in which their feelings were either overly pessimistic or overly optimistic? Should we just go through each of them one by one?</p><p><strong>Andrej Karpathy </strong><em>00:03:16</em></p><p>That\u2019s a giant question because you\u2019re talking about 15 years of stuff that happened. AI is so wonderful because there have been a number of seismic shifts where the entire field has suddenly looked a different way. I\u2019ve maybe lived through two or three of those. I still think there will continue to be some because they come with almost surprising regularity.</p><p>When my career began, when I started to work on <a href=\\\"https://en.wikipedia.org/wiki/Deep_learning\\\">deep learning</a>, when I became interested in deep learning, this was by chance of being right next to <a href=\\\"https://en.wikipedia.org/wiki/Geoffrey_Hinton\\\">Geoff Hinton</a> at the University of Toronto. Geoff Hinton, of course, is the godfather figure of AI. He was training all these <a href=\\\"https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\\\">neural networks</a>. I thought it was incredible and interesting. This was not the main thing that everyone in AI was doing by far. This was a niche little subject on the side. That\u2019s maybe the first dramatic seismic shift that came with the AlexNet and so on.</p><p>AlexNet reoriented everyone, and everyone started to train neural networks, but it was still very per-task, per specific task. Maybe I have an image classifier or I have a neural machine translator or something like that. People became very slowly interested in agents. People started to think, \u201COkay, maybe we have a check mark next to the visual cortex or something like that, but what about the other parts of the brain, and how can we get a full agent or a full entity that can interact in the world?\u201D</p><p>The <a href=\\\"https://arxiv.org/abs/1312.5602\\\">Atari deep reinforcement learning shift in 2013</a> or so was part of that early effort of agents, in my mind, because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. At the time, this was Atari games.</p><p>I feel that was a misstep. It was a misstep that even the early <a href=\\\"https://en.wikipedia.org/wiki/OpenAI\\\">OpenAI</a> that I was a part of adopted because at that time, the zeitgeist was <a href=\\\"https://en.wikipedia.org/wiki/Reinforcement_learning\\\">reinforcement learning</a> environments, games, game playing, beat games, get lots of different types of games, and OpenAI was doing a lot of that. That was another prominent part of AI where maybe for two or three or four years, everyone was doing reinforcement learning on games. That was all a bit of a misstep.</p><p>What I was trying to do at OpenAI is I was always a bit suspicious of games as being this thing that would lead to <a href=\\\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\\\">AGI</a>. Because in my mind, you want something like an accountant or something that\u2019s interacting with the real world. I just didn\u2019t see how games add up to it. My project at OpenAI, for example, was within the scope of the <a href=\\\"https://openai.com/index/universe/\\\">Universe project</a>, on an agent that was using keyboard and mouse to operate web pages. I really wanted to have something that interacts with the actual digital world that can do knowledge work.</p><p>It just so turns out that this was extremely early, way too early, so early that we shouldn\u2019t have been working on that. Because if you\u2019re just stumbling your way around and keyboard mashing and mouse clicking and trying to get rewards in these environments, your reward is too sparse and you just won\u2019t learn. You\u2019re going to burn a forest computing, and you\u2019re never going to get something off the ground. What you\u2019re missing is this power of representation in the neural network.</p><p>For example, today people are training those computer-using agents, but they\u2019re doing it on top of a large language model. You have to get the language model first, you have to get the representations first, and you have to do that by all the <a href=\\\"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\\\">pre-training</a> and all the LLM stuff.</p><p>I feel maybe loosely speaking, people kept trying to get the full thing too early a few times, where people really try to go after agents too early, I would say. That was Atari and Universe and even my own experience. You actually have to do some things first before you get to those agents. Now the agents are a lot more competent, but maybe we\u2019re still missing some parts of that stack.</p><p>I would say those are the three major buckets of what people were doing: training neural nets per-tasks, trying the first round of agents, and then maybe the LLMs and seeking the representation power of the neural networks before you tack on everything else on top.</p><p><strong>Dwarkesh Patel </strong><em>00:07:02</em></p><p>Interesting. If I were to <a href=\\\"https://en.wikipedia.org/wiki/Straw_man#Steelmanning\\\">steelman</a> the <a href=\\\"https://www.dwarkesh.com/p/richard-sutton\\\">Sutton perspective</a>, it would be that humans can just take on everything at once, or even animals can take on everything at once. Animals are maybe a better example because they don\u2019t even have the scaffold of language. They just get thrown out into the world, and they just have to make sense of everything without any labels.</p><p>The vision for AGI then should just be something which looks at sensory data, looks at the computer screen, and it just figures out what\u2019s going on from scratch. If a human were put in a similar situation and had to be trained from scratch\u2026 This is like a human growing up or an animal growing up. Why shouldn\u2019t that be the vision for AI, rather than this thing where we\u2019re doing millions of years of training?</p><p><strong>Andrej Karpathy </strong><em>00:07:41</em></p><p>That\u2019s a really good question. Sutton was on your podcast and I saw the podcast and <a href=\\\"https://x.com/karpathy/status/1973435013875314729\\\">I had a write-up about that podcast</a> that gets into a bit of how I see things. I\u2019m very careful to make analogies to animals because they came about by a very different optimization process. Animals are evolved, and they come with a huge amount of hardware that\u2019s built in.</p><p>For example, my example in the post was the zebra. A zebra gets born, and a few minutes later it\u2019s running around and following its mother. That\u2019s an extremely complicated thing to do. That\u2019s not reinforcement learning. That\u2019s something that\u2019s baked in. Evolution obviously has some way of encoding the <a href=\\\"https://deepai.org/machine-learning-glossary-and-terms/weight-artificial-neural-network\\\">weights</a> of our neural nets in <a href=\\\"https://en.wikipedia.org/wiki/Nucleotide_base\\\">ATCGs</a>, and I have no idea how that works, but it apparently works.</p><p>Brains just came from a very different process, and I\u2019m very hesitant to take inspiration from it because we\u2019re not actually running that process. In my post, I said we\u2019re not building animals. We\u2019re building ghosts or spirits or whatever people want to call it, because we\u2019re not doing training by evolution. We\u2019re doing training by imitation of humans and the data that they\u2019ve put on the Internet.</p><p>You end up with these ethereal spirit entities because they\u2019re fully digital and they\u2019re mimicking humans. It\u2019s a different kind of intelligence. If you imagine a space of intelligences, we\u2019re starting off at a different point almost. We\u2019re not really building animals. But it\u2019s also possible to make them a bit more animal-like over time, and I think we should be doing that.</p><p>One more point. I do feel Sutton has a very... His framework is, \u201CWe want to build animals.\u201D I think that would be wonderful if we can get that to work. That would be amazing. If there were a single algorithm that you can just run on the Internet and it learns everything, that would be incredible. I\u2019m not sure that it exists and that\u2019s certainly not what animals do, because animals have this outer loop of evolution.</p><p>A lot of what looks like learning is more like maturation of the brain. I think there\u2019s very little reinforcement learning for animals. A lot of the reinforcement learning is more like motor tasks; it\u2019s not intelligence tasks. So I actually kind of think humans don\u2019t really use RL, roughly speaking.</p><p><strong>Dwarkesh Patel </strong><em>00:09:52</em></p><p>Can you repeat the last sentence? A lot of that intelligence is not motor task\u2026it\u2019s what, sorry?</p><p><strong>Andrej Karpathy </strong><em>00:09:54</em></p><p>A lot of the reinforcement learning, in my perspective, would be things that are a lot more motor-like, simple tasks like throwing a hoop. But I don\u2019t think that humans use reinforcement learning for a lot of intelligence tasks like problem-solving and so on.<strong> </strong>That doesn\u2019t mean we shouldn\u2019t do that for research, but I just feel like that\u2019s what animals do or don\u2019t.</p><p><strong>Dwarkesh Patel </strong><em>00:10:17</em></p><p>I\u2019m going to take a second to digest that because there are a lot of different ideas. Here\u2019s one clarifying question I can ask to understand the perspective. You suggest that evolution is doing the kind of thing that pre-training does in the sense of building something which can then understand the world.</p><p>The difference is that evolution has to be titrated in the case of humans through three gigabytes of DNA. That\u2019s very unlike the weights of a model. Literally, the weights of the model are a brain, which obviously does not exist in the sperm and the egg. So it has to be grown. Also, the information for every single synapse in the brain simply cannot exist in the three gigabytes that exist in the DNA.</p><p>Evolution seems closer to finding the algorithm which then does the lifetime learning. Now, maybe the lifetime learning is not analogous to RL, to your point. Is that compatible with the thing you were saying, or would you disagree with that?</p><p><strong>Andrej Karpathy </strong><em>00:11:17</em></p><p>I think so. I would agree with you that there\u2019s some miraculous compression going on because obviously, the weights of the neural net are not stored in ATCGs. There\u2019s some dramatic compression. There are some learning algorithms encoded that take over and do some of the learning online. I definitely agree with you on that. I would say I\u2019m a lot more practically minded. I don\u2019t come at it from the perspective of, let\u2019s build animals. I come from it from the perspective of, let\u2019s build useful things. I have a hard hat on, and I\u2019m just observing that we\u2019re not going to do evolution, because I don\u2019t know how to do that.</p><p>But it does turn out we can build these ghosts, spirit-like entities, by imitating internet documents. This works. It\u2019s a way to bring you up to something that has a lot of built-in knowledge and intelligence in some way, similar to maybe what evolution has done. That\u2019s why I call pre-training this crappy evolution. It\u2019s the practically possible version with our technology and what we have available to us to get to a starting point where we can do things like reinforcement learning and so on.</p><p><strong>Dwarkesh Patel </strong><em>00:12:15</em></p><p>Just to steelman the other perspective, after doing this Sutton interview and thinking about it a bit, he has an important point here. Evolution does not give us the knowledge, really. It gives us the algorithm to find the knowledge, and that seems different from pre-training.</p><p>Perhaps the perspective is that pre-training helps build the kind of entity which can learn better. It teaches meta-learning, and therefore it is similar to finding an algorithm. But if it\u2019s \u201CEvolution gives us knowledge, pre-training gives us knowledge,\u201D that analogy seems to break down.</p><p><strong>Andrej Karpathy </strong><em>00:12:42</em></p><p>It\u2019s subtle and I think you\u2019re right to push back on it, but basically the thing that pre-training is doing, you\u2019re getting the <a href=\\\"https://research.google/pubs/mechanics-of-next-token-prediction-with-transformers/\\\">next-token predictor</a> over the internet, and you\u2019re training that into a neural net. It\u2019s doing two things that are unrelated. Number one, it\u2019s picking up all this knowledge, as I call it. Number two, it\u2019s actually becoming intelligent.</p><p>By observing the algorithmic patterns in the internet, it boots up all these little circuits and algorithms inside the neural net to do things like <a href=\\\"https://www.hopsworks.ai/dictionary/in-context-learning-icl#:~:text=In%2Dcontext%20learning%20(ICL)%20learns%20a%20new%20task%20from,objective%20of%20next%20token%20prediction.\\\">in-context learning</a> and all this stuff. You don\u2019t need or want the knowledge. I think that\u2019s probably holding back the neural networks overall because it\u2019s getting them to rely on the knowledge a little too much sometimes.</p><p>For example, I feel agents, one thing they\u2019re not very good at, is going off the data manifold of what exists on the internet. If they had less knowledge or less memory, maybe they would be better. What I think we have to do going forward\u2014and this would be part of the research paradigms\u2014is figure out ways to remove some of the knowledge and to keep what I call this <a href=\\\"https://x.com/karpathy/status/1938626382248149433\\\">cognitive core</a>. It\u2019s this intelligent entity that is stripped from knowledge but contains the algorithms and contains the magic of intelligence and problem-solving and the strategies of it and all this stuff.</p><p><strong>Dwarkesh Patel </strong><em>00:13:50</em></p><p>There\u2019s so much interesting stuff there. Let\u2019s start with in-context learning. This is an obvious point, but I think it\u2019s worth just saying it explicitly and meditating on it. The situation in which these models seem the most intelligent\u2014in which I talk to them and I\u2019m like, \u201CWow, there\u2019s really something on the other end that\u2019s responding to me thinking about things\u2014is if it makes a mistake it\u2019s like, \u201COh wait, that\u2019s the wrong way to think about it. I\u2019m backing up.\u201D All that is happening in context. That\u2019s where I feel like the real intelligence is that you can visibly see.</p><p>That in-context learning process is developed by <a href=\\\"https://en.wikipedia.org/wiki/Gradient_descent\\\">gradient descent</a> on pre-training. It spontaneously meta-learns in-context learning, but the in-context learning itself is not gradient descent, in the same way that our lifetime intelligence as humans to be able to do things is conditioned by evolution but our learning during our lifetime is happening through some other process.</p><p><strong>Andrej Karpathy </strong><em>00:14:42</em></p><p>I don\u2019t fully agree with that, but you should continue your thought.</p><p><strong>Dwarkesh Patel </strong><em>00:14:44</em></p><p>Well, I\u2019m very curious to understand how that analogy breaks down.</p><p><strong>Andrej Karpathy </strong><em>00:14:48</em></p><p>I\u2019m hesitant to say that in-context learning is not doing gradient descent. It\u2019s not doing explicit gradient descent. In-context learning is pattern completion within a <a href=\\\"https://blogs.nvidia.com/blog/ai-tokens-explained/\\\">token</a> window. It just turns out that there\u2019s a huge amount of patterns on the internet. You\u2019re right, the model learns to complete the pattern, and that\u2019s inside the weights. The weights of the neural network are trying to discover patterns and complete the pattern. There\u2019s some adaptation that happens inside the neural network, which is magical and just falls out from the internet just because there\u2019s a lot of patterns.</p><p>I will say that there have been some papers that I thought were interesting that look at the mechanisms behind in-context learning. I do think it\u2019s possible that in-context learning runs a small gradient descent loop internally in the layers of the neural network. <a href=\\\"https://arxiv.org/abs/2211.15661\\\">I recall one paper in particular</a> where they were doing linear regression using in-context learning. Your inputs into the neural network are XY pairs, XY, XY, XY that happen to be on the line. Then you do X and you expect Y. The neural network, when you train it in this way, does <a href=\\\"https://en.wikipedia.org/wiki/Linear_regression\\\">linear regression</a>.</p><p>Normally when you would run linear regression, you have a small gradient descent <a href=\\\"https://en.wikipedia.org/wiki/Mathematical_optimization\\\">optimizer</a> that looks at XY, looks at an error, calculates the gradient of the weights and does the update a few times. It just turns out that when they looked at the weights of that in-context learning algorithm, they found some analogies to gradient descent mechanics. In fact, I think the paper was even stronger because they hardcoded the weights of a neural network to do gradient descent through attention and all the internals of the neural network.</p><p>That\u2019s just my only pushback. Who knows how in-context learning works, but I think that it\u2019s probably doing a bit of some funky gradient descent internally. I think that that\u2019s possible. I was only pushing back on your saying that it\u2019s not doing in-context learning. Who knows what it\u2019s doing, but it\u2019s probably maybe doing something similar to it, but we don\u2019t know.</p><p><strong>Dwarkesh Patel </strong><em>00:16:39</em></p><p>So then it\u2019s worth thinking okay, if in-context learning and pre-training are both implementing something like gradient descent, why does it feel like with in-context learning we\u2019re getting to this continual learning, real intelligence-like thing? Whereas you don\u2019t get the analogous feeling just from pre-training. You could argue that.</p><p>If it\u2019s the same algorithm, what could be different? One way you could think about it is, how much information does the model store per information it receives from training? If you look at pre-training, if you look at <a href=\\\"https://www.llama.com/models/llama-3/\\\">Llama 3</a> for example, I think it\u2019s trained on 15 trillion tokens. If you look at the 70B model, that would be the equivalent of 0.07 bits per token that it sees in pre-training, in terms of the information in the weights of the model compared to the tokens it reads. Whereas if you look at the <a href=\\\"https://huggingface.co/blog/not-lain/kv-caching\\\">KV cache</a> and how it grows per additional token in in-context learning, it\u2019s like 320 kilobytes. So that\u2019s a 35 million-fold difference in how much information per token is assimilated by the model. I wonder if that\u2019s relevant at all.</p><p><strong>Andrej Karpathy </strong><em>00:17:46</em></p><p>I kind of agree. The way I usually put this is that anything that happens during the training of the neural network, the knowledge is only a hazy recollection of what happened in training time. That\u2019s because the compression is dramatic. You\u2019re taking 15 trillion tokens and you\u2019re compressing it to just your final neural network of a few billion parameters. Obviously it\u2019s a massive amount of compression going on. So I refer to it as a hazy recollection of the internet documents.</p><p>Whereas anything that happens in the context window of the neural network\u2014you\u2019re plugging in all the tokens and building up all those KV cache representations\u2014is very directly accessible to the neural net. So I compare the KV cache and the stuff that happens at test time to more like a working memory. All the stuff that\u2019s in the context window is very directly accessible to the neural net.</p><p>There\u2019s always these almost surprising analogies between LLMs and humans. I find them surprising because we\u2019re not trying to build a human brain directly. We\u2019re just finding that this works and we\u2019re doing it. But I do think that anything that\u2019s in the weights, it\u2019s a hazy recollection of what you read a year ago. Anything that you give it as a context at test time is directly in the working memory. That\u2019s a very powerful analogy to think through things.</p><p>When you, for example, go to an LLM and you ask it about some book and what happened in it, like <a href=\\\"https://amzn.to/3KROqqU\\\">Nick Lane\u2019s book</a> or something like that, the LLM will often give you some stuff which is roughly correct. But if you give it the full chapter and ask it questions, you\u2019re going to get much better results because it\u2019s now loaded in the working memory of the model. So a very long way of saying I agree and that\u2019s why.</p><p><strong>Dwarkesh Patel </strong><em>00:19:11</em></p><p>Stepping back, what is the part about human intelligence that we have most failed to replicate with these models?</p><p><strong>Andrej Karpathy </strong><em>00:19:20</em></p><p>Just a lot of it. So maybe one way to think about it, I don\u2019t know if this is the best way, but I almost feel like \u2014 again, making these analogies imperfect as they are \u2014 we\u2019ve stumbled by with the <a href=\\\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\\\">transformer</a> neural network, which is extremely powerful, very general. You can train transformers on audio, or video, or text, or whatever you want, and it just learns patterns and they\u2019re very powerful, and it works really well. That to me almost indicates that this is some piece of <a href=\\\"https://en.wikipedia.org/wiki/Cerebral_cortex\\\">cortical tissue</a>. It\u2019s something like that, because the cortex is famously very plastic as well. You can rewire parts of brains. <a href=\\\"https://news.mit.edu/2000/brain\\\">There were the slightly gruesome experiments</a> with rewiring the visual cortex to the auditory cortex, and this animal learned fine, et cetera.</p><p>So I think that this is cortical tissue. I think when we\u2019re doing reasoning and planning inside the neural networks, doing reasoning traces for thinking models, that\u2019s kind of like the <a href=\\\"https://en.wikipedia.org/wiki/Prefrontal_cortex\\\">prefrontal cortex</a>. Maybe those are like little checkmarks, but I still think there are many brain parts and nuclei that are not explored. For example, there\u2019s a <a href=\\\"https://en.wikipedia.org/wiki/Basal_ganglia\\\">basal ganglia</a> doing a bit of reinforcement learning when we <a href=\\\"https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)\\\">fine-tune</a> the models on reinforcement learning. But where\u2019s the <a href=\\\"https://en.wikipedia.org/wiki/Hippocampus\\\">hippocampus</a>? Not obvious what that would be. Some parts are probably not important. Maybe the <a href=\\\"https://en.wikipedia.org/wiki/Cerebellum\\\">cerebellum</a> is not important to cognition, its thoughts, so maybe we can skip some of it. But I still think there\u2019s, for example, the <a href=\\\"https://en.wikipedia.org/wiki/Amygdala\\\">amygdala</a>, all the emotions and instincts. There\u2019s probably a bunch of other nuclei in the brain that are very ancient that I don\u2019t think we\u2019ve really replicated.</p><p>I don\u2019t know that we should be pursuing the building of an analog of a human brain. I\u2019m an engineer mostly at heart. Maybe another way to answer the question is that you\u2019re not going to hire this thing as an intern. It\u2019s missing a lot of it because it comes with a lot of these cognitive deficits that we all intuitively feel when we talk to the models. So it\u2019s not fully there yet. You can look at it as not all the brain parts are checked off yet.</p><p><strong>Dwarkesh Patel </strong><em>00:21:16</em></p><p>This is maybe relevant to the question of thinking about how fast these issues will be solved. Sometimes people will say about continual learning, \u201CLook, you could easily replicate this capability. Just as in-context learning emerged spontaneously as a result of pre-training, continual learning over longer horizons will emerge spontaneously if the model is incentivized to recollect information over longer horizons, or horizons longer than one session.\u201D So if there\u2019s some outer loop RL which has many sessions within that outer loop, then this continual learning where it fine-tunes itself, or it writes to an external memory or something, will just emerge spontaneously. Do you think things like that are plausible? I just don\u2019t have a prior over how plausible that is. How likely is that to happen?</p><p><strong>Andrej Karpathy </strong><em>00:22:07</em></p><p>I don\u2019t know that I fully resonate with that. These models, when you boot them up and they have zero tokens in the window, they\u2019re always restarting from scratch where they were. So I don\u2019t know in that worldview what it looks like. Maybe making some analogies to humans\u2014just because I think it\u2019s roughly concrete and interesting to think through\u2014I feel like when I\u2019m awake, I\u2019m building up a context window of stuff that\u2019s happening during the day. But when I go to sleep, something magical happens where I don\u2019t think that context window stays around. There\u2019s some process of distillation into the weights of my brain. This happens during sleep and all this stuff.</p><p>We don\u2019t have an equivalent of that in large language models. That\u2019s to me more adjacent to when you talk about continual learning and so on as absent. These models don\u2019t really have a distillation phase of taking what happened, analyzing it obsessively, thinking through it, doing some synthetic data generation process and distilling it back into the weights, and maybe having a specific neural net per person. Maybe it\u2019s a <a href=\\\"https://www.ibm.com/think/topics/lora\\\">LoRA</a>. It\u2019s not a full-weight neural network. It\u2019s just some small sparse subset of the weights that are changed.</p><p>But we do want to create ways of creating these individuals that have very long context. It\u2019s not only remaining in the context window because the context windows grow very, very long. Maybe we have some very elaborate, sparse attention over it. But I still think that humans obviously have some process for distilling some of that knowledge into the weights. We\u2019re missing it. I do also think that humans have some very elaborate, <a href=\\\"https://medium.com/@vishal09vns/sparse-attention-dad17691478c\\\">sparse attention scheme</a>, which I think we\u2019re starting to see some early hints of. <a href=\\\"https://api-docs.deepseek.com/news/news250929\\\">DeepSeek v3.2</a> just came out and I saw that they have sparse attention as an example, and this is one way to have very, very long context windows. So I feel like we are redoing a lot of the cognitive tricks that evolution came up with through a very different process. But we\u2019re going to converge on a similar architecture cognitively.</p><p><strong>Dwarkesh Patel </strong><em>00:24:02</em></p><p>In 10 years, do you think it\u2019ll still be something like a transformer, but with much more modified attention and more sparse <a href=\\\"https://en.wikipedia.org/wiki/Multilayer_perceptron\\\">MLPs</a> and so forth?</p><p><strong>Andrej Karpathy </strong><em>00:24:10</em></p><p>The way I like to think about it is <a href=\\\"https://en.wikipedia.org/wiki/Translational_symmetry\\\">translation invariance</a> in time. So 10 years ago, where were we? 2015. In 2015, we had <a href=\\\"https://en.wikipedia.org/wiki/Convolutional_neural_network\\\">convolutional neural networks</a> primarily, <a href=\\\"https://en.wikipedia.org/wiki/Residual_neural_network\\\">residual networks</a> just came out. So remarkably similar, I guess, but quite a bit different still. The transformer was not around. All these more modern tweaks on the transformer were not around. Maybe some of the things that we can bet on, I think in 10 years by translational equivariance, is that we\u2019re still training giant neural networks with a <a href=\\\"https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc/\\\">forward backward pass</a> and update through gradient descent, but maybe it looks a bit different, and it\u2019s just that everything is much bigger.</p><p>Recently I went back all the way to 1989 which was a fun exercise for me, a few years ago, because I was reproducing <a href=\\\"https://en.wikipedia.org/wiki/Yann_LeCun\\\">Yann LeCun\u2019s</a> <a href=\\\"https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf\\\">1989 convolutional network</a>, which was <a href=\\\"https://en.wikipedia.org/wiki/LeNet\\\">the first neural network</a> I\u2019m aware of trained via gradient descent, like modern neural network trained gradient descent on digit recognition. I was just interested in how I could modernize this. How much of this is algorithms? How much of this is data? How much of this progress is compute and systems? I was able to very quickly halve the learning just by time traveling by 33 years.</p><p>So if I time travel by algorithms 33 years, I could adjust what Yann LeCun did in 1989, and I could halve the error. But to get further gains, I had to add a lot more data, I had to 10x the training set, and then I had to add more computational optimizations. I had to train for much longer with dropout and other regularization techniques.</p><p>So all these things have to improve simultaneously. We\u2019re probably going to have a lot more data, we\u2019re probably going to have a lot better hardware, probably going to have a lot better <a href=\\\"https://en.wikipedia.org/wiki/Compute_kernel\\\">kernels</a> and software, we\u2019re probably going to have better algorithms. All of those, it\u2019s almost like no one of them is winning too much. All of them are surprisingly equal. This has been the trend for a while.</p><p>So to answer your question, I expect differences algorithmically to what\u2019s happening today. But I do also expect that some of the things that have stuck around for a very long time will probably still be there. It\u2019s probably still a giant neural network trained with gradient descent. That would be my guess.</p><p><strong>Dwarkesh Patel </strong><em>00:26:16</em></p><p>It\u2019s surprising that all of those things together only halved the error, 30 years of progress\u2026. Maybe half is a lot. Because if you halve the error, that actually means that\u2026</p><p><strong>Andrej Karpathy </strong><em>00:26:30</em></p><p>Half is a lot. But I guess what was shocking to me is everything needs to improve across the board: architecture, optimizer, loss function. It also has improved across the board forever. So I expect all those changes to be alive and well.</p><p><strong>Dwarkesh Patel </strong><em>00:26:43</em></p><p>Yeah. I was about to ask you a very similar question about <a href=\\\"https://x.com/karpathy/status/1977755427569111362\\\">nanochat</a>. Since you just coded it up recently, every single step in the process of building a chatbot is fresh in your RAM. I\u2019m curious if you had similar thoughts about, \u201COh, there was no one thing that was relevant to going from GPT-2 to nanochat.\u201D What are some surprising takeaways from the experience?</p><p><strong>Andrej Karpathy </strong><em>00:27:08</em></p><p>Of building nanochat? So nanochat is a repository I released. Was it yesterday or the day before? I can\u2019t remember.</p><p><strong>Dwarkesh Patel </strong><em>00:27:15</em></p><p>We can see the sleep deprivation that went into the\u2026</p><p><strong>Andrej Karpathy </strong><em>00:27:18</em></p><p>It\u2019s trying to be the simplest complete repository that covers the whole pipeline end-to-end of building a ChatGPT clone. So you have all of the steps, not just any individual step, which is a bunch. I worked on all the individual steps in the past and released small pieces of code that show you how that\u2019s done in an algorithmic sense, in simple code. But this handles the entire pipeline. In terms of learning, I don\u2019t know that I necessarily found something that I learned from it. I already had in my mind how you build it. This is just the process of mechanically building it and making it clean enough so that people can learn from it and that they find it useful.</p><p><strong>Dwarkesh Patel </strong><em>00:28:04</em></p><p>What is the best way for somebody to learn from it? Is it to just delete all the code and try to reimplement from scratch, try to add modifications to it?</p><p><strong>Andrej Karpathy </strong><em>00:28:10</em></p><p>That\u2019s a great question. Basically it\u2019s about 8,000 lines of code that takes you through the entire pipeline. I would probably put it on the right monitor. If you have two monitors, you put it on the right. You want to build it from scratch, you build it from the start. You\u2019re not allowed to copy-paste, you\u2019re allowed to reference, you\u2019re not allowed to copy-paste. Maybe that\u2019s how I would do it.</p><p>But I also think the repository by itself is a pretty large beast. When you write this code, you don\u2019t go from top to bottom, you go from chunks and you grow the chunks, and that information is absent. You wouldn\u2019t know where to start. So it\u2019s not just a final repository that\u2019s needed, it\u2019s the building of the repository, which is a complicated chunk-growing process. So that part is not there yet. I would love to add that probably later this week. It\u2019s probably a video or something like that. Roughly speaking, that\u2019s what I would try to do. Build the stuff yourself, but don\u2019t allow yourself copy-paste.</p><p>I do think that there\u2019s two types of knowledge, almost. There\u2019s the high-level surface knowledge, but when you build something from scratch, you\u2019re forced to come to terms with what you don\u2019t understand and you don\u2019t know that you don\u2019t understand it.</p><p>It always leads to a deeper understanding. It\u2019s the only way to build. If I can\u2019t build it, I don\u2019t understand it. That\u2019s a <a href=\\\"https://en.wikipedia.org/wiki/Richard_Feynman\\\">Feynman</a> <a href=\\\"https://www.quora.com/What-did-Richard-Feynman-mean-when-he-said-What-I-cannot-create-I-do-not-understand\\\">quote</a>, I believe. I 100% have always believed this very strongly, because there are all these micro things that are just not properly arranged and you don\u2019t really have the knowledge. You just think you have the knowledge. So don\u2019t write blog posts, don\u2019t do slides, don\u2019t do any of that. Build the code, arrange it, get it to work. It\u2019s the only way to go. Otherwise, you\u2019re missing knowledge.</p><h3>00:29:45 \u2013 LLM cognitive deficits</h3><p><strong>Dwarkesh Patel </strong><em>00:29:45</em></p><p>You tweeted out that coding models were of very little help to you in assembling this repository. I\u2019m curious why that was.</p><p><strong>Andrej Karpathy </strong><em>00:29:53</em></p><p>I guess I built the repository over a period of a bit more than a month. I would say there are three major classes of how people interact with code right now. Some people completely reject all of LLMs and they are just writing by scratch. This is probably not the right thing to do anymore.</p><p>The intermediate part, which is where I am, is you still write a lot of things from scratch, but you use the autocomplete that\u2019s available now from these models. So when you start writing out a little piece of it, it will autocomplete for you and you can just tap through. Most of the time it\u2019s correct, sometimes it\u2019s not, and you edit it. But you\u2019re still very much the architect of what you\u2019re writing. Then there\u2019s the <a href=\\\"https://en.wikipedia.org/wiki/Vibe_coding\\\">vibe coding</a>: \u201CHi, please implement this or that,\u201D enter, and then let the model do it. That\u2019s the agents.</p><p>I do feel like the agents work in very specific settings, and I would use them in specific settings. But these are all tools available to you and you have to learn what they\u2019re good at, what they\u2019re not good at, and when to use them. So the agents are pretty good, for example, if you\u2019re doing boilerplate stuff. Boilerplate code that\u2019s just copy-paste stuff, they\u2019re very good at that. They\u2019re very good at stuff that occurs very often on the Internet because there are lots of examples of it in the training sets of these models. There are features of things where the models will do very well.</p><p>I would say nanochat is not an example of those because it\u2019s a fairly unique repository. There\u2019s not that much code in the way that I\u2019ve structured it. It\u2019s not boilerplate code. It\u2019s intellectually intense code almost, and everything has to be very precisely arranged. The models have so many cognitive deficits. One example, they kept misunderstanding the code because they have too much memory from all the typical ways of doing things on the Internet that I just wasn\u2019t adopting. The models, for example\u2014I don\u2019t know if I want to get into the full details\u2014but they kept thinking I\u2019m writing normal code, and I\u2019m not.</p><p><strong>Dwarkesh Patel </strong><em>00:31:49</em></p><p>Maybe one example?</p><p><strong>Andrej Karpathy </strong><em>00:31:51</em></p><p>You have eight <a href=\\\"https://en.wikipedia.org/wiki/Graphics_processing_unit\\\">GPUs</a> that are all doing forward, backwards. The way to synchronize gradients between them is to use a <a href=\\\"https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html\\\">Distributed Data Parallel</a> container of <a href=\\\"https://en.wikipedia.org/wiki/PyTorch\\\">PyTorch</a>, which automatically as you\u2019re doing the backward, it will start communicating and synchronizing gradients. I didn\u2019t use DDP because I didn\u2019t want to use it, because it\u2019s not necessary. I threw it out and wrote my own synchronization routine that\u2019s inside the step of the optimizer. The models were trying to get me to use the DDP container. They were very concerned. This gets way too technical, but I wasn\u2019t using that container because I don\u2019t need it and I have a custom implementation of something like it.</p><p><strong>Dwarkesh Patel </strong><em>00:32:26</em></p><p>They just couldn\u2019t internalize that you had your own.</p><p><strong>Andrej Karpathy </strong><em>00:32:28</em></p><p>They couldn\u2019t get past that. They kept trying to mess up the style. They\u2019re way too over-defensive. They make all these try-catch statements. They keep trying to make a production code base, and I have a bunch of assumptions in my code, and it\u2019s okay. I don\u2019t need all this extra stuff in there. So I feel like they\u2019re bloating the code base, bloating the complexity, they keep misunderstanding, they\u2019re using deprecated <a href=\\\"https://en.wikipedia.org/wiki/API\\\">APIs</a> a bunch of times. It\u2019s a total mess. It\u2019s just not net useful. I can go in, I can clean it up, but it\u2019s not net useful.</p><p>I also feel like it\u2019s annoying to have to type out what I want in English because it\u2019s too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear and I start typing out the first few letters, autocomplete gets it and just gives you the code. This is a very high information bandwidth to specify what you want. You point to the code where you want it, you type out the first few pieces, and the model will complete it.</p><p>So what I mean is, these models are good in certain parts of the stack. There are two examples where I use the models that I think are illustrative. One was when I generated the report. That\u2019s more boilerplate-y, so I partially vibe-coded some of that stuff. That was fine because it\u2019s not mission-critical stuff, and it works fine.</p><p>The other part is when I was rewriting the tokenizer in <a href=\\\"https://en.wikipedia.org/wiki/Rust_(programming_language)\\\">Rust</a>. I\u2019m not as good at Rust because I\u2019m fairly new to Rust. So there\u2019s a bit of vibe coding going on when I was writing some of the Rust code. But I had a Python implementation that I fully understand, and I\u2019m just making sure I\u2019m making a more efficient version of it, and I have tests so I feel safer doing that stuff. They increase accessibility to languages or paradigms that you might not be as familiar with. I think they\u2019re very helpful there as well. There\u2019s a ton of Rust code out there, the models are pretty good at it. I happen to not know that much about it, so the models are very useful there.</p><p><strong>Dwarkesh Patel </strong><em>00:34:23</em></p><p>The reason this question is so interesting is because the main story people have about AI exploding and getting to <a href=\\\"https://en.wikipedia.org/wiki/Superintelligence\\\">superintelligence</a> pretty rapidly is AI automating AI engineering and AI research. They\u2019ll look at the fact that you can have <a href=\\\"https://www.claude.com/product/claude-code\\\">Claude Code</a> and make entire applications, <a href=\\\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\\\">CRUD</a> applications, from scratch and think, \u201CIf you had this same capability inside of OpenAI and DeepMind and everything, just imagine a thousand of you or a million of you in parallel, finding little architectural tweaks.\u201D</p><p>It\u2019s quite interesting to hear you say that this is the thing they\u2019re asymmetrically worse at. It\u2019s quite relevant to forecasting whether the <a href=\\\"https://ai-2027.com/\\\">AI 2027</a>-type explosion is likely to happen anytime soon.</p><p><strong>Andrej Karpathy </strong><em>00:35:05</em></p><p>That\u2019s a good way of putting it, and you\u2019re getting at why my timelines are a bit longer. You\u2019re right. They\u2019re not very good at code that has never been written before, maybe it\u2019s one way to put it, which is what we\u2019re trying to achieve when we\u2019re building these models.</p><p><strong>Dwarkesh Patel </strong><em>00:35:19</em></p><p>Very naive question, but the architectural tweaks that you\u2019re adding to nanochat, they\u2019re in a paper somewhere, right? They might even be in a repo somewhere. Is it surprising that they aren\u2019t able to integrate that into whenever you\u2019re like, \u201CAdd <a href=\\\"https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html\\\">RoPE embeddings</a>\u201D or something, they do that in the wrong way?</p><p><strong>Andrej Karpathy </strong><em>00:35:42</em></p><p>It\u2019s tough. They know, but they don\u2019t fully know. They don\u2019t know how to fully integrate it into the repo and your style and your code and your place, and some of the custom things that you\u2019re doing and how it fits with all the assumptions of the repository. They do have some knowledge, but they haven\u2019t gotten to the place where they can integrate it and make sense of it.</p><p>A lot of the stuff continues to improve. Currently, the state-of-the-art model that I go to is the <a href=\\\"https://platform.openai.com/docs/models/gpt-5-pro\\\">GPT-5 Pro</a>, and that\u2019s a very powerful model. If I have 20 minutes, I will copy-paste my entire repo and I go to GPT-5 Pro, the oracle, for some questions. Often it\u2019s not too bad and surprisingly good compared to what existed a year ago.</p><p>Overall, the models are not there. I feel like the industry is making too big of a jump and is trying to pretend like this is amazing, and it\u2019s not. It\u2019s slop. They\u2019re not coming to terms with it, and maybe they\u2019re trying to fundraise or something like that. I\u2019m not sure what\u2019s going on, but we\u2019re at this intermediate stage. The models are amazing. They still need a lot of work. For now, autocomplete is my sweet spot. But sometimes, for some types of code, I will go to an LLM agent.</p><p><strong>Dwarkesh Patel </strong><em>00:36:53</em></p><p>Here\u2019s another reason this is really interesting. Through the history of programming, there have been many productivity improvements\u2014<a href=\\\"https://en.wikipedia.org/wiki/Compiler\\\">compilers</a>, <a href=\\\"https://en.wikipedia.org/wiki/Lint_(software)\\\">linting</a>, better programming languages\u2014which have increased programmer productivity but have not led to an explosion. That sounds very much like the autocomplete tab, and this other category is just automation of the programmer. It\u2019s interesting you\u2019re seeing more in the category of the historical analogies of better compilers or something.</p><p><strong>Andrej Karpathy </strong><em>00:37:26</em></p><p>Maybe this gets to one other thought. I have a hard time differentiating where AI begins and stops because I see AI as fundamentally an extension of computing in a pretty fundamental way. I see a continuum of this recursive self-improvement or speeding up programmers all the way from the beginning: code editors, syntax highlighting, or checking even of the types, like data type checking\u2014all these tools that we\u2019ve built for each other.</p><p>Even search engines. Why aren\u2019t search engines part of AI? Ranking is AI. At some point, Google, even early on, was thinking of themselves as an AI company doing Google Search engine, which is totally fair.</p><p>I see it as a lot more of a continuum than other people do, and it\u2019s hard for me to draw the line. I feel like we\u2019re now getting a much better autocomplete, and now we\u2019re also getting some agents which are these loopy things, but they go off-rails sometimes. What\u2019s going on is that the human is progressively doing a bit less and less of the low-level stuff. We\u2019re not writing the <a href=\\\"https://en.wikipedia.org/wiki/Assembly_language\\\">assembly code</a> because we have compilers. Compilers will take my high-level language in <a href=\\\"https://en.wikipedia.org/wiki/C_(programming_language)\\\">C</a> and write the assembly code.</p><p>We\u2019re abstracting ourselves very, very slowly. There\u2019s this what I call \u201Cautonomy slider,\u201D where more and more stuff is automated\u2014of the stuff that can be automated at any point in time\u2014and we\u2019re doing a bit less and less and raising ourselves in the layer of abstraction over the automation.</p><h3>00:40:05 \u2013 RL is terrible</h3><p><strong>Dwarkesh Patel </strong><em>00:40:05</em></p><p>Let\u2019s talk about RL a bit. <a href=\\\"https://x.com/karpathy/status/1944435412489171119\\\">You tweeted some very interesting things about this</a>. Conceptually, how should we think about the way that humans are able to build a rich <a href=\\\"https://www.youtube.com/watch?v=hguIUmMsvA4\\\">world model</a> just from interacting with our environment, and in ways that seem almost irrespective of the final reward at the end of the episode?</p><p>If somebody is starting a business, and at the end of 10 years, she finds out whether the business succeeded or failed, we say that she\u2019s earned a bunch of wisdom and experience. But it\u2019s not because the log probs of every single thing that happened over the last 10 years are up-weighted or down-weighted. Something much more deliberate and rich is happening. What is the <a href=\\\"https://en.wikipedia.org/wiki/Machine_learning\\\">ML</a> analogy, and how does that compare to what we\u2019re doing with LLMs right now?</p><p><strong>Andrej Karpathy </strong><em>00:40:47</em></p><p>Maybe the way I would put it is that humans don\u2019t use reinforcement learning, as I said. I think they do something different. Reinforcement learning is a lot worse than I think the average person thinks. Reinforcement learning is terrible. It just so happens that everything that we had before it is much worse because previously we were just imitating people, so it has all these issues.</p><p>In reinforcement learning, say you\u2019re solving a math problem, because it\u2019s very simple. You\u2019re given a math problem and you\u2019re trying to find the solution. In reinforcement learning, you will try lots of things in parallel first. You\u2019re given a problem, you try hundreds of different attempts. These attempts can be complex. They can be like, \u201COh, let me try this, let me try that, this didn\u2019t work, that didn\u2019t work,\u201D etc. Then maybe you get an answer. Now you check the back of the book and you see, \u201COkay, the correct answer is this.\u201D You can see that this one, this one, and that one got the correct answer, but these other 97 of them didn\u2019t. Literally what reinforcement learning does is it goes to the ones that worked really well and every single thing you did along the way, every single token gets upweighted like, \u201CDo more of this.\u201D</p><p>The problem with that is people will say that your estimator has high variance, but it\u2019s just noisy. It\u2019s noisy. It almost assumes that every single little piece of the solution that you made that arrived at the right answer was the correct thing to do, which is not true. You may have gone down the wrong alleys until you arrived at the right solution. Every single one of those incorrect things you did, as long as you got to the correct solution, will be upweighted as, \u201CDo more of this.\u201D It\u2019s terrible. It\u2019s noise.</p><p>You\u2019ve done all this work only to find, at the end, you get a single number of like, \u201COh, you did correct.\u201D Based on that, you weigh that entire trajectory as like, upweight or downweight. The way I like to put it is you\u2019re sucking supervision through a straw. You\u2019ve done all this work that could be a minute of rollout, and you\u2019re sucking the bits of supervision of the final reward signal through a straw and you\u2019re broadcasting that across the entire trajectory and using that to upweight or downweight that trajectory. It\u2019s just stupid and crazy.</p><p>A human would never do this. Number one, a human would never do hundreds of rollouts. Number two, when a person finds a solution, they will have a pretty complicated process of review of, \u201COkay, I think these parts I did well, these parts I did not do that well. I should probably do this or that.\u201D They think through things. There\u2019s nothing in current LLMs that does this. There\u2019s no equivalent of it. But I do see papers popping out that are trying to do this because it\u2019s obvious to everyone in the field.</p><p>The first <a href=\\\"https://en.wikipedia.org/wiki/Imitation_learning\\\">imitation learning</a>, by the way, was extremely surprising and miraculous and amazing, that we can fine-tune by imitation on humans. That was incredible. Because in the beginning, all we had was base models. Base models are autocomplete. It wasn\u2019t obvious to me at the time, and I had to learn this. <a href=\\\"https://arxiv.org/abs/2203.02155\\\">The paper that blew my mind</a> was InstructGPT, because it pointed out that you can take the pretrained model, which is autocomplete, and if you just fine-tune it on text that looks like conversations, the model will very rapidly adapt to become very conversational, and it keeps all the knowledge from pre-training. This blew my mind because I didn\u2019t understand that stylistically, it can adjust so quickly and become an assistant to a user through just a few loops of fine-tuning on that kind of data. It was very miraculous to me that that worked. So incredible. That was two to three years of work.</p><p>Now came RL. And RL allows you to do a bit better than just imitation learning because you can have these <a href=\\\"https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning\\\">reward functions</a> and you can <a href=\\\"https://en.wikipedia.org/wiki/Hill_climbing\\\">hill-climb</a> on the reward functions. Some problems have just correct answers, you can hill-climb on that without getting expert trajectories to imitate. So that\u2019s amazing. The model can also discover solutions that a human might never come up with. This is incredible. Yet, it\u2019s still stupid.</p><p>We need more. I saw a paper from Google yesterday that tried to have this reflect &amp; review idea in mind. Was it the <a href=\\\"https://arxiv.org/pdf/2509.25140\\\">memory bank paper</a> or something? I don\u2019t know. I\u2019ve seen a few papers along these lines. So I expect there to be some major update to how we do algorithms for LLMs coming in that realm. I think we need three or four or five more, something like that.</p><p><strong>Dwarkesh Patel </strong><em>00:44:54</em></p><p>You\u2019re so good at coming up with evocative phrases. \u201CSucking supervision through a straw.\u201D It\u2019s so good.</p><p>You\u2019re saying the problem with outcome-based reward is that you have this huge trajectory, and then at the end, you\u2019re trying to learn every single possible thing about what you should do and what you should learn about the world from that one final bit. Given the fact that this is obvious, why hasn\u2019t <a href=\\\"https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/\\\">process-based supervision</a> as an alternative been a successful way to make models more capable? What has been preventing us from using this alternative paradigm?</p><p><strong>Andrej Karpathy </strong><em>00:45:29</em></p><p>Process-based supervision just refers to the fact that we\u2019re not going to have a reward function only at the very end. After you\u2019ve done 10 minutes of work, I\u2019m not going to tell you you did well or not well. I\u2019m going to tell you at every single step of the way how well you\u2019re doing. The reason we don\u2019t have that is it\u2019s tricky how you do that properly. You have partial solutions and you don\u2019t know how to assign credit. \u200ASo when you get the right answer, it\u2019s just an equality match to the answer. It\u2019s very simple to implement. If you\u2019re doing process supervision, how do you assign in an automatable way, a partial credit assignment? It\u2019s not obvious how you do it.</p><p>Lots of labs are trying to do it with these LLM judges. You get LLMs to try to do it. You prompt an LLM, \u201CHey, look at a partial solution of a student. How well do you think they\u2019re doing if the answer is this?\u201D and they try to tune the prompt.</p><p>The reason that this is tricky is quite subtle. It\u2019s the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with billions of <a href=\\\"https://www.ibm.com/think/topics/model-parameters\\\">parameters</a>, and they\u2019re gameable. If you\u2019re reinforcement learning with respect to them, you will find adversarial examples for your LLM judges, almost guaranteed. So you can\u2019t do this for too long. You do maybe 10 steps or 20 steps, and maybe it will work, but you can\u2019t do 100 or 1,000. I understand it\u2019s not obvious, but basically the model will find little cracks. It will find all these spurious things in the nooks and crannies of the giant model and find a way to cheat it.</p><p>One example that\u2019s prominently in my mind, this was probably public, if you\u2019re using an LLM judge for a reward, you just give it a solution from a student and ask it if the student did well or not. We were training with reinforcement learning against that reward function, and it worked really well. Then, suddenly, the reward became extremely large. It was a massive jump, and it did perfect. You\u2019re looking at it like, \u201CWow, this means the student is perfect in all these problems. It\u2019s fully solved math.\u201D</p><p>But when you look at the completions that you\u2019re getting from the model, they are complete nonsense. They start out okay, and then they change to \u201Cdhdhdhdh.\u201D It\u2019s just like, \u201COh, okay, let\u2019s take two plus three and we do this and this, and then dhdhdhdh.\u201D You\u2019re looking at it, and it\u2019s like, this is crazy. How is it getting a reward of one or 100%? You look at the LLM judge, and it turns out that \u201Cdhdhdhdh\u201D is an adversarial example for the model, and it assigns 100% probability to it.</p><p>It\u2019s just because this is an out-of-sample example to the LLM. It\u2019s never seen it during training, and you\u2019re in pure generalization land. It\u2019s never seen it during training, and in the pure generalization land, you can find these examples that break it.</p><p><strong>Dwarkesh Patel </strong><em>00:47:52</em></p><p>You\u2019re basically training the LLM to be a <a href=\\\"https://en.wikipedia.org/wiki/Prompt_injection\\\">prompt injection</a> model.</p><p><strong>Andrej Karpathy </strong><em>00:47:56</em></p><p>Not even that. Prompt injection is way too fancy. You\u2019re finding adversarial examples, as they\u2019re called. These are nonsensical solutions that are obviously wrong, but the model thinks they are amazing.</p><p><strong>Dwarkesh Patel </strong><em>00:48:07</em></p><p>To the extent you think this is the bottleneck to making RL more functional, then that will require making LLMs better judges, if you want to do this in an automated way. Is it just going to be some sort of <a href=\\\"https://en.wikipedia.org/wiki/Generative_adversarial_network\\\">GAN</a>-like approach where you have to train models to be more robust?</p><p><strong>Andrej Karpathy </strong><em>00:48:22</em></p><p>The labs are probably doing all that. The obvious thing is, \u201Cdhdhdhdh\u201D should not get 100% reward. Okay, well, take \u201Cdhdhdhdh,\u201D put it in the training set of the LLM judge, and say this is not 100%, this is 0%. You can do this, but every time you do this, you get a new LLM, and it still has adversarial examples. There\u2019s an infinity of adversarial examples.</p><p>Probably if you iterate this a few times, it\u2019ll probably be harder and harder to find adversarial examples, but I\u2019m not 100% sure because this thing has a trillion parameters or whatnot. I bet you the labs are trying. I still think we need other ideas.</p><p><strong>Dwarkesh Patel </strong><em>00:48:57</em></p><p>Interesting. Do you have some shape of what the other idea could be?</p><p><strong>Andrej Karpathy </strong><em>00:49:02</em></p><p>This idea of a review solution encompassing <a href=\\\"https://en.wikipedia.org/wiki/Synthetic_data\\\">synthetic examples</a> such that when you train on them, you get better, and meta-learn it in some way. I think there are some papers that I\u2019m starting to see pop out. I am only at a stage of reading abstracts because a lot of these papers are just ideas. Someone has to make it work on a frontier LLM lab scale in full generality because when you see these papers, they pop up, and it\u2019s just a bit noisy. They\u2019re cool ideas, but I haven\u2019t seen anyone convincingly show that this is possible. That said, the LLM labs are fairly closed, so who knows what they\u2019re doing now.</p><h3>00:49:38 \u2013 How do humans learn?</h3><p><strong>Dwarkesh Patel </strong><em>00:49:38</em></p><p>I can conceptualize how you would be able to train on synthetic examples or synthetic problems that you have made for yourself. But there seems to be another thing humans do\u2014maybe sleep is this, maybe daydreaming is this\u2014which is not necessarily to come up with fake problems, but just to reflect.</p><p>I\u2019m not sure what the ML analogy is for daydreaming or sleeping, or just reflecting. I haven\u2019t come up with a new problem. Obviously, the very basic analogy would just be fine-tuning on reflection bits, but I feel like in practice that probably wouldn\u2019t work that well. Do you have some take on what the analogy of this thing is?</p><p><strong>Andrej Karpathy </strong><em>00:50:17</em></p><p>I do think that we\u2019re missing some aspects there. As an example, let\u2019s take reading a book. Currently when LLMs are reading a book, what that means is we stretch out the sequence of text, and the model is predicting the next token, and it\u2019s getting some knowledge from that. That\u2019s not really what humans do. When you\u2019re reading a book, I don\u2019t even feel like the book is exposition I\u2019m supposed to be attending to and training on. The book is a set of prompts for me to do synthetic data generation, or for you to get to a book club and talk about it with your friends. It\u2019s by manipulating that information that you actually gain that knowledge. We have no equivalent of that with LLMs. They don\u2019t really do that. I\u2019d love to see during pre-training some stage that thinks through the material and tries to reconcile it with what it already knows, and thinks through it for some amount of time and gets that to work. There\u2019s no equivalence of any of this. This is all research.</p><p>There are some subtle\u2014very subtle that I think are very hard to understand\u2014reasons why it\u2019s not trivial. If I can just describe one: why can\u2019t we just synthetically generate and train on it? Because every synthetic example, if I just give synthetic generation of the model thinking about a book, you look at it and you\u2019re like, \u201CThis looks great. Why can\u2019t I train on it?\u201D You could try, but the model will get much worse if you continue trying. That\u2019s because all of the samples you get from models are silently collapsed. Silently\u2014it is not obvious if you look at any individual example of it\u2014they occupy a very tiny manifold of the possible space of thoughts about content. The LLMs, when they come off, they\u2019re what we call \u201C<a href=\\\"https://en.wikipedia.org/wiki/Model_collapse\\\">collapsed</a>.\u201D They have a collapsed data distribution. One easy way to see it is to go to ChatGPT and ask it, \u201CTell me a joke.\u201D It only has like three jokes. It\u2019s not giving you the whole breadth of possible jokes. It knows like three jokes. They\u2019re silently collapsed.</p><p>You\u2019re not getting the richness and the diversity and the entropy from these models as you would get from humans. Humans are a lot noisier, but at least they\u2019re not biased, in a statistical sense. They\u2019re not silently collapsed. They maintain a huge amount of entropy. So how do you get synthetic data generation to work despite the collapse and while maintaining the entropy? That\u2019s a research problem.</p><p><strong>Dwarkesh Patel </strong><em>00:52:20</em></p><p>Just to make sure I understood, the reason that the collapse is relevant to synthetic data generation is because you want to be able to come up with synthetic problems or reflections which are not already in your data distribution?</p><p><strong>Andrej Karpathy </strong><em>00:52:32</em></p><p>I guess what I\u2019m saying is, say we have a chapter of a book and I ask an LLM to think about it, it will give you something that looks very reasonable. But if I ask it 10 times, you\u2019ll notice that all of them are the same.</p><p><strong>Dwarkesh Patel </strong><em>00:52:44</em></p><p>You can\u2019t just keep scaling \u201Creflection\u201D on the same amount of prompt information and then get returns from that.</p><p><strong>Andrej Karpathy </strong><em>00:52:54</em></p><p>Any individual sample will look okay, but the distribution of it is quite terrible. It\u2019s quite terrible in such a way that if you continue training on too much of your own stuff, you actually collapse.</p><p>I think that there\u2019s possibly no fundamental solution to this. I also think humans collapse over time. These analogies are surprisingly good. Humans collapse during the course of their lives. This is why children, they haven\u2019t overfit yet. They will say stuff that will shock you because you can see where they\u2019re coming from, but it\u2019s just not the thing people say, because they\u2019re not yet collapsed. But we\u2019re collapsed. We end up revisiting the same thoughts. We end up saying more and more of the same stuff, and the learning rates go down, and the collapse continues to get worse, and then everything deteriorates.</p><p><strong>Dwarkesh Patel </strong><em>00:53:39</em></p><p>Have you seen this <a href=\\\"https://www.sciencedirect.com/science/article/pii/S2666389921000945?ref=pdf_download&amp;fr=RR-7&amp;rr=98fc403fefa4f8a7\\\">super interesting paper that dreaming is a way of preventing this</a> kind of <a href=\\\"https://aws.amazon.com/what-is/overfitting/\\\">overfitting</a> and collapse? The reason dreaming is evolutionary adaptive is to put you in weird situations that are very unlike your day-to-day reality, so as to prevent this kind of overfitting.</p><p><strong>Andrej Karpathy </strong><em>00:53:55</em></p><p>It\u2019s an interesting idea. I do think that when you\u2019re generating things in your head and then you\u2019re attending to it, you\u2019re training on your own samples, you\u2019re training on your synthetic data. If you do it for too long, you go off-rails and you collapse way too much. You always have to seek entropy in your life. Talking to other people is a great source of entropy, and things like that. So maybe the brain has also built some internal mechanisms for increasing the amount of entropy in that process. That\u2019s an interesting idea.</p><p><strong>Dwarkesh Patel </strong><em>00:54:25</em></p><p>This is a very ill-formed thought so I\u2019ll just put it out and let you react to it. The best learners that we are aware of, which are children, are extremely bad at recollecting information. In fact, at the very earliest stages of childhood, you will forget everything. You\u2019re just an amnesiac about everything that happens before a certain year date. But you\u2019re extremely good at picking up new languages and learning from the world. Maybe there\u2019s some element of being able to see the forest for the trees.</p><p>Whereas if you compare it to the opposite end of the spectrum, you have LLM pre-training, where these models will literally be able to regurgitate word-for-word what is the next thing in a Wikipedia page. But their ability to learn abstract concepts really quickly, the way a child can, is much more limited. Then adults are somewhere in between, where they don\u2019t have the flexibility of childhood learning, but they can memorize facts and information in a way that is harder for kids. I don\u2019t know if there\u2019s something interesting about that spectrum.</p><p><strong>Andrej Karpathy </strong><em>00:55:19</em></p><p>I think there\u2019s something very interesting about that, 100%. I do think that humans have a lot more of an element, compared to LLMs, of seeing the forest for the trees. We\u2019re not actually that good at memorization, which is actually a feature. Because we\u2019re not that good at memorization, we\u2019re forced to find patterns in a more general sense.</p><p>LLMs in comparison are extremely good at memorization. They will recite passages from all these training sources. You can give them completely nonsensical data. You can hash some amount of text or something like that, you get a completely random sequence. If you train on it, even just for a single iteration or two, it can suddenly regurgitate the entire thing. It will memorize it. There\u2019s no way a person can read a single sequence of random numbers and recite it to you.</p><p>That\u2019s a feature, not a bug, because it forces you to only learn the generalizable components. Whereas LLMs are distracted by all the memory that they have of the pre-training documents, and it\u2019s probably very distracting to them in a certain sense. So that\u2019s why when I talk about the <a href=\\\"https://x.com/karpathy/status/1938626382248149433\\\">cognitive core</a>, I want to remove the memory, which is what we talked about. I\u2019d love to have them have less memory so that they have to look things up, and they only maintain the algorithms for thought, and the idea of an experiment, and all this cognitive glue of acting.</p><p><strong>Dwarkesh Patel </strong><em>00:56:36</em></p><p>And this is also relevant to preventing model collapse?</p><p><strong>Andrej Karpathy </strong><em>00:56:41</em></p><p>Let me think. I\u2019m not sure. It\u2019s almost like a separate axis. The models are way too good at memorization, and somehow we should remove that. People are much worse, but it\u2019s a good thing.</p><p><strong>Dwarkesh Patel </strong><em>00:56:57</em></p><p>What is a solution to model collapse? There are very naive things you could attempt. The distribution over <a href=\\\"https://en.wikipedia.org/wiki/Logit\\\">logits</a> should be wider or something. There are many naive things you could try. What ends up being the problem with the naive approaches?</p><p><strong>Andrej Karpathy </strong><em>00:57:11</em></p><p>That\u2019s a great question. You can imagine having a regularization for entropy and things like that. I guess they just don\u2019t work as well empirically because right now the models are collapsed. But I will say most of the tasks that we want from them don\u2019t actually demand diversity. That\u2019s probably the answer to what\u2019s going on.</p><p>The frontier labs are trying to make the models useful. I feel like the diversity of the outputs is not so much... Number one, it\u2019s much harder to work with and evaluate and all this stuff, but maybe it\u2019s not what\u2019s capturing most of the value.</p><p><strong>Dwarkesh Patel </strong><em>00:57:42</em></p><p>In fact, it\u2019s actively penalized. If you\u2019re super creative in RL, it\u2019s not good.</p><p><strong>Andrej Karpathy </strong><em>00:57:48</em></p><p>Yeah. Or maybe if you\u2019re doing a lot of writing, help from LLMs and stuff like that, it\u2019s probably bad because the models will silently give you all the same stuff. They won\u2019t explore lots of different ways of answering a question.</p><p>Maybe this diversity, not as many applications need it so the models don\u2019t have it. But then it\u2019s a problem at synthetic data generation time, et cetera. So we\u2019re shooting ourselves in the foot by not allowing this entropy to maintain in the model. Possibly the labs should try harder.</p><p><strong>Dwarkesh Patel </strong><em>00:58:17</em></p><p>I think you hinted that it\u2019s a very fundamental problem, it won\u2019t be easy to solve. What\u2019s your intuition for that?</p><p><strong>Andrej Karpathy </strong><em>00:58:24</em></p><p>I don\u2019t know if it\u2019s super fundamental. I don\u2019t know if I intended to say that. I do think that I haven\u2019t done these experiments, but I do think that you could probably regularize the entropy to be higher. So you\u2019re encouraging the model to give you more and more solutions, but you don\u2019t want it to start deviating too much from the training data. It\u2019s going to start making up its own language. It\u2019s going to start using words that are extremely rare, so it\u2019s going to drift too much from the distribution.</p><p>So I think controlling the distribution is just tricky. It\u2019s probably not trivial in that sense.</p><p><strong>Dwarkesh Patel </strong><em>00:58:58</em></p><p>How many bits should the optimal core of intelligence end up being if you just had to make a guess? The thing we put on the <a href=\\\"https://en.wikipedia.org/wiki/Self-replicating_spacecraft\\\">von Neumann probes</a>, how big does it have to be?</p><p><strong>Andrej Karpathy </strong><em>00:59:10</em></p><p>It\u2019s really interesting in the history of the field because at one point everything was very scaling-pilled in terms of like, \u201COh, we\u2019re gonna make much bigger models, trillions of parameter models.\u201D What the models have done in size is they\u2019ve gone up and now they\u2019ve come down. State-of-the-art models are smaller. Even then, I think they memorized way too much. So I had a <a href=\\\"https://x.com/karpathy/status/1938626382248149433\\\">prediction</a> a while back that I almost feel like we can get cognitive cores that are very good at even a billion parameters.</p><p>If you talk to a billion parameter model, I think in 20 years, you can have a very productive conversation. It thinks and it\u2019s a lot more like a human. But if you ask it some factual question, it might have to look it up, but it knows that it doesn\u2019t know and it might have to look it up and it will just do all the reasonable things.</p><p><strong>Dwarkesh Patel </strong><em>00:59:54</em></p><p>That\u2019s surprising that you think it\u2019ll take a billion parameters. Because already we have billion parameter models or a couple billion parameter models that are very intelligent.</p><p><strong>Andrej Karpathy </strong><em>01:00:02</em></p><p>Well, state-of-the-art models are like a trillion parameters. But they remember so much stuff.</p><p><strong>Dwarkesh Patel </strong><em>01:00:06</em></p><p>Yeah, but I\u2019m surprised that in 10 years, given the pace\u2026 We have <a href=\\\"https://openai.com/index/introducing-gpt-oss/\\\">gpt-oss-20b</a>. That\u2019s way better than GPT-4 original, which was a trillion plus parameters. Given that trend, I\u2019m surprised you think in 10 years the cognitive core is still a billion parameters. I\u2019m surprised you\u2019re not like, \u201COh it\u2019s gonna be like tens of millions or millions.\u201D</p><p><strong>Andrej Karpathy </strong><em>01:00:30</em></p><p>Here\u2019s the issue, the training data is the internet, which is really terrible. There\u2019s a huge amount of gains to be made because the internet is terrible. Even the internet, when you and I think of the internet, you\u2019re thinking of like <em>The Wall Street Journal</em>. That\u2019s not what this is. When you\u2019re looking at a pre-training dataset in the frontier lab and you look at a random internet document, it\u2019s total garbage. I don\u2019t even know how this works at all. It\u2019s some like stock tickers, symbols, it\u2019s a huge amount of slop and garbage from like all the corners of the internet. It\u2019s not like your <em>Wall Street Journal</em> article, that\u2019s extremely rare. So because the internet is so terrible, we have to build really big models to compress all that. Most of that compression is memory work instead of cognitive work.</p><p>But what we really want is the cognitive part, delete the memory. I guess what I\u2019m saying is that we need intelligent models to help us refine even the pre-training set to just narrow it down to the cognitive components. Then I think you get away with a much smaller model because it\u2019s a much better dataset and you could train it on it. But probably it\u2019s not trained directly on it, it\u2019s probably distilled from a much better model still.</p><p><strong>Dwarkesh Patel </strong><em>01:01:35</em></p><p>But why is the distilled version still a billion?</p><p><strong>Andrej Karpathy </strong><em>01:01:39</em></p><p>I just feel like distillation works extremely well. So almost every small model, if you have a small model, it\u2019s almost certainly distilled.</p><p><strong>Dwarkesh Patel </strong><em>01:01:46</em></p><p>Right, but why is the distillation in 10 years not getting below 1 billion?</p><p><strong>Andrej Karpathy </strong><em>01:01:50</em></p><p>Oh, you think it should be smaller than a billion? I mean, come on, right? I don\u2019t know. At some point it should take at least a billion knobs to do something interesting. You\u2019re thinking it should be even smaller?</p><p><strong>Dwarkesh Patel </strong><em>01:02:01</em></p><p>Yeah. If you look at the trend over the last few years of just finding low-hanging fruit and going from trillion plus models to models that are literally two orders of magnitude smaller in a matter of two years and having better performance, it makes me think the sort of core of intelligence might be even way, way smaller. <a href=\\\"https://en.wikipedia.org/wiki/There%27s_Plenty_of_Room_at_the_Bottom\\\">Plenty of room at the bottom, to paraphrase Feynman.</a></p><p><strong>Andrej Karpathy </strong><em>01:02:22</em></p><p>I feel like I\u2019m already contrarian by talking about a billion parameter cognitive core and you\u2019re outdoing me. Maybe we could get a little bit smaller. I do think that practically speaking, you want the model to have some knowledge. You don\u2019t want it to be looking up everything because then you can\u2019t think in your head. You\u2019re looking up way too much stuff all the time. Some basic curriculum needs to be there for knowledge, but it doesn\u2019t have esoteric knowledge.</p><p><strong>Dwarkesh Patel </strong><em>01:02:48</em></p><p>We\u2019re discussing what plausibly could be the cognitive core. There\u2019s a separate question which is what will be the size of frontier models over time? I\u2019m curious if you have predictions. We had increasing scale up to maybe <a href=\\\"https://openai.com/index/introducing-gpt-4-5/\\\">GPT 4.5</a> and now we\u2019re seeing decreasing or plateauing scale. There are many reasons this could be going on. Do you have a prediction going forward? Will the biggest models be bigger, will they be smaller, will they be the same?</p><p><strong>Andrej Karpathy </strong><em>01:03:14</em></p><p>I don\u2019t have a super strong prediction. The labs are just being practical. They have a <a href=\\\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\\\">flops</a> budget and a cost budget. It just turns out that pre-training is not where you want to put most of your flops or your cost. That\u2019s why the models have gotten smaller. They are a bit smaller, the pre-training stage is smaller, but they make it up in reinforcement learning, mid-training, and all this stuff that follows. They\u2019re just being practical in terms of all the stages and how you get the most bang for the buck.</p><p>Forecasting that trend is quite hard. I do still expect that there\u2019s so much low-hanging fruit. That\u2019s my basic expectation. I have a very wide distribution here.</p><p><strong>Dwarkesh Patel </strong><em>01:03:51</em></p><p>Do you expect the low-hanging fruit to be similar in kind to the kinds of things that have been happening over the last two to five years? If I look at nanochat versus <a href=\\\"https://github.com/karpathy/nanoGPT\\\">nanoGPT</a> and the architectural tweaks you made, is that the flavor of things you expect to continue to keep happening? You\u2019re not expecting any giant paradigm shifts.</p><p><strong>Andrej Karpathy </strong><em>01:04:11</em></p><p>For the most part, yeah. I expect the datasets to get much, much better. When you look at the average datasets, they\u2019re extremely terrible. They\u2019re so bad that I don\u2019t even know how anything works. Look at the average example in the training set: factual mistakes, errors, nonsensical things. Somehow when you do it at scale, the noise washes away and you\u2019re left with some of the signal. Datasets will improve a ton.</p><p>Everything gets better. Our hardware, all the kernels for running the hardware and maximizing what you get with the hardware. <a href=\\\"https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell\\\">Nvidia is slowly tuning the hardware itself</a>, <a href=\\\"https://www.nvidia.com/en-us/data-center/tensor-cores/\\\">Tensor Cores</a>, all that needs to happen and will continue to happen. All the kernels will get better and utilize the chip to the max extent. All the algorithms will probably improve over optimization, architecture, and all the modeling components of how everything is done and what the algorithms are that we\u2019re even training with. I do expect that nothing dominates. Everything plus 20%. This is roughly what I\u2019ve seen.</p><h3>01:06:25 \u2013 AGI will blend into 2% GDP growth</h3><p><strong>Dwarkesh Patel </strong><em>01:06:25</em></p><p>People have proposed different ways of charting how much progress we\u2019ve made towards full AGI. If you can come up with some line, then you can see where that line intersects with AGI and where that would happen on the x-axis. People have proposed it\u2019s the education level. We had a high schooler, and then they went to college with RL, and they\u2019re going to get a Ph.D.</p><p><strong>Andrej Karpathy </strong><em>01:06:44</em></p><p>I don\u2019t like that one.</p><p><strong>Dwarkesh Patel </strong><em>01:06:45</em></p><p>Or they\u2019ll propose <a href=\\\"https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks\\\">horizon length</a>. Maybe they can do tasks that take a minute, they can do those autonomously. Then they can autonomously do tasks that take an hour, a human an hour, a human a week. How do you think about the relevant y-axis here? How should we think about how AI is making progress?</p><p><strong>Andrej Karpathy </strong><em>01:07:05</em></p><p>I have two answers to that. Number one, I\u2019m almost tempted to reject the question entirely because I see this as an extension of computing. Have we talked about how to chart progress in computing, or how do you chart progress in computing since the 1970s or whatever? What is the y-axis? The whole question is funny from that perspective a little bit.</p><p>When people talk about AI and the original AGI and how we spoke about it when OpenAI started, AGI was a system you could go to that can do any economically valuable task at human performance or better. That was the definition. I was pretty happy with that at the time. I\u2019ve stuck to that definition forever, and then people have made up all kinds of other definitions. But I like that definition.</p><p>The first concession that people make all the time is they just take out all the physical stuff because we\u2019re just talking about digital knowledge work. That\u2019s a pretty major concession compared to the original definition, which was any task a human can do. I can lift things, etc. AI can\u2019t do that, obviously, but we\u2019ll take it. What fraction of the economy are we taking away by saying, \u201COh, only knowledge work?\u201D I don\u2019t know the numbers. I feel about 10% to 20%, if I had to guess, is only knowledge work, someone could work from home and perform tasks, something like that. It\u2019s still a really large market. What is the size of the economy, and what is 10% or 20%? We\u2019re still talking about a few trillion dollars, even in the US, of market share or work. So it\u2019s still a very massive bucket.</p><p>Going back to the definition, what I would be looking for is to what extent is that definition true? Are there jobs or lots of tasks? If we think of tasks as not jobs but tasks. It\u2019s difficult because the problem is society will refactor based on the tasks that make up jobs, based on what\u2019s automatable or not. Today, what jobs are replaceable by AI? A good example recently was Geoff Hinton\u2019s prediction that radiologists would not be a job anymore, and <a href=\\\"https://www.nytimes.com/2025/05/14/technology/ai-jobs-radiologists-mayo-clinic.html\\\">this turned out to be very wrong in a bunch of ways</a>. Radiologists are alive and well and growing, even though <a href=\\\"https://en.wikipedia.org/wiki/Computer_vision\\\">computer vision</a> is really, really good at recognizing all the different things that they have to recognize in images. It\u2019s just a messy, complicated job with a lot of surfaces and dealing with patients and all this stuff in the context of it.</p><p>I don\u2019t know that by that definition AI has made a huge dent yet. Some of the jobs that I would be looking for have some features that make it very amenable to automation earlier than later. As an example, call center employees often come up, and I think rightly so. Call center employees have a number of simplifying properties with respect to what\u2019s automatable today. Their jobs are pretty simple. It\u2019s a sequence of tasks, and every task looks similar. You take a phone call with a person, it\u2019s 10 minutes of interaction or whatever it is, probably a bit longer. In my experience, a lot longer. You complete some task in some scheme, and you change some database entries around or something like that. So you keep repeating something over and over again, and that\u2019s your job.</p><p>You do want to bring in the task horizon\u2014how long it takes to perform a task\u2014and then you want to also remove context. You\u2019re not dealing with different parts of services of companies or other customers. It\u2019s just the database, you, and a person you\u2019re serving. It\u2019s more closed, it\u2019s more understandable, it\u2019s purely digital. So I would be looking for those things.</p><p>But even there, I\u2019m not looking at full automation yet. I\u2019m looking for an autonomy slider. I expect that we are not going to instantly replace people. We\u2019re going to be swapping in AIs that do 80% of the volume. They delegate 20% of the volume to humans, and humans are supervising teams of five AIs doing the call center work that\u2019s more rote. I would be looking for new interfaces or new companies that provide some layer that allows you to manage some of these AIs that are not yet perfect. Then I would expect that across the economy. A lot of jobs are a lot harder than a call center employee.</p><p><strong>Dwarkesh Patel </strong><em>01:11:02</em></p><p>With radiologists, I\u2019m totally speculating and I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when <a href=\\\"https://en.wikipedia.org/wiki/Waymo\\\">Waymos</a> were first being rolled out, there\u2019d be a person sitting in the front seat, and you just had to have them there to make sure that if something went really wrong, they\u2019re there to monitor. Even today, people are still watching to make sure things are going well. <a href=\\\"https://www.tesla.com/robotaxi\\\">Robotaxi</a>, which was just deployed, still has a person inside it.</p><p>Now we could be in a similar situation where if you automate 99% of a job, that last 1% the human has to do is incredibly valuable because it\u2019s bottlenecking everything else. If it were the case with radiologists, where the person sitting in the front of Waymo has to be specially trained for years in order to provide the last 1%, their wages should go up tremendously because they\u2019re the one thing bottlenecking wide deployment. Radiologists, I think their wages have gone up for similar reasons, if you\u2019re the last bottleneck and you\u2019re not fungible. A Waymo driver might be fungible with others. So you might see this thing where your wages go up until you get to 99% and then fall just like that when the last 1% is gone. And I wonder if we\u2019re seeing similar things with radiology or salaries of call center workers or anything like that.</p><p><strong>Andrej Karpathy </strong><em>01:12:17</em></p><p>That\u2019s an interesting question. I don\u2019t think we\u2019re currently seeing that with radiology. I think radiology is not a good example. I don\u2019t know why Geoff Hinton picked on radiology because I think it\u2019s an extremely messy, complicated profession.</p><p>I would be a lot more interested in what\u2019s happening with call center employees today, for example, because I would expect a lot of the rote stuff to be automatable today. I don\u2019t have first-level access to it but I would be looking for trends of what\u2019s happening with the call center employees. Some of the things I would also expect is that maybe they are swapping in AI, but then I would still wait for a year or two because I would potentially expect them to pull back and rehire some of the people.</p><p><strong>Dwarkesh Patel </strong><em>01:13:00</em></p><p>There\u2019s been evidence that that\u2019s already been happening generally in companies that have been adopting AI, which I think is quite surprising.</p><p>I also found what was really surprising. AGI, right? A thing which would do everything. We\u2019ll take out physical work, but it should be able to do all knowledge work. What you would have naively anticipated is that the way this progression would happen is that you take a little task that a consultant is doing, you take that out of the bucket. You take a little task that an accountant is doing, you take that out of the bucket. Then you\u2019re just doing this across all knowledge work.</p><p>But instead, if we do believe we\u2019re on the path of AGI with the current paradigm, the progression is very much not like that. It does not seem like consultants and accountants are getting huge productivity improvements. It\u2019s very much like programmers are getting more and more chiseled away at their work. If you look at the revenues of these companies, discounting normal chat revenue\u2014which is similar to Google or something\u2014just looking at API revenues, it\u2019s dominated by coding. So this thing which is \u201Cgeneral\u201D, which should be able to do any knowledge work, is just overwhelmingly doing only coding. It\u2019s a surprising way that you would expect the AGI to be deployed.</p><p><strong>Andrej Karpathy </strong><em>01:14:13</em></p><p>There\u2019s an interesting point here. I do believe coding is the perfect first thing for these LLMs and agents. That\u2019s because coding has always fundamentally worked around text. It\u2019s computer terminals and text, and everything is based around text. LLMs, the way they\u2019re trained on the Internet, love text. They\u2019re perfect text processors, and there\u2019s all this data out there. It\u2019s a perfect fit.</p><p>We also have a lot of infrastructure pre-built for handling code and text. For example, we have <a href=\\\"https://code.visualstudio.com/\\\">Visual Studio Code</a> or your favorite <a href=\\\"https://en.wikipedia.org/wiki/Integrated_development_environment\\\">IDE</a> showing you code, and an agent can plug into that. If an agent has a <a href=\\\"https://en.wikipedia.org/wiki/Diff\\\">diff</a> where it made some change, we suddenly have all this code already that shows all the differences to a code base using a diff. It\u2019s almost like we\u2019ve pre-built a lot of the infrastructure for code.</p><p>Contrast that with some of the things that don\u2019t enjoy that at all. As an example, there are people trying to build automation not for coding, but for slides. I saw a company doing slides. That\u2019s much, much harder. The reason it\u2019s much harder is because slides are not text. Slides are little graphics, they\u2019re arranged spatially, and there\u2019s a visual component to it. Slides don\u2019t have this pre-built infrastructure. For example, if an agent is to make a change to your slides, how does a thing show you the diff? How do you see the diff? There\u2019s nothing that shows diffs for slides. Someone has to build it. Some of these things are not amenable to AIs as they are, which are text processors, and code surprisingly is.</p><p><strong>Dwarkesh Patel </strong><em>01:15:48</em></p><p>I\u2019m not sure that alone explains it. I personally have tried to get LLMs to be useful in domains which are just pure language-in, language-out, like rewriting transcripts, coming up with clips based on transcripts. It\u2019s very plausible that I didn\u2019t do every single possible thing I could do. I put a bunch of good examples in context, but maybe I should have done some kind of fine-tuning.</p><p>Our mutual friend, <a href=\\\"https://www.dwarkesh.com/p/andy-matuschak\\\">Andy Matuschak</a>, told me that he tried 50 billion things to try to get models to be good at writing spaced repetition prompts. Again, very much language-in, language-out tasks, the kind of thing that should be dead center in the repertoire of these LLMs. He tried in-context learning with a <a href=\\\"https://www.ibm.com/think/topics/few-shot-learning\\\">few-shot</a> examples. He tried supervised fine-tuning and retrieval. He could not get them to make cards to his satisfaction.</p><p>So I find it striking that even in language-out domains, it\u2019s very hard to get a lot of economic value out of these models separate from coding. I don\u2019t know what explains it.</p><p><strong>Andrej Karpathy </strong><em>01:16:57</em></p><p>That makes sense. I\u2019m not saying that anything text is trivial. I do think that code is pretty structured. Text is maybe a lot more flowery, and there\u2019s a lot more entropy in text, I would say. I don\u2019t know how else to put it. Also code is hard, and so people feel quite empowered by LLMs, even from simple knowledge. I don\u2019t know that I have a very good answer. Obviously, text makes it much, much easier, but it doesn\u2019t mean that all text is trivial.</p><h3>01:17:36 \u2013 ASI</h3><p><strong>Dwarkesh Patel </strong><em>01:17:36</em></p><p>How do you think about <a href=\\\"https://en.wikipedia.org/wiki/Superintelligence\\\">superintelligence</a>? Do you expect it to feel qualitatively different from normal humans or human companies?</p><p><strong>Andrej Karpathy </strong><em>01:17:45</em></p><p>I see it as a progression of automation in society.<strong> </strong>Extrapolating the trend of computing, there will be a gradual automation of a lot of things, and superintelligence will an extrapolation of that. We expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work some amount of time later. Basically I see it as just automation, roughly speaking.</p><p><strong>Dwarkesh Patel </strong><em>01:18:10</em></p><p>But automation includes the things humans can already do, and superintelligence implies things humans can\u2019t do.</p><p><strong>Andrej Karpathy </strong><em>01:18:16</em></p><p>But one of the things that people do is invent new things, which I would just put into the automation if that makes sense.</p><p><strong>Dwarkesh Patel </strong><em>01:18:20</em></p><p>But I guess, less abstractly and more qualitatively, do you expect something to feel like\u2026 Because this thing can either think so fast, or has so many copies, or the copies can merge back into themselves, or is much smarter, any number of advantages an AI might have, will the civilization in which these AIs exist just feel qualitatively different from humans?</p><p><strong>Andrej Karpathy </strong><em>01:18:51</em></p><p>I think it will. It is fundamentally automation, but it will be extremely foreign. It will look really strange. Like you mentioned, we can run all of this on a computer cluster and much faster.</p><p>Some of the scenarios that I start to get nervous about when the world looks like that is this gradual loss of control and understanding of what\u2019s happening. I think that\u2019s the most likely outcome, that there will be a gradual loss of understanding. We\u2019ll gradually layer all this stuff everywhere, and there will be fewer and fewer people who understand it. Then there will be a gradual loss of control and understanding of what\u2019s happening. That to me seems the most likely outcome of how all this stuff will go down.</p><p><strong>Dwarkesh Patel </strong><em>01:19:31</em></p><p>Let me probe on that a bit. It\u2019s not clear to me that loss of control and loss of understanding are the same things. A board of directors at TSMC, Intel\u2014name a random company\u2014they\u2019re just prestigious 80-year-olds. They have very little understanding, and maybe they don\u2019t practically actually have control.</p><p>A better example is the President of the United States. The President has a lot of fucking power. I\u2019m not trying to make a good statement about the current operant, or maybe I am, but the actual level of understanding is very different from the level of control.</p><p><strong>Andrej Karpathy </strong><em>01:20:06</em></p><p>I think that\u2019s fair. That\u2019s a good pushback. I think I expect loss of both.</p><p><strong>Dwarkesh Patel </strong><em>01:20:15</em></p><p>How come? Loss of understanding is obvious, but why loss of control?</p><p><strong>Andrej Karpathy </strong><em>01:20:20</em></p><p>We\u2019re really far into a territory where I don\u2019t know what this looks like, but if I were to write sci-fi novels, they would look along the lines of not even a single entity that takes over everything, but multiple competing entities that gradually become more and more autonomous. Some of them go rogue and the others fight them off. It\u2019s this hot pot of completely autonomous activity that we\u2019ve delegated to. I feel it would have that flavor.</p><p><strong>Dwarkesh Patel </strong><em>01:20:52</em></p><p>It is not the fact that they are smarter than us that is resulting in the loss of control. It\u2019s the fact that they are competing with each other, and whatever arises out of that competition leads to the loss of control.</p><p><strong>Andrej Karpathy </strong><em>01:21:06</em></p><p>A lot of these things, they will be tools to people, they\u2019re acting on behalf of people or something like that. So maybe those people are in control, but maybe it\u2019s a loss of control overall for society in the sense of outcomes we want. You have entities acting on behalf of individuals that are still roughly seen as out of control.</p><p><strong>Dwarkesh Patel </strong><em>01:21:30</em></p><p>This is a question I should have asked earlier. We were talking about how currently it feels like when you\u2019re doing AI engineering or AI research, these models are more in the category of compiler rather than in the category of a replacement.</p><p>At some point, if you have AGI, it should be able to do what you do. Do you feel like having a million copies of you in parallel results in some huge speed-up of AI progress? If that does happen, do you expect to see an intelligence explosion once we have a true AGI? I\u2019m not talking about LLMs today.</p><p><strong>Andrej Karpathy </strong><em>01:22:01</em></p><p>I do, but it\u2019s business as usual because we\u2019re in an intelligence explosion already and have been for decades. It\u2019s basically the GDP curve that is an exponential weighted sum over so many aspects of the industry. Everything is gradually being automated and has been for hundreds of years. The <a href=\\\"https://en.wikipedia.org/wiki/Industrial_Revolution\\\">Industrial Revolution</a> is automation and some of the physical components and tool building and all this stuff. Compilers are early software automation, et cetera. We\u2019ve been recursively self-improving and exploding for a long time.</p><p>Another way to see it is that Earth was a pretty boring place if you don\u2019t look at the biomechanics and so on, and looked very similar. If you look from space, we\u2019re in the middle of this firecracker event, but we\u2019re seeing it in slow motion. I definitely feel like this has already happened for a very long time. Again, I don\u2019t see AI as a distinct technology with respect to what has already been happening for a long time.</p><p><strong>Dwarkesh Patel </strong><em>01:23:00</em></p><p>You think it\u2019s continuous with this hyper-exponential trend?</p><p><strong>Andrej Karpathy </strong><em>01:23:03</em></p><p>Yes. That\u2019s why this was very interesting to me, because I was trying to find AI in the GDP for a while. I thought that GDP should go up. But then I looked at some of the other technologies that I thought were very transformative, like computers or mobile phones or et cetera. You can\u2019t find them in GDP. GDP is the same exponential.</p><p>Even the early iPhone didn\u2019t have the App Store, and it didn\u2019t have a lot of the bells and whistles that the modern iPhone has. So even though we think of 2008, when the iPhone came out, as this major seismic change, it\u2019s actually not. Everything is so spread out and it so slowly diffuses that everything ends up being averaged up into the same exponential. It\u2019s the exact same thing with computers. You can\u2019t find them in the GDP like, \u201COh, we have computers now.\u201D That\u2019s not what happened, because it\u2019s such slow progression.</p><p>With AI we\u2019re going to see the exact same thing. It\u2019s just more automation. It allows us to write different kinds of programs that we couldn\u2019t write before, but AI is still fundamentally a program. It\u2019s a new kind of computer and a new kind of computing system. But it has all these problems, it\u2019s going to diffuse over time, and it\u2019s still going to add up to the same exponential. We\u2019re still going to have an exponential that\u2019s going to get extremely vertical. It\u2019s going to be very foreign to live in that kind of an environment.</p><p><strong>Dwarkesh Patel </strong><em>01:24:10</em></p><p>Are you saying that, if you look at the trend before the Industrial Revolution to now, you have a hyper-exponential where you go from 0% growth to then 10,000 years ago, 0.02% growth, and to now when we\u2019re at 2% growth. That\u2019s a hyper-exponential. Are you saying if you\u2019re charting AI on there, then AI takes you to 20% growth or 200% growth?</p><p>Or are you saying that if you look at the last 300 years, what you\u2019ve been seeing is that you have technology after technology\u2014computers, electrification, steam engines, railways, et cetera\u2014but the rate of growth is the exact same, it\u2019s 2%. Are you saying the rate of growth will go up?</p><p><strong>Andrej Karpathy </strong><em>01:24:46</em></p><p>The rate of growth has also stayed roughly constant, right?</p><p><strong>Dwarkesh Patel </strong><em>01:24:49</em></p><p>Only over the last 200, 300 years. But over the course of human history it\u2019s exploded. It\u2019s gone from 0% to faster, faster, faster. Industrial explosion, 2%.</p><p><strong>Andrej Karpathy </strong><em>01:25:01</em></p><p>For a while I tried to find AI or look for AI in the GDP curve, and I\u2019ve convinced myself that this is false. Even when people talk about recursive self-improvement and labs and stuff like that, this is business as usual. Of course it\u2019s going to recursively self-improve, and it\u2019s been recursively self-improving.</p><p>LLMs allow the engineers to work much more efficiently to build the next round of LLM, and a lot more of the components are being automated and tuned and et cetera. All the engineers having access to Google Search is part of it. All the engineers having an IDE, all of them having autocomplete or having Claude code, et cetera, it\u2019s all just part of the same speed-up of the whole thing. It\u2019s just so smooth.</p><p><strong>Dwarkesh Patel </strong><em>01:25:41</em></p><p>Just to clarify, you\u2019re saying that the rate of growth will not change. The intelligence explosion will show up as it just enabled us to continue staying on the 2% growth trajectory, just as the Internet helped us stay on the 2% growth trajectory.</p><p><strong>Andrej Karpathy </strong><em>01:25:53</em></p><p>Yes, my expectation is that it stays in the same pattern.</p><p><strong>Dwarkesh Patel </strong><em>01:25:58</em></p><p>Just to throw the opposite argument against you, my expectation is that it blows up because I think true AGI\u2014and I\u2019m not talking about LLM coding bots, I\u2019m talking about actual replacement of a human in a server\u2014is qualitatively different from these other productivity-improving technologies because it\u2019s labor itself.</p><p>I think we live in a very labor-constrained world. If you talk to any startup founder or any person, you can be like, what do you need more of? You need really talented people. And if you have billions of extra people who are inventing stuff, integrating themselves, making companies bottom start to finish, that feels qualitatively different from a single technology. It\u2019s as if you get 10 billion extra people on the planet.</p><p><strong>Andrej Karpathy </strong><em>01:26:44</em></p><p>Maybe a counterpoint. I\u2019m pretty willing to be convinced one way or another on this point. But I will say, for example, computing is labor. Computing was labor. Computers, a lot of jobs disappeared because computers are automating a bunch of digital information processing that you now don\u2019t need a human for. So computers are labor, and that has played out.</p><p>Self-driving as an example is also computers doing labor. That\u2019s already been playing out. It\u2019s still business as usual.</p><p><strong>Dwarkesh Patel </strong><em>01:27:13</em></p><p>You have a machine which is spitting out more things like that at potentially faster pace. Historically, we have examples of the growth regime changing where you went from 0.2% growth to 2% growth. It seems very plausible to me that a machine which is then spitting out the next self-driving car and the next Internet and whatever\u2026</p><p><strong>Andrej Karpathy </strong><em>01:27:33</em></p><p>I see where it\u2019s coming from. At the same time, I do feel like people make this assumption of, \u201CWe have God in a box, and now it can do everything,\u201D and it just won\u2019t look like that. It\u2019s going to be able to do some of the things. It\u2019s going to fail at some other things. It\u2019s going to be gradually put into society, and we\u2019ll end up with the same pattern. That is my prediction.</p><p>This assumption of suddenly having a completely intelligent, fully flexible, fully general human in a box, and we can dispense it at arbitrary problems in society, I don\u2019t think that we will have this discrete change. I think we\u2019ll arrive at the same kind of gradual diffusion of this across the industry.</p><p><strong>Dwarkesh Patel </strong><em>01:28:14</em></p><p>It often ends up being misleading in these conversations. I don\u2019t like to use the word intelligence in this context because intelligence implies you think there\u2019ll be a single superintelligence sitting in a server and it\u2019ll divine how to come up with new technologies and inventions that cause this explosion. That\u2019s not what I\u2019m imagining when I\u2019m imagining 20% growth. I\u2019m imagining that there are billions of very smart human-like minds, potentially, or that\u2019s all that\u2019s required.</p><p>But the fact that there\u2019s hundreds of millions of them, billions of them, each individually making new products, figuring out how to integrate themselves into the economy. If a highly experienced smart immigrant came to the country, you wouldn\u2019t need to figure out how we integrate them in the economy. They figure it out. They could start a company, they could make inventions, or increase productivity in the world.</p><p>We have examples, even in the current regime, of places that have had 10-20% economic growth. If you just have a lot of people and less capital in comparison to the people, you can have Hong Kong or Shenzhen or whatever with decades of 10% plus growth. There\u2019s a lot of really smart people who are ready to make use of the resources and do this period of catch-up because we\u2019ve had this discontinuity, and I think AI might be similar.</p><p><strong>Andrej Karpathy </strong><em>01:29:33</em></p><p>I understand, but I still think that you\u2019re presupposing some discrete jump. There\u2019s some unlock that we\u2019re waiting to claim. And suddenly we\u2019re going to have geniuses in data centers. I still think you\u2019re presupposing some discrete jump that has no historical precedent that I can\u2019t find in any of the statistics and that I think probably won\u2019t happen.</p><p><strong>Dwarkesh Patel </strong><em>01:29:52</em></p><p>I mean, the Industrial Revolution is such a jump. You went from 0.2% growth to 2% growth. I\u2019m just saying you\u2019ll see another jump like that.</p><p><strong>Andrej Karpathy </strong><em>01:30:00</em></p><p>I\u2019m a little bit suspicious, I would have to take a look. For example, some of the logs are not very good from before the Industrial Revolution. I\u2019m a bit suspicious of it but I don\u2019t have strong opinions. You\u2019re saying that this was a singular event that was extremely magical. You\u2019re saying that maybe there\u2019s going to be another event that\u2019s going to be just like that, extremely magical. It will break the paradigm, and so on.</p><p><strong>Dwarkesh Patel </strong><em>01:30:23</em></p><p>I actually don\u2019t think\u2026 The crucial thing with the Industrial Revolution was that it was not magical. If you just zoomed in, what you would see in 1770 or 1870 is not that there was some key invention. But at the same time, you did move the economy to a regime where the progress was much faster and the exponential 10x\u2019d. I expect a similar thing from AI where it\u2019s not like there\u2019s going to be a single moment where we\u2019ve made the crucial invention.</p><p><strong>Andrej Karpathy </strong><em>01:30:51</em></p><p>It\u2019s an overhang that\u2019s being unlocked. Like maybe there\u2019s a new energy source. There\u2019s some unlock\u2014in this case, some kind of a cognitive capacity\u2014and there\u2019s an overhang of cognitive work to do.</p><p><strong>Dwarkesh Patel </strong><em>01:31:02</em></p><p>That\u2019s right.</p><p><strong>Andrej Karpathy </strong><em>01:31:03</em></p><p>You\u2019re expecting that overhang to be filled by this new technology when it crosses the threshold.</p><p><strong>Dwarkesh Patel </strong><em>01:31:06</em></p><p>Maybe one way to think about it is throughout history, a lot of growth comes because people come up with ideas, and then people are out there doing stuff to execute those ideas and make valuable output. Through most of this time, the population has been exploding. That has been driving growth.</p><p>For the last 50 years, people have argued that growth has stagnated. The population in frontier countries has also stagnated. I think we go back to the exponential growth in population that causes hyper-exponential growth in output.</p><p><strong>Andrej Karpathy </strong><em>01:31:37</em></p><p>It\u2019s really hard to tell. I understand that viewpoint. I don\u2019t intuitively feel that viewpoint.</p><h3>01:32:50 \u2013 Evolution of intelligence &amp; culture</h3><p><strong>Dwarkesh Patel </strong><em>01:32:50</em></p><p>You recommended <a href=\\\"https://www.dwarkesh.com/p/nick-lane\\\">Nick Lane\u2019s</a> <a href=\\\"https://amzn.to/42KeI4w\\\">book</a> to me. On that basis, I also found it super interesting and I interviewed him. I have some questions about thinking about intelligence and evolutionary history.</p><p>Now that you, over the last 20 years of doing AI research, you maybe have a more tangible sense of what intelligence is, what it takes to develop it. Are you more or less surprised as a result that evolution just spontaneously stumbled upon it?</p><p><strong>Andrej Karpathy </strong><em>01:33:19</em></p><p>I love Nick Lane\u2019s books. I was just listening to his podcast on the way up here. With respect to intelligence and its evolution, it\u2019s very, very recent. I am surprised that it evolved.</p><p>I find it fascinating to think about all the worlds out there. Say there\u2019s a thousand planets like Earth and what they look like. I think Nick Lane was here talking about some of the earliest parts. He expects very similar life forms, roughly speaking, and bacteria-like things in most of them. There are a few breaks in there. The evolution of intelligence intuitively feels to me like it should be a fairly rare event.</p><p>Maybe you should base it on how long something has existed. If bacteria were around for 2 billion years and nothing happened, then going to eukaryote is probably pretty hard because bacteria came up quite early in Earth\u2019s evolution or history. How long have we had animals? Maybe a couple hundred million years, multicellular animals that run around, crawl, et cetera. That\u2019s maybe 10% of Earth\u2019s lifespan. Maybe on that timescale it\u2019s not too tricky. It\u2019s still surprising to me, intuitively, that it developed. \u200AI would maybe expect just a lot of animal-like life forms doing animal-like things. The fact that you can get something that creates culture and knowledge and accumulates it is surprising to me.</p><p><strong>Dwarkesh Patel </strong><em>01:34:42</em></p><p>There\u2019s a couple of interesting follow-ups. If you buy the Sutton perspective that the crux of intelligence is animal intelligence\u2026 The quote he said is \u201CIf you got to the squirrel, you\u2019d be most of the way to AGI.\u201D</p><p>We got to squirrel intelligence right after the <a href=\\\"https://en.wikipedia.org/wiki/Cambrian_explosion\\\">Cambrian explosion</a> 600 million years ago. It seems like what instigated that was the <a href=\\\"https://en.wikipedia.org/wiki/Great_Oxidation_Event\\\">oxygenation event</a> 600 million years ago. But immediately the intelligence algorithm was there to make the squirrel intelligence. It\u2019s suggestive that animal intelligence was like <em>that</em>. As soon as you had the oxygen in the environment, you had the eukaryote, you could just get the algorithm. Maybe it was an accident that evolution stumbled upon it so fast, but I don\u2019t know if that suggests that at the end it\u2019s going to be quite simple.</p><p><strong>Andrej Karpathy </strong><em>01:35:31</em></p><p>It\u2019s so hard to tell with any of this stuff. You can base it a bit on how long something has existed or how long it feels like something has been bottlenecked. Nick Lane is very good about describing this very apparent bottleneck in bacteria and archaea. For two billion years, nothing happened. There\u2019s extreme diversity of biochemistry, and yet nothing grows to become animals. Two billion years.</p><p>I don\u2019t know that we\u2019ve seen exactly that kind of an equivalent with animals and intelligence, to your point. We could also look at it with respect to how many times we think certain intelligence has individually sprung up.</p><p><strong>Dwarkesh Patel </strong><em>01:36:07</em></p><p>That\u2019s a really good thing to investigate.</p><p><strong>Andrej Karpathy </strong><em>01:36:09</em></p><p>One thought on that. There\u2019s hominid intelligence, and then there\u2019s bird intelligence. Ravens, etc., are extremely clever, but their brain parts are quite distinct, and we don\u2019t have that much in common. That\u2019s a slight indication of maybe intelligence springing up a few times. In that case, you\u2019d expect it more frequently.</p><p><strong>Dwarkesh Patel </strong><em>01:36:32</em></p><p>A former guest, <a href=\\\"https://www.dwarkesh.com/p/gwern-branwen\\\">Gwern</a>, and <a href=\\\"https://www.dwarkesh.com/p/carl-shulman\\\">Carl Shulman</a>, they\u2019ve made a really interesting point about that. Their perspective is that the scalable algorithm which humans have and primates have, arose in birds as well, and maybe other times as well. But humans found an evolutionary niche which rewarded marginal increases in intelligence and also had a scalable brain algorithm that could achieve those increases in intelligence.</p><p>For example, if a bird had a bigger brain, it would just collapse out of the air. It\u2019s very smart for the size of its brain, but it\u2019s not in a niche which rewards the brain getting bigger. It\u2019s maybe similar to some really smart\u2026</p><p><strong>Andrej Karpathy</strong></p><p>Like dolphins?</p><p><strong>Dwarkesh Patel</strong></p><p>Exaclty, humans, we have hands that reward being able to learn how to do tool use. We can externalize digestion, more energy to the brain, and that kicks off the flywheel.</p><p><strong>Andrej Karpathy </strong><em>01:37:28</em></p><p>Also stuff to work with. I\u2019m guessing it would be harder if I were a dolphin. How do you have fire? The universe of things you can do in water, inside water, is probably lower than what you can do on land, just chemically.</p><p>I do agree with this viewpoint of these niches and what\u2019s being incentivized. I still find it miraculous. I would have expected things to get stuck on animals with bigger muscles. Going through intelligence is a really fascinating breaking point.</p><p><strong>Dwarkesh Patel </strong><em>01:38:02</em></p><p>The way Gwern put it is the reason it was so hard is that it\u2019s a very tight line between being in a situation where something is so important to learn that it\u2019s not worth distilling the exact right circuits directly back into your DNA, versus it\u2019s not important enough to learn at all. It has to be something that incentivizes building the algorithm to learn in a lifetime.</p><p><strong>Andrej Karpathy </strong><em>01:38:28</em></p><p>You have to incentivize some kind of adaptability. You want environments that are unpredictable so evolution can\u2019t bake your algorithms into your weights. A lot of animals are pre-baked in this sense. Humans have to figure it out at test time when they get born. You want these environments that change really rapidly, where you can\u2019t foresee what will work well. You create intelligence to figure it out at test time.</p><p><strong>Dwarkesh Patel </strong><em>01:38:55</em></p><p><a href=\\\"https://www.alignmentforum.org/users/quintin-pope\\\">Quintin Pope</a> had <a href=\\\"https://www.openphilanthropy.org/wp-content/uploads/Evolution-provides-no-evidence-for-the-sharp-left-turn-LessWrong-Quintin-Pope.pdf\\\">this interesting blog post</a> where he\u2019s saying the reason he doesn\u2019t expect a sharp takeoff is that humans had the sharp takeoff where 60,000 years ago we seem to have had the cognitive architectures that we have today. 10,000 years ago, <a href=\\\"https://en.wikipedia.org/wiki/Neolithic_Revolution\\\">agricultural revolution</a>, modernity. What was happening in that 50,000 years? You had to build this cultural scaffold where you can accumulate knowledge over generations.</p><p>This is an ability that exists for free in the way we do AI training. In many cases they are literally distilled. If you retrain a model, they can be trained on each other, they can be trained on the same pre-training corpus, they don\u2019t literally have to start from scratch. There\u2019s a sense in which it took humans a long time to get this cultural loop going, but it just comes for free with the way we do LLM training.</p><p><strong>Andrej Karpathy </strong><em>01:39:45</em></p><p>Yes and no. Because LLMs don\u2019t really have the equivalent of culture. Maybe we\u2019re giving them way too much and incentivizing not to create it or something like that. But the invention of culture and of written record and of passing down notes between each other, I don\u2019t think there\u2019s an equivalent of that with LLMs right now. LLMs don\u2019t really have culture right now and it\u2019s one of the impediments I would say.</p><p><strong>Dwarkesh Patel </strong><em>01:40:05</em></p><p>Can you give me some sense of what LLM culture might look like?</p><p><strong>Andrej Karpathy </strong><em>01:40:09</em></p><p>In the simplest case it would be a giant scratchpad that the LLM can edit and as it\u2019s reading stuff or as it\u2019s helping out with work, it\u2019s editing the scratchpad for itself. Why can\u2019t an LLM write a book for the other LLMs? That would be cool. Why can\u2019t other LLMs read this LLM\u2019s book and be inspired by it or shocked by it or something like that? There\u2019s no equivalence for any of this stuff.</p><p><strong>Dwarkesh Patel </strong><em>01:40:29</em></p><p>Interesting. When would you expect that kind of thing to start happening? Also, <a href=\\\"https://en.wikipedia.org/wiki/Multi-agent_system\\\">multi-agent systems</a> and a sort of independent AI civilization and culture?</p><p><strong>Andrej Karpathy </strong><em>01:40:40</em></p><p>There are two powerful ideas in the realm of multi-agent that have both not been really claimed or so on. The first one I would say is culture and LLMs having a growing repertoire of knowledge for their own purposes.</p><p>The second one looks a lot more like the powerful idea of <a href=\\\"https://en.wikipedia.org/wiki/Self-play\\\">self-play</a>. In my mind it\u2019s extremely powerful. Evolution has a lot of competition driving intelligence and evolution. In <a href=\\\"https://en.wikipedia.org/wiki/AlphaGo\\\">AlphaGo</a> more algorithmically, AlphaGo is playing against itself and that\u2019s how it learns to get really good at Go. There\u2019s no equivalent of self-playing LLMs, but I would expect that to also exist. No one has done it yet. Why can\u2019t an LLM for example, create a bunch of problems that another LLM is learning to solve? Then the LLM is always trying to serve more and more difficult problems, stuff like that.</p><p>There\u2019s a bunch of ways to organize it. It\u2019s a realm of research, but I haven\u2019t seen anything that convincingly claims both of those multi-agent improvements. We\u2019re mostly in the realm of a single individual agent, but that will change. In the realm of culture also, I would also bucket organizations. We haven\u2019t seen anything like that convincingly either. That\u2019s why we\u2019re still early.</p><p><strong>Dwarkesh Patel </strong><em>01:41:53</em></p><p>Can you identify the key bottleneck that\u2019s preventing this kind of collaboration between LLMs?</p><p><strong>Andrej Karpathy </strong><em>01:41:59</em></p><p>Maybe the way I would put it is, some of these analogies work and they shouldn\u2019t, but somehow, remarkably, they do. A lot of the smaller models, or the dumber models, remarkably resemble a kindergarten student, or an elementary school student or high school student. Somehow, we still haven\u2019t graduated enough where this stuff can take over. My Claude Code or Codex, they still feel like this elementary-grade student. I know that they can take PhD quizzes, but they still cognitively feel like a kindergarten or an elementary school student.</p><p>I don\u2019t think they can create culture because they\u2019re still kids. They\u2019re savant kids. They have perfect memory of all this stuff. They can convincingly create all kinds of slop that looks really good. But I still think they don\u2019t really know what they\u2019re doing and they don\u2019t really have the cognition across all these little checkboxes that we still have to collect.</p><h3>01:42:55 - Why self driving took so long</h3><p><strong>Dwarkesh Patel</strong> <em>01:42:55</em></p><p>You\u2019ve talked about how you were at Tesla leading self-driving from 2017 to 2022. And you firsthand saw this progress from cool demos to now thousands of cars out there actually autonomously doing drives. Why did that take a decade? What was happening through that time?</p><p><strong>Andrej Karpathy</strong> <em>01:43:11</em></p><p>One thing I will almost instantly push back on is that this is not even near done, in a bunch of ways that I\u2019m going to get to. Self-driving is very interesting because it\u2019s definitely where I get a lot of my intuitions because I spent five years on it. It has this <a href=\\\"https://en.wikipedia.org/wiki/Self-driving_car#History\\\">entire history</a> where the first demos of self-driving go all the way to the 1980s. You can see a <a href=\\\"https://en.wikipedia.org/wiki/Navlab\\\">demo from CMU in 1986</a>. There\u2019s a truck that\u2019s driving itself on roads.</p><p>Fast forward. When I was joining Tesla, I had a very early demo of Waymo. It basically gave me a perfect drive in 2014 or something like that, so a perfect Waymo drive a decade ago. It took us around Palo Alto and so on because I had a friend who worked there. I thought it was very close and then it still took a long time.</p><p>For some kinds of tasks and jobs and so on, there\u2019s a very large demo-to-product gap where the demo is very easy, but the product is very hard. It\u2019s especially the case in cases like self-driving where the cost of failure is too high. Many industries, tasks, and jobs maybe don\u2019t have that property, but when you do have that property, that definitely increases the timelines.</p><p>For example, in software engineering, I do think that property does exist. For a lot of vibe coding, it doesn\u2019t. But if you\u2019re writing actual production-grade code, that property should exist, because any kind of mistake leads to a security vulnerability or something like that. Millions and hundreds of millions of people\u2019s personal Social Security numbers get leaked or something like that. So in software, people should be careful, kind of like in self-driving. In self-driving, if things go wrong, you might get injured. There are worse outcomes. But in software, it\u2019s almost unbounded how terrible something could be.</p><p>I do think that they share that property. What takes the long amount of time and the way to think about it is that it\u2019s a march of <a href=\\\"https://en.wikipedia.org/wiki/High_availability#%22Nines%22\\\">nines</a>. Every single nine is a constant amount of work. Every single nine is the same amount of work. When you get a demo and something works 90% of the time, that\u2019s just the first nine. Then you need the second nine, a third nine, a fourth nine, a fifth nine. While I was at Tesla for five years or so, we went through maybe three nines or two nines. I don\u2019t know what it is, but multiple nines of iteration. There are still more nines to go.</p><p>That\u2019s why these things take so long. It\u2019s definitely formative for me, seeing something that was a demo. I\u2019m very unimpressed by demos. Whenever I see demos of anything, I\u2019m extremely unimpressed by that. If it\u2019s a demo that someone cooked up just to show you, it\u2019s worse. If you can interact with it, it\u2019s a bit better. But even then, you\u2019re not done. You need the actual product. It\u2019s going to face all these challenges when it comes in contact with reality and all these different pockets of behavior that need patching.</p><p>We\u2019re going to see all this stuff play out. It\u2019s a march of nines. Each nine is constant. Demos are encouraging. It\u2019s still a huge amount of work to do. It is a critical safety domain, unless you\u2019re doing vibe coding, which is all nice and fun and so on. That\u2019s why this also enforced my timelines from that perspective.</p><p><strong>Dwarkesh Patel</strong> <em>01:46:25</em></p><p>It\u2019s very interesting to hear you say that, that the safety guarantees you need from software are not dissimilar to self-driving. What people will often say is that self-driving took so long because the cost of failure is so high. A human makes a mistake on average every 400,000 miles or every seven years. If you had to release a coding agent that couldn\u2019t make a mistake for at least seven years, it would be much harder to deploy.</p><p>But your point is that if you made a catastrophic coding mistake, like breaking some important system every seven years...</p><p><strong>Andrej Karpathy </strong><em>01:46:56</em></p><p>Very easy to do.</p><p><strong>Dwarkesh Patel</strong> <em>01:46:57</em></p><p>In fact, in terms of wall clock time, it would be much less than seven years because you\u2019re constantly outputting code like that. In terms of tokens, it would be seven years. But in terms of wall clock time...</p><p><strong>Andrej Karpathy</strong> <em>01:47:09</em></p><p>In some ways, it\u2019s a much harder problem. Self-driving is just one of thousands of things that people do. It\u2019s almost like a single vertical, I suppose. Whereas when we\u2019re talking about general software engineering, it\u2019s even more... There\u2019s more surface area.</p><p><strong>Dwarkesh Patel</strong> <em>01:47:20</em></p><p>There\u2019s another objection people make to that analogy, which is that with self-driving, what took a big fraction of that time was solving the problem of having basic perception that\u2019s robust, building representations, and having a model that has some common sense so it can generalize to when it sees something that\u2019s slightly out of distribution. If somebody\u2019s waving down the road this way, you don\u2019t need to train for it. The thing will have some understanding of how to respond to something like that.</p><p>These are things we\u2019re getting for free with LLMs or <a href=\\\"https://www.nvidia.com/en-us/glossary/vision-language-models/\\\">VLMs</a> today, so we don\u2019t have to solve these very basic representation problems. So now deploying AIs across different domains will sort of be like deploying a self-driving car with current models to a different city, which is hard but not like a 10-year-long task.</p><p><strong>Andrej Karpathy</strong> <em>01:48:07</em></p><p>I\u2019m not 100% sure if I fully agree with that. I don\u2019t know how much we\u2019re getting for free. There\u2019s still a lot of gaps in understanding what we are getting. We\u2019re definitely getting more generalizable intelligence in a single entity, whereas self-driving is a very special-purpose task that requires. In some sense building a special-purpose task is maybe even harder in a certain sense because it doesn\u2019t fall out from a more general thing that you\u2019re doing at scale, if that makes sense.</p><p>But the analogy still doesn\u2019t fully resonate because the LLMs are still pretty fallible and they have a lot of gaps that still need to be filled in. I don\u2019t think that we\u2019re getting magical generalization completely out of the box, in a certain sense.</p><p>The other aspect that I wanted to return to is that self-driving cars are nowhere near done still. The deployments are pretty minimal. Even Waymo and so on has very few cars. They\u2019re doing that roughly speaking because they\u2019re not economical. They\u2019ve built something that lives in the future. They\u2019ve had to pull back the future, but they had to make it uneconomical. There are all these costs, not just marginal costs for those cars and their operation and maintenance, but also the capex of the entire thing. Making it economical is still going to be a slog for them.</p><p>Also, when you look at these cars and there\u2019s no one driving, I actually think it\u2019s a little bit deceiving because there are very elaborate teleoperation centers of people kind of in a loop with these cars. I don\u2019t have the full extent of it, but there\u2019s more human-in-the-loop than you might expect. There are people somewhere out there beaming in from the sky. I don\u2019t know if they\u2019re fully in the loop with the driving. Some of the time they are, but they\u2019re certainly involved and there are people. In some sense, we haven\u2019t actually removed the person, we\u2019ve moved them to somewhere where you can\u2019t see them.</p><p>I still think there will be some work, as you mentioned, going from environment to environment. There are still challenges to make self-driving real. But I do agree that it\u2019s definitely crossed a threshold where it kind of feels real, unless it\u2019s really teleoperated. For example, Waymo can\u2019t go to all the different parts of the city. My suspicion is that it\u2019s parts of the city where you don\u2019t get good signal. Anyway, I don\u2019t know anything about the stack. I\u2019m just making stuff up.</p><p><strong>Dwarkesh Patel</strong> <em>01:50:23</em></p><p>You led self-driving for five years at Tesla.</p><p><strong>Andrej Karpathy</strong> <em>01:50:27</em></p><p>Sorry, I don\u2019t know anything about the specifics of Waymo. By the way, I love Waymo and I take it all the time. I just think that people are sometimes a little bit too naive about some of the progress and there\u2019s still a huge amount of work. Tesla took in my mind a much more scalable approach and the team is doing extremely well. I\u2019m kind of on the record for predicting how this thing will go. Waymo had an early start because you can package up so many sensors. But I do think Tesla is taking the more scalable strategy and it\u2019s going to look a lot more like that. So this will still have to play out and hasn\u2019t. But I don\u2019t want to talk about self-driving as something that took a decade because it didn\u2019t take it yet, if that makes sense.</p><p><strong>Dwarkesh Patel</strong> <em>01:51:08</em></p><p>Because one, the start is at 1980 and not 10 years ago, and then two, the end is not here yet.</p><p><strong>Andrej Karpathy</strong> <em>01:51:14</em></p><p>The end is not near yet because when we\u2019re talking about self-driving, usually in my mind it\u2019s self-driving at scale. People don\u2019t have to get a driver\u2019s license, etc.</p><p><strong>Dwarkesh Patel</strong> <em>01:51:22</em></p><p>I\u2019m curious to bounce two other ways in which the analogy might be different. The reason I\u2019m especially curious about this is because the question of how fast AI is deployed, how valuable it is when it\u2019s early on is potentially the most important question in the world right now. If you\u2019re trying to model what the year 2030 looks like, this is the question you ought to have some understanding of.</p><p>Another thing you might think is one, you have this latency requirement with self-driving. I have no idea what the actual models are, but I assume it\u2019s like tens of millions of parameters or something, which is not the necessary constraint for knowledge work with LLMs. Maybe it might be with computer use and stuff.</p><p>But the other big one is, maybe more importantly, on this capex question. Yes, there is additional cost to serving up an additional copy of a model, but the opex of a session is quite low and you can amortize the cost of AI into the training run itself, depending on how inference scaling goes and stuff. But it\u2019s certainly not as much as building a whole new car to serve another instance of a model. So the economics of deploying more widely are much more favorable.</p><p><strong>Andrej Karpathy</strong> <em>01:52:37</em></p><p>I think that\u2019s right. If you\u2019re sticking to the realm of bits, bits are a million times easier than anything that touches the physical world. I definitely grant that. Bits are completely changeable, arbitrarily reshuffleable at a very rapid speed. You would expect a much faster adaptation also in the industry and so on. What was the first one?</p><p><strong>Dwarkesh Patel</strong> <em>01:52:59</em></p><p>The latency requirements and its implications for model size?</p><p><strong>Andrej Karpathy</strong> <em>01:53:02</em></p><p>I think that\u2019s roughly right. I also think that if we are talking about knowledge work at scale, there will be some latency requirements, practically speaking, because we\u2019re going to have to create a huge amount of compute and serve that.</p><p>The last aspect that I very briefly want to also talk about is all the rest of it. What does society think about it? What are the legal ramifications? How is it working legally? How is it working insurance-wise? What are those layers of it and aspects of it? What is the equivalent of people putting a cone on a Waymo? There are going to be equivalents of all that. So I feel like self-driving is a very nice analogy that you can borrow things from. What is the equivalent of a cone in the car? What is the equivalent of a teleoperating worker who\u2019s hidden away and all the aspects of it.</p><p><strong>Dwarkesh Patel</strong> <em>01:53:53</em></p><p>Do you have any opinions on what this implies about the current AI buildout, which would 10x the amount of available compute in the world in a year or two and maybe more than 100x it by the end of the decade. If the use of AI will be lower than some people naively predict, does that mean that we\u2019re overbuilding compute or is that a separate question?</p><p><strong>Andrej Karpathy</strong> <em>01:54:15</em></p><p>Kind of like what happened with <a href=\\\"https://www.focus-economics.com/blog/railway-mania-the-largest-speculative-bubble-you-never-heard-of/\\\">railroads</a>.</p><p><strong>Dwarkesh Patel</strong> <em>01:54:18</em></p><p>With what, sorry?</p><p><strong>Andrej Karpathy</strong> <em>01:54:19</em></p><p>Was it railroads or?</p><p><strong>Dwarkesh Patel</strong> <em>01:54:20</em></p><p>Yeah, it was.</p><p><strong>Andrej Karpathy</strong> <em>01:54:21</em></p><p>Yeah. There\u2019s historical precedent. Or was it with the telecommunication industry? Pre-paving the internet that only came a decade later and creating a <a href=\\\"https://www.fabricatedknowledge.com/p/lessons-from-history-the-rise-and\\\">whole bubble in the telecommunications industry</a> in the late \u201890s.</p><p>I understand I\u2019m sounding very pessimistic here. I\u2019m actually optimistic. I think this will work. I think it\u2019s tractable. I\u2019m only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me. There\u2019s a lot of reasons for why that exists. A lot of it is honestly just fundraising. It\u2019s just incentive structures. A lot of it may be fundraising. A lot of it is just attention, converting attention to money on the internet, stuff like that. There\u2019s a lot of that going on, and I\u2019m only reacting to that.</p><p>But I\u2019m still overall very bullish on technology. We\u2019re going to work through all this stuff. There\u2019s been a rapid amount of progress. I don\u2019t know that there\u2019s overbuilding. I think we\u2019re going to be able to gobble up what, in my understanding, is being built. For example, Claude Code or OpenAI Codex and stuff like that didn\u2019t even exist a year ago. Is that right? This is a miraculous technology that didn\u2019t exist. There\u2019s going to be a huge amount of demand, as we see the demand in ChatGPT already and so on.</p><p>So I don\u2019t know that there\u2019s overbuilding. I\u2019m just reacting to some of the very fast timelines that people continue to say incorrectly. I\u2019ve heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. I want this to be properly calibrated, and some of this also has geopolitical ramifications and things like that with some of these questions. I don\u2019t want people to make mistakes in that sphere of things. I do want us to be grounded in the reality of what technology is and isn\u2019t.</p><h3>01:56:20 - Future of education</h3><p><strong>Dwarkesh Patel </strong><em>01:56:20</em></p><p>Let\u2019s talk about education and <a href=\\\"https://eurekalabs.ai/\\\">Eureka</a>. One thing you could do is start another AI lab and then try to solve those problems. I\u2019m curious what you\u2019re up to now, and why not AI research itself?</p><p><strong>Andrej Karpathy </strong><em>01:56:33</em></p><p>I guess the way I would put it is I feel some amount of determinism around the things that AI labs are doing. I feel like I could help out there, but I don\u2019t know that I would uniquely improve it. My personal big fear is that a lot of this stuff happens on the side of humanity, and that humanity gets disempowered by it. I care not just about all the <a href=\\\"https://en.wikipedia.org/wiki/Dyson_sphere\\\">Dyson spheres</a> that we\u2019re going to build and that AI is going to build in a fully autonomous way, I care about what happens to humans. I want humans to be well off in the future.</p><p>I feel like that\u2019s where I can a lot more uniquely add value than an incremental improvement in the frontier lab. I\u2019m most afraid of something depicted in movies like <em>WALL-E</em> or <em>Idiocracy</em> or something like that, where humanity is on the side of this stuff. I want humans to be much, much better in this future. To me, this is through education that you can achieve this.</p><p><strong>Dwarkesh Patel </strong><em>01:57:35</em></p><p>So what are you working on there?</p><p><strong>Andrej Karpathy </strong><em>01:57:36</em></p><p>The easiest way I can describe it is we\u2019re trying to build the <a href=\\\"https://memory-alpha.fandom.com/wiki/Starfleet_Academy\\\">Starfleet Academy</a>. I don\u2019t know if you\u2019ve watched <em>Star Trek</em>.</p><p><strong>Dwarkesh Patel </strong><em>01:57:44</em></p><p>I haven\u2019t.</p><p><strong>Andrej Karpathy </strong><em>01:57:44</em></p><p>Starfleet Academy is this elite institution for frontier technology, building spaceships, and graduating cadets to be the pilots of these spaceships and whatnot. So I just imagine an elite institution for technical knowledge and a kind of school that\u2019s very up-to-date and a premier institution.</p><p><strong>Dwarkesh Patel </strong><em>01:58:05</em></p><p>A category of questions I have for you is explaining how one teaches technical or scientific content well, because you are one of the world masters at it. I\u2019m curious both about how you think about it for content you\u2019ve already put out there on <a href=\\\"https://www.youtube.com/andrejkarpathy\\\">YouTube</a>, but also, to the extent it\u2019s any different, how you think about it for Eureka.</p><p><strong>Andrej Karpathy </strong><em>01:58:25</em></p><p>With respect to Eureka, one thing that is very fascinating to me about education is that I do think education will pretty fundamentally change with AIs on the side. It has to be rewired and changed to some extent.</p><p>I still think that we\u2019re pretty early. There\u2019s going to be a lot of people who are going to try to do the obvious things. Have an LLM and ask it questions. Do all the basic things that you would do via prompting right now. It\u2019s helpful, but it still feels to me a bit like slop. I\u2019d like to do it properly, and I think the capability is not there for what I would want. What I\u2019d want is an actual tutor experience.</p><p>A prominent example in my mind is I was recently learning Korean, so language learning. I went through a phase where I was learning Korean by myself on the internet. I went through a phase where I was part of a small class in Korea taking Korean with a bunch of other people, which was really funny. We had a teacher and 10 people or so taking Korean. Then I switched to a one-on-one tutor.</p><p>I guess what was fascinating to me was, I think I had a really good tutor, but just thinking through what this tutor was doing for me and how incredible that experience was and how high the bar is for what I want to build eventually. Instantly from a very short conversation, she understood where I am as a student, what I know and don\u2019t know. She was able to probe exactly the kinds of questions or things to understand my world model. No LLM will do that for you 100% right now, not even close. But a tutor will do that if they\u2019re good. Once she understands, she really served me all the things that I needed at my current sliver of capability. I need to be always appropriately challenged. I can\u2019t be faced with something too hard or too trivial, and a tutor is really good at serving you just the right stuff.</p><p>I felt like I was the only constraint to learning. I was always given the perfect information. I\u2019m the only constraint. I felt good because I\u2019m the only impediment that exists. It\u2019s not that I can\u2019t find knowledge or that it\u2019s not properly explained or etc. It\u2019s just my ability to memorize and so on. This is what I want for people.</p><p><strong>Dwarkesh Patel </strong><em>02:00:27</em></p><p>How do you automate that?</p><p><strong>Andrej Karpathy </strong><em>02:00:29</em></p><p>Very good question. At the current capability, you don\u2019t. That\u2019s why I think it\u2019s not actually the right time to build this kind of an AI tutor. I still think it\u2019s a useful product, and lots of people will build it, but the bar is so high and the capability is not there. Even today, I would say ChatGPT is an extremely valuable educational product. But for me, it was so fascinating to see how high the bar is. When I was with her, I almost felt like there\u2019s no way I can build this.</p><p><strong>Dwarkesh Patel </strong><em>02:01:02</em></p><p>But you are building it, right?</p><p><strong>Andrej Karpathy </strong><em>02:01:03</em></p><p>Anyone who\u2019s had a really good tutor is like, \u201CHow are you going to build this?\u201D I\u2019m waiting for that capability.</p><p>I did some AI consulting for computer vision. A lot of times, the value that I brought to the company was telling them not to use AI. I was the AI expert, and they described the problem, and I said, \u201CDon\u2019t use AI.\u201D This is my value add. I feel like it\u2019s the same in education right now, where I feel like for what I have in mind, it\u2019s not yet the time, but the time will come. For now, I\u2019m building something that looks maybe a bit more conventional that has a physical and digital component and so on. But it\u2019s obvious how this should look in the future.</p><p><strong>Dwarkesh Patel </strong><em>02:01:43</em></p><p>To the extent you\u2019re willing to say, what is the thing you hope will be released this year or next year?</p><p><strong>Andrej Karpathy </strong><em>02:01:49</em></p><p>I\u2019m building the first course. I want to have a really, really good course, the obvious state-of-the-art destination you go to to learn, AI in this case. That\u2019s just what I\u2019m familiar with, so it\u2019s a really good first product to get to be really good at it. So that\u2019s what I\u2019m building. Nanochat, which you briefly mentioned, is a capstone project of <a href=\\\"https://github.com/karpathy/LLM101n\\\">LLM101N</a>, which is a class that I\u2019m building. That\u2019s a really big piece of it. But now I have to build out a lot of the intermediates, and then I have to hire a small team of TAs and so on and build the entire course.</p><p>One more thing that I would say is that many times, when people think about education, they think more about what I would say is a softer component of diffusing knowledge. I have something very hard and technical in mind. In my mind, education is the very difficult technical process of building ramps to knowledge. In my mind, nanochat is a ramp to knowledge because it\u2019s very simple. It\u2019s the super simplified full-stack thing. If you give this artifact to someone and they look through it, they\u2019re learning a ton of stuff. It\u2019s giving you a lot of what I call eurekas per second, which is understanding per second. That\u2019s what I want, lots of eurekas per second. So to me, this is a technical problem of how do we build these ramps to knowledge.</p><p>So I almost think of Eureka as maybe not that different from some of the frontier labs or some of the work that\u2019s going on there. I want to figure out how to build these ramps very efficiently so that people are never stuck and everything is always not too hard or not too trivial, and you have just the right material to progress.</p><p><strong>Dwarkesh Patel </strong><em>02:03:25</em></p><p>You\u2019re imagining in the short term that instead of a tutor being able to probe your understanding, if you have enough self-awareness to be able to probe yourself, you\u2019re never going to be stuck. You can find the right answer between talking to the TA or talking to an LLM and looking at the reference implementation. It sounds like automation or AI is not a significant part. So far, the big alpha here is your ability to explain AI codified in the source material of the class. That\u2019s fundamentally what the course is.</p><p><strong>Andrej Karpathy </strong><em>02:04:00</em></p><p>You always have to be calibrated to what capability exists in the industry. A lot of people are going to pursue just asking ChatGPT, etc. But I think right now, for example, if you go to ChatGPT and you say, teach me AI, there\u2019s no way. It\u2019s going to give you some slop. AI is never going to write nanochat right now. But nanochat is a really useful intermediate point. I\u2019m collaborating with AI to create all this material, so AI is still fundamentally very helpful.</p><p>Earlier on, I built <a href=\\\"https://cs231n.stanford.edu/\\\">CS231n</a> at Stanford, which I think was the first deep learning class at Stanford, which became very popular. The difference in building out 231n then and LLM101N now is quite stark. I feel really empowered by the LLMs as they exist right now, but I\u2019m very much in the loop. They\u2019re helping me build the materials, I go much faster. They\u2019re doing a lot of the boring stuff, etc. I feel like I\u2019m developing the course much faster, and it\u2019s LLM-infused, but it\u2019s not yet at a place where it can creatively create the content. I\u2019m still there to do that. The trickiness is always calibrating yourself to what exists.</p><p><strong>Dwarkesh Patel </strong><em>02:05:04</em></p><p>When you imagine what is available through Eureka in a couple of years, it seems like the big bottleneck is going to be finding Karpathys in field after field who can convert their understanding into these ramps.</p><p><strong>Andrej Karpathy </strong><em>02:05:18</em></p><p>It would change over time. Right now, it would be hiring faculty to help work hand-in-hand with AI and a team of people probably to build state-of-the-art courses. Over time maybe some of the TAs can become AIs. You just take all the course materials and then I think you could serve a very good automated TA for the student when they have more basic questions or something like that. But I think you\u2019ll need faculty for the overall architecture of a course and making sure that it fits. So I see a progression of how this will evolve. Maybe at some future point I\u2019m not even that useful and AI is doing most of the design much better than I could. But I still think that\u2019s going to take some time to play out.</p><p><strong>Dwarkesh Patel </strong><em>02:05:59</em></p><p>Are you imagining that people who have expertise in other fields are then contributing courses, or do you feel like it\u2019s quite essential to the vision that you, given your understanding of how you want to teach, are the one designing the content? <a href=\\\"https://en.wikipedia.org/wiki/Sal_Khan\\\">Sal Khan</a> is narrating all the videos on <a href=\\\"http://khanacademy.org/\\\">Khan Academy</a>. Are you imagining something like that?</p><p><strong>Andrej Karpathy </strong><em>02:06:20</em></p><p>No, I will hire faculty because there are domains in which I\u2019m not an expert. That\u2019s the only way to offer the state-of-the-art experience for the student ultimately. I do expect that I would hire faculty, but I will probably stick around in AI for some time. I do have something more conventional in mind for the current capability than what people would probably anticipate.</p><p>When I\u2019m building Starfleet Academy, I do probably imagine a physical institution, and maybe a tier below that a digital offering that is not the state-of-the-art experience you would get when someone comes in physically full-time and we work through material from start to end and make sure you understand it. That\u2019s the physical offering. The digital offering is a bunch of stuff on the internet and maybe some LLM assistant. It\u2019s a bit more gimmicky in a tier below, but at least it\u2019s accessible to 8 billion people.</p><p><strong>Dwarkesh Patel </strong><em>02:07:08</em></p><p>I think you\u2019re basically inventing college from first principles for the tools that are available today and just selecting for people who have the motivation and the interest of really engaging with material.</p><p><strong>Andrej Karpathy </strong><em>02:07:26</em></p><p>There\u2019s going to have to be a lot of not just education but also re-education. I would love to help out there because the jobs will probably change quite a bit. For example, today a lot of people are trying to upskill in AI specifically. I think it\u2019s a really good course to teach in this respect. Motivation-wise, before AGI motivation is very simple to solve because people want to make money. This is how you make money in the industry today. Post-AGI is a lot more interesting possibly because if everything is automated and there\u2019s nothing to do for anyone, why would anyone go to a school?</p><p>I often say that pre-AGI education is useful. Post-AGI education is fun. In a similar way, people go to the gym today. We don\u2019t need their physical strength to manipulate heavy objects because we have machines that do that. They still go to the gym. Why do they go to the gym? Because it\u2019s fun, it\u2019s healthy, and you look hot when you have a six-pack. It\u2019s attractive for people to do that in a very deep, psychological, evolutionary sense for humanity. Education will play out in the same way. You\u2019ll go to school like you go to the gym.</p><p>Right now, not that many people learn because learning is hard. You bounce from material. Some people overcome that barrier, but for most people, it\u2019s hard. It\u2019s a technical problem to solve. It\u2019s a technical problem to do what my tutor did for me when I was learning Korean. It\u2019s tractable and buildable, and someone should build it. It\u2019s going to make learning anything trivial and desirable, and people will do it for fun because it\u2019s trivial. If I had a tutor like that for any arbitrary piece of knowledge, it\u2019s going to be so much easier to learn anything, and people will do it. They\u2019ll do it for the same reasons they go to the gym.</p><p><strong>Dwarkesh Patel </strong><em>02:09:17</em></p><p>That sounds different from using\u2026 So post-AGI, you\u2019re using this as entertainment or as self-betterment. But it sounded like you had a vision also that this education is relevant to keeping humanity in control of AI. That sounds different. Is it entertaining for some people, but then empowerment for some others? How do you think about that?</p><p><strong>Andrej Karpathy </strong><em>02:09:41</em></p><p>I do think eventually it\u2019s a bit of a losing game, if that makes sense. It is in the long term. In the long term, which is longer than maybe most people in the industry think about, it\u2019s a losing game. I do think people can go so far and we\u2019ve barely scratched the surface of how much a person can go. That\u2019s just because people are bouncing off of material that\u2019s too easy or too hard. People will be able to go much further. Anyone will speak five languages because why not? Because it\u2019s so trivial. Anyone will know all the basic curriculum of undergrad, et cetera.</p><p><strong>Dwarkesh Patel </strong><em>02:10:18</em></p><p>Now that I\u2019m understanding the vision, that\u2019s very interesting. It has a perfect analog in gym culture. I don\u2019t think 100 years ago anybody would be ripped. Nobody would have been able to just spontaneously bench two plates or three plates or something. It\u2019s very common now because of this idea of systematically training and lifting weights in the gym, or systematically training to be able to run a marathon, which is a capability most humans would not spontaneously have. You\u2019re imagining similar things for learning across many different domains, much more intensely, deeply, faster.</p><p><strong>Andrej Karpathy </strong><em>02:10:54</em></p><p>Exactly. I am betting a bit implicitly on some of the timelessness of human nature. It will be desirable to do all these things, and I think people will look up to it as they have for millennia. This will continue to be true. There\u2019s some evidence of that historically. If you look at, for example, aristocrats, or you look at ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, people have spent a lot of their time flourishing in a certain way, either physically or cognitively. I feel okay about the prospects of that.</p><p>If this is false and I\u2019m wrong and we end up in a <em>WALL-E</em> or <em>Idiocracy</em> future, then I don\u2019t even care if there are Dyson spheres. This is a terrible outcome. I really do care about humanity. Everyone has to just be superhuman in a certain sense.</p><p><strong>Dwarkesh Patel </strong><em>02:11:52</em></p><p>It\u2019s still a world in which that is not enabling us to\u2026 It\u2019s like the culture world, right? You\u2019re not fundamentally going to be able to transform the trajectory of technology or influence decisions by your own labor or cognition alone. Maybe you can influence decisions because the AI is asking for your approval, but it\u2019s not because I\u2019ve invented something or I\u2019ve come up with a new design that I\u2019m really influencing the future.</p><p><strong>Andrej Karpathy </strong><em>02:12:21</em></p><p>Maybe. I think there will be a transitional period where we are going to be able to be in the loop and advance things if we understand a lot of stuff. In the long-term, that probably goes away. It might even become a sport. Right now you have powerlifters who go extreme in this direction. What is powerlifting in a cognitive era? Maybe it\u2019s people who are really trying to make Olympics out of knowing stuff. If you have a perfect AI tutor, maybe you can get extremely far. I feel that the geniuses of today are barely scratching the surface of what a human mind can do, I think.</p><p><strong>Dwarkesh Patel </strong><em>02:12:59</em></p><p>I love this vision. I also feel like the person you have the most product-market fit with is me because my job involves having to learn different subjects every week, and I am very excited.</p><p><strong>Andrej Karpathy </strong><em>02:13:17</em></p><p>I\u2019m similar, for that matter. A lot of people, for example, hate school and want to get out of it. I really liked school. I loved learning things, et cetera. I wanted to stay in school. I stayed all the way until Ph.D. and then they wouldn\u2019t let me stay longer, so I went to the industry. Roughly speaking, I love learning, even for the sake of learning, but I also love learning because it\u2019s a form of empowerment and being useful and productive.</p><p><strong>Dwarkesh Patel </strong><em>02:13:39</em></p><p>You also made a point that was subtle and I want to spell it out. With what\u2019s happened so far with online courses, why haven\u2019t they already enabled us to enable every single human to know everything? They\u2019re just so motivation-laden because there are no obvious on-ramps and it\u2019s so easy to get stuck. If you had this thing instead\u2014like a really good human tutor\u2014it would just be such an unlock from a motivation perspective.</p><p><strong>Andrej Karpathy </strong><em>02:14:10</em></p><p>I think so. It feels bad to bounce from material. It feels bad. You get negative reward from sinking an amount of time in something and it doesn\u2019t pan out, or being completely bored because what you\u2019re getting is too easy or too hard. When you do it properly, learning feels good. It\u2019s a technical problem to get there. For a while, it\u2019s going to be AI plus human collab, and at some point, maybe it\u2019s just AI.</p><p><strong>Dwarkesh Patel </strong><em>02:14:36</em></p><p>Can I ask some questions about teaching well? If you had to give advice to another educator in another field that you\u2019re curious about to make the kinds of YouTube tutorials you\u2019ve made. Maybe it might be especially interesting to talk about domains where you can\u2019t test someone\u2019s technical understanding by having them code something up or something. What advice would you give them?</p><p><strong>Andrej Karpathy </strong><em>02:14:58</em></p><p>That\u2019s a pretty broad topic. There are 10\u201320 tips and tricks that I semi-consciously do probably. But a lot of this comes from my physics background. I really, really did enjoy my physics background. I have a whole rant on how everyone should learn physics in early school education because early school education is not about accumulating knowledge or memory for tasks later in the industry. It\u2019s about booting up a brain. Physics uniquely boots up the brain the best because some of the things that they get you to do in your brain during physics is extremely valuable later.</p><p>The idea of building models and abstractions and understanding that there\u2019s a first-order approximation that describes most of the system, but then there\u2019re second-order, third-order, fourth-order terms that may or may not be present. The idea that you\u2019re observing a very noisy system, but there are these fundamental frequencies that you can abstract away. When a physicist walks into the class and they say, \u201CAssume there\u2019s a spherical cow,\u201D everyone laughs at that, but this is brilliant. It\u2019s brilliant thinking that\u2019s very generalizable across the industry because a cow can be approximated as a sphere in a bunch of ways.</p><p>There\u2019s a really good book, for example, <em><a href=\\\"https://amzn.to/47ilBeP\\\">Scale</a></em>. It\u2019s from a <a href=\\\"https://en.wikipedia.org/wiki/Geoffrey_West\\\">physicist</a> talking about biology. Maybe this is also a book I would recommend reading. You can get a lot of really interesting approximations and chart scaling laws of animals. You can look at their <a href=\\\"https://johnmjennings.com/animal-size-heartbeats-and-longevity/\\\">heartbeats</a> and things like that, and they line up with the size of the animal and things like that. You can talk about an animal as a volume. You can talk about the heat dissipation of that, because your heat dissipation grows as the surface area, which is growing as a square. But your heat creation or generation is growing as a cube. So I just feel like physicists have all the right cognitive tools to approach problem solving in the world.</p><p>So because of that training, I always try to find the first-order terms or the second-order terms of everything. When I\u2019m observing a system or a thing, I have a tangle of a web of ideas or knowledge in my mind. I\u2019m trying to find, what is the thing that matters? What is the first-order component? How can I simplify it? How can I have a simplest thing that shows that thing, shows it in action, and then I can tack on the other terms?</p><p>Maybe an example from one of my repos that I think illustrates it well is called <a href=\\\"https://github.com/karpathy/micrograd\\\">micrograd</a>. I don\u2019t know if you\u2019re familiar with this. So micrograd is 100 lines of code that shows backpropagation. You can create neural networks out of simple operations like plus and times, et cetera. Lego blocks of neural networks. You build up a computational graph and you do a forward pass and a backward pass to get the gradients. Now, this is at the heart of all neural network learning.</p><p>So micrograd is a 100 lines of pretty interpretable Python code, and it can do forward and backward arbitrary neural networks, but not efficiently. So micrograd, these 100 lines of Python, are everything you need to understand how neural networks train. Everything else is just efficiency. Everything else is efficiency. There\u2019s a huge amount of work to get efficiency. You need your <a href=\\\"https://en.wikipedia.org/wiki/Tensor_(machine_learning)\\\">tensors</a>, you lay them out, you stride them, you make sure your kernels, orchestrating memory movement correctly, et cetera. It\u2019s all just efficiency, roughly speaking. But the core intellectual piece of neural network training is micrograd. It\u2019s 100 lines. You can easily understand it. It\u2019s a recursive application of <a href=\\\"https://en.wikipedia.org/wiki/Chain_rule\\\">chain rule</a> to derive the gradient, which allows you to optimize any arbitrary differentiable function.</p><p>So I love finding these small-order terms and serving them on a platter and discovering them. I feel like education is the most intellectually interesting thing because you have a tangle of understanding and you\u2019re trying to lay it out in a way that creates a ramp where everything only depends on the thing before it. I find that this untangling of knowledge is just so intellectually interesting as a cognitive task. I love doing it personally, but I just have a fascination with trying to lay things out in a certain way. Maybe that helps me.</p><p><strong>Dwarkesh Patel</strong> <em>02:18:41</em></p><p>It also makes the learning experience so much more motivated. Your tutorial on the transformer begins with <a href=\\\"https://en.wikipedia.org/wiki/Bigram\\\">bigrams</a>, literally a lookup table from, \u201CHere\u2019s the word right now, or here\u2019s the previous word, here\u2019s the next word.\u201D It\u2019s literally just a lookup table.</p><p><strong>Andrej Karpathy</strong> <em>02:18:58</em></p><p>That\u2019s the essence of it, yeah.</p><p><strong>Dwarkesh Patel</strong> <em>02:18:59</em></p><p>It\u2019s such a brilliant way, starting with a lookup table and then going to a transformer. Each piece is motivated. Why would you add that? Why would you add the next thing? You could memorize the attention formula, but having an understanding of why every single piece is relevant, what problem it solves.</p><p><strong>Andrej Karpathy</strong> <em>02:19:13</em></p><p>You\u2019re presenting the pain before you present a solution, and how clever is that? You want to take the student through that progression. There are a lot of other small things that make it nice and engaging and interesting. Always prompting the student.</p><p>\u200AThere\u2019s a lot of small things like that are important and a lot of good educators will do this. How would you solve this? I\u2019m not going to present the solution before you guess. That would be wasteful. That\u2019s a little bit of a\u2026I don\u2019t want to swear but it\u2019s a dick move towards you to present you with the solution before I give you a shot to try to come up with it yourself.</p><p><strong>Dwarkesh Patel</strong> <em>02:19:51</em></p><p>Because if you try to come up with it yourself, you get a better understanding of what the action space is, what the objective is, and then why only this action fulfills that objective.</p><p><strong>Andrej Karpathy</strong> <em>02:20:03</em></p><p>You have a chance to try it yourself, and you have an appreciation when I give you the solution. It maximizes the amount of knowledge per new fact added.</p><p><strong>Dwarkesh Patel</strong> <em>02:20:11</em></p><p>Why do you think, by default, people who are genuine experts in their field are often bad at explaining it to somebody ramping up?</p><p><strong>Andrej Karpathy</strong> <em>02:20:24</em></p><p>It\u2019s the <a href=\\\"https://en.wikipedia.org/wiki/Curse_of_knowledge\\\">curse of knowledge</a> and expertise. This is a real phenomenon, and I suffered from it myself as much as I try not to. But you take certain things for granted, and you can\u2019t put yourself in the shoes of new people who are just starting out. This is pervasive and happens to me as well.</p><p>One thing that\u2019s extremely helpful. As an example, someone was trying to show me a paper in biology recently, and I just instantly had so many terrible questions. What I did was I used ChatGPT to ask the questions with the paper in the context window. It worked through some of the simple things. Then I shared the thread to the person who wrote that paper or worked on that work. I felt like if they could see the dumb questions I had, it might help them explain better in the future.</p><p>For my material, I would love it if people shared their dumb conversations with ChatGPT about the stuff that I\u2019ve created because it really helps me put myself again in the shoes of someone who\u2019s starting out.</p><p><strong>Dwarkesh Patel</strong> <em>02:21:19</em></p><p>Another trick that just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in 100% of cases that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there\u2019s something about communicating one-on-one with a person which compels you to just say the thing.</p><p><strong>Andrej Karpathy</strong> <em>02:22:07</em></p><p>Just say the thing. I saw that tweet, I thought it was really good. I shared it with a bunch of people. I noticed this many, many times.</p><p>The most prominent example is that I remember back in my PhD days doing research. You read someone\u2019s paper, and you work to understand what it\u2019s doing. Then you catch them, you\u2019re having beers at the conference later, and you ask them, \u201CSo this paper, what were you doing? What is the paper about?\u201D</p><p>They will just tell you these three sentences that perfectly captured the essence of that paper and totally give you the idea. And you didn\u2019t have to read the paper. It\u2019s only when you\u2019re sitting at the table with a beer or something, and they\u2019re like, \u201COh yeah, the paper is just, you take this idea, you take that idea and try this experiment and you try out this thing.\u201D They have a way of just putting it conversationally just perfectly. Why isn\u2019t that the abstract?</p><p><strong>Dwarkesh Patel</strong> <em>02:22:51</em></p><p>Exactly. This is coming from the perspective of how somebody who\u2019s trying to explain an idea should formulate it better. What is your advice as a student to other students, if you don\u2019t have a Karpathy who is doing the exposition of an idea? If you\u2019re reading a paper from somebody or reading a book, what strategies do you employ to learn material you\u2019re interested in in fields you\u2019re not an expert at?</p><p><strong>Andrej Karpathy</strong> <em>02:23:20</em></p><p>I don\u2019t know that I have unique tips and tricks, to be honest. It\u2019s a painful process. One thing that has always helped me quite a bit is\u2014<a href=\\\"https://x.com/karpathy/status/1325154823856033793?lang=en\\\">I had a small tweet about this</a>\u2014learning things on demand is pretty nice. Learning depth-wise. I do feel you need a bit of alternation of learning depth-wise, on demand\u2014you\u2019re trying to achieve a certain project that you\u2019re going to get a reward from\u2014and learning breadth-wise, which is just, \u201COh, let\u2019s do whatever 101, and here\u2019s all the things you might need.\u201D Which is a lot of school\u2014does breadth-wise learning, like, \u201COh, trust me, you\u2019ll need this later,\u201D that kind of stuff. Okay, I trust you. I\u2019ll learn it because I guess I need it. But I love the kind of learning where you\u2019ll get a reward out of doing something, and you\u2019re learning on demand.</p><p>The other thing that I\u2019ve found extremely helpful. This is an aspect where education is a bit more selfless, but explaining things to people is a beautiful way to learn something more deeply. This happens to me all the time. It probably happens to other people too because I realize if I don\u2019t really understand something, I can\u2019t explain it. I\u2019m trying and I\u2019m like, \u201COh, I don\u2019t understand this.\u201D It\u2019s so annoying to come to terms with that. You can go back and make sure you understood it. It fills these gaps of your understanding. It forces you to come to terms with them and to reconcile them.</p><p>I love to re-explain things and people should be doing that more as well. That forces you to manipulate the knowledge and make sure that you know what you\u2019re talking about when you\u2019re explaining it.</p><p><strong>Dwarkesh Patel</strong> <em>02:24:48</em></p><p>That\u2019s an excellent note to close on. Andrej, that was great.</p><p><strong>Andrej Karpathy</strong> <em>02:24:51</em></p><p>Thank you.</p>\",\"truncated_body_text\":null,\"wordcount\":26939,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[247,204,6],\"population\":65},\"DarkVibrant\":{\"rgb\":[113,88,11],\"population\":57},\"LightVibrant\":{\"rgb\":[225,166,131],\"population\":8},\"Muted\":{\"rgb\":[179,113,89],\"population\":427},\"DarkMuted\":{\"rgb\":[83,73,44],\"population\":37},\"LightMuted\":{\"rgb\":[216,214,213],\"population\":76}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":224,\"comment_count\":15,\"child_comment_count\":13,\"is_geoblocked\":false,\"hasCashtag\":false},\"comments\":null,\"canonicalUrl\":\"https://www.dwarkesh.com/p/andrej-karpathy\",\"inlineComments\":true,\"readerIsSearchCrawler\":false,\"ogUrl\":\"https://www.dwarkesh.com/p/andrej-karpathy\",\"bannedFromNotes\":false,\"themeVariables\":{\"color_theme_bg_pop\":\"#f3c016\",\"background_pop\":\"#f3c016\",\"color_theme_bg_web\":\"#ffffff\",\"cover_bg_color\":\"#ffffff\",\"background_pop_darken\":\"#e4b20c\",\"print_on_pop\":\"#ffffff\",\"color_theme_bg_pop_darken\":\"#e4b20c\",\"color_theme_print_on_pop\":\"#ffffff\",\"color_theme_bg_pop_20\":\"rgba(243, 192, 22, 0.2)\",\"color_theme_bg_pop_30\":\"rgba(243, 192, 22, 0.3)\",\"border_subtle\":\"rgba(204, 204, 204, 0.5)\",\"background_subtle\":\"rgba(253, 246, 220, 0.4)\",\"print_pop\":\"#f3c016\",\"color_theme_accent\":\"#f3c016\",\"cover_print_primary\":\"#363737\",\"cover_print_secondary\":\"#757575\",\"cover_print_tertiary\":\"#b6b6b6\",\"cover_border_color\":\"#f3c016\",\"font_family_headings_preset\":\"Lora,sans-serif\",\"font_weight_headings_preset\":600,\"font_preset_heading\":\"fancy_serif\",\"home_hero\":\"podcast\",\"home_posts\":\"custom\",\"home_show_top_posts\":true,\"web_bg_color\":\"#ffffff\",\"background_contrast_1\":\"#f0f0f0\",\"color_theme_bg_contrast_1\":\"#f0f0f0\",\"background_contrast_2\":\"#dddddd\",\"color_theme_bg_contrast_2\":\"#dddddd\",\"background_contrast_3\":\"#b7b7b7\",\"color_theme_bg_contrast_3\":\"#b7b7b7\",\"background_contrast_4\":\"#929292\",\"color_theme_bg_contrast_4\":\"#929292\",\"background_contrast_5\":\"#515151\",\"color_theme_bg_contrast_5\":\"#515151\",\"color_theme_bg_elevated\":\"#ffffff\",\"color_theme_bg_elevated_secondary\":\"#f0f0f0\",\"color_theme_bg_elevated_tertiary\":\"#dddddd\",\"color_theme_detail\":\"#e6e6e6\",\"background_contrast_pop\":\"rgba(243, 192, 22, 0.4)\",\"color_theme_bg_contrast_pop\":\"rgba(243, 192, 22, 0.4)\",\"input_background\":\"#ffffff\",\"cover_input_background\":\"#ffffff\",\"tooltip_background\":\"#191919\",\"web_bg_color_h\":\"0\",\"web_bg_color_s\":\"0%\",\"web_bg_color_l\":\"100%\",\"print_on_web_bg_color\":\"#363737\",\"print_secondary_on_web_bg_color\":\"#868787\",\"selected_comment_background_color\":\"#fdf9f3\",\"background_pop_rgb\":\"243, 192, 22\",\"background_pop_rgb_pc\":\"243 192 22\",\"color_theme_bg_pop_rgb\":\"243, 192, 22\",\"color_theme_bg_pop_rgb_pc\":\"243 192 22\",\"color_theme_accent_rgb\":\"243, 192, 22\",\"color_theme_accent_rgb_pc\":\"243 192 22\"},\"recentEpisodes\":[{\"id\":176425744,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Andrej Karpathy \u2014 AGI is still a decade away\",\"social_title\":\"Andrej Karpathy \u2014 AGI is still a decade away\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"andrej-karpathy\",\"post_date\":\"2025-10-17T16:54:33.052Z\",\"audience\":\"everyone\",\"podcast_duration\":8719.281,\"video_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"podcast_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/andrej-karpathy\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[69345],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":224},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\\\"The problems are tractable, but they're still difficult\u201D\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/176425744/248940bd-cafb-4de4-bf60-6b24fae0fa2e/transcoded-1760720000.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/59851d57-8ed7-4904-9da0-a1c611dc620c/src\",\"videoUpload\":{\"id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"name\":\"SUB0 Karpathy.mov\",\"created_at\":\"2025-10-17T15:45:15.207Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.336,\"height\":1080,\"width\":1920,\"thumbnail_id\":1760720000,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":12874298785,\"is_mux\":true,\"mux_asset_id\":\"l1NV65oRNv3QqVIoePiv01zgAygo9naA1E02c2etMnLa4\",\"mux_playback_id\":\"00jalP00100HXe4zcvoOVKJnUXYKFIsHb8TkVSwfClJeDE\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"name\":null,\"created_at\":\"2025-10-17T16:02:24.631Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.281,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":139509059,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"created_at\":\"2025-10-17T16:06:35.002Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K7SERJ8AEM97B0P3D4BZ0GXW\",\"approved_at\":\"2025-10-17T16:15:41.370Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=FHVsNF9PIV1VN-tjNAjsl4dsgk5rc1SXnUbS5PZcVYlmkOFhlznNxYlYgQ43ZekS5UIu0DAlY9Dq9wnmdKDpyHlDFIr9595Krl1risOCUGzBtHrs2sdDFf-w5xtnJOwwl8X-R0lW8aRYUIffq8vSAc9jYLu6iXmNBVD2pRgHMUrlqj449R7WaBmbDFcLKx3CoOu8h2NIr7UEtuY4RG19GWtdoZ6RdtaUijN0pnkbuQwsNyDHPJ4S-V3hH4oAFDFCZlEcimtxhe3O0cRfq~8QZLSm5o4TTJza2HZ4hcybMR2JxaWAGSUYh76pRZFJWDcEMiIGy~kX14H22Sg8qgoesA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/unaligned_transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=WiPDp-MMfScttlAjJ-E41Fi9kZxl7ozKUjo6dB0aoDv6Ce2nnLbiprCtIP1Xvvj5iUxwQhji06dYVUHC6JBdhcDkAGzCWk7NLuxIE2bmdAQrKJF6M8Cq-qb8X1gMvZbDYm1HQd-4Nf39CinyzwUOoQoaisGe2hcgcnv9LjvoGE~-yorw8n~h0UIl4xU8OcEVIc8gWEYpO0kxC4~-Of5pBaGeV0AwRPcxF7cZITDmy2gyZhctzXAcN5ro9tB8O7gyPCgrzBr03KxzGvfYbNXNKvs34AeS1nFAW6fF2qdcmlW4A2JOkfSgluP2gTVrXUYJeoKFZwF9A6vlD6Tcmiy5yQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hIEGBRT~Wivmj-J7hxBQqP3vVk8dc~AxD3li-ZuNrYrqr3NWuf~3X21JuQ~X-hPoiq3KCGPJ50~tcu8cDmTHx2qM~CFwOWM6s3yeL~1JfRFYtQl~~IcNhCUx7rrfd-G2uxWVZMNKTshFaWD0wA5SYgLNFmKsirU8lwAlSOeCL6JVAxqxOGqKYJYVZ8aoXm7GB8FYENtXf58RRpFOHNq7EfU9ffw~7nTFpLCzHcXVh5rttIYh3PifWAyS7mHaHglz8ig2AV7BywQDNgsf8rwoTvZjYucWMWI21iBQOPhhnrJE6ZZ2BLFAywZ2YYB-zZk8xkujzABYTAZHb720QCy3qQ__\",\"original\":true}]}}},\"podcastFields\":{\"post_id\":176425744,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"name\":null,\"created_at\":\"2025-10-17T16:02:24.631Z\",\"uploaded_at\":\"2025-10-17T16:02:24.594Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":176425744,\"user_id\":262158209,\"duration\":8719.281,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":139509059,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"248940bd-cafb-4de4-bf60-6b24fae0fa2e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"59851d57-8ed7-4904-9da0-a1c611dc620c\",\"created_at\":\"2025-10-17T16:06:35.002Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K7SERJ8AEM97B0P3D4BZ0GXW\",\"approved_at\":\"2025-10-17T16:15:41.370Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=FHVsNF9PIV1VN-tjNAjsl4dsgk5rc1SXnUbS5PZcVYlmkOFhlznNxYlYgQ43ZekS5UIu0DAlY9Dq9wnmdKDpyHlDFIr9595Krl1risOCUGzBtHrs2sdDFf-w5xtnJOwwl8X-R0lW8aRYUIffq8vSAc9jYLu6iXmNBVD2pRgHMUrlqj449R7WaBmbDFcLKx3CoOu8h2NIr7UEtuY4RG19GWtdoZ6RdtaUijN0pnkbuQwsNyDHPJ4S-V3hH4oAFDFCZlEcimtxhe3O0cRfq~8QZLSm5o4TTJza2HZ4hcybMR2JxaWAGSUYh76pRZFJWDcEMiIGy~kX14H22Sg8qgoesA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/unaligned_transcription.json?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=WiPDp-MMfScttlAjJ-E41Fi9kZxl7ozKUjo6dB0aoDv6Ce2nnLbiprCtIP1Xvvj5iUxwQhji06dYVUHC6JBdhcDkAGzCWk7NLuxIE2bmdAQrKJF6M8Cq-qb8X1gMvZbDYm1HQd-4Nf39CinyzwUOoQoaisGe2hcgcnv9LjvoGE~-yorw8n~h0UIl4xU8OcEVIc8gWEYpO0kxC4~-Of5pBaGeV0AwRPcxF7cZITDmy2gyZhctzXAcN5ro9tB8O7gyPCgrzBr03KxzGvfYbNXNKvs34AeS1nFAW6fF2qdcmlW4A2JOkfSgluP2gTVrXUYJeoKFZwF9A6vlD6Tcmiy5yQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/176425744/59851d57-8ed7-4904-9da0-a1c611dc620c/1760717214/en.vtt?Expires=1761964836&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hIEGBRT~Wivmj-J7hxBQqP3vVk8dc~AxD3li-ZuNrYrqr3NWuf~3X21JuQ~X-hPoiq3KCGPJ50~tcu8cDmTHx2qM~CFwOWM6s3yeL~1JfRFYtQl~~IcNhCUx7rrfd-G2uxWVZMNKTshFaWD0wA5SYgLNFmKsirU8lwAlSOeCL6JVAxqxOGqKYJYVZ8aoXm7GB8FYENtXf58RRpFOHNq7EfU9ffw~7nTFpLCzHcXVh5rttIYh3PifWAyS7mHaHglz8ig2AV7BywQDNgsf8rwoTvZjYucWMWI21iBQOPhhnrJE6ZZ2BLFAywZ2YYB-zZk8xkujzABYTAZHb720QCy3qQ__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"\\\"The problems are tractable, but they're still difficult\u201D\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":26939,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[247,204,6],\"population\":65},\"DarkVibrant\":{\"rgb\":[113,88,11],\"population\":57},\"LightVibrant\":{\"rgb\":[225,166,131],\"population\":8},\"Muted\":{\"rgb\":[179,113,89],\"population\":427},\"DarkMuted\":{\"rgb\":[83,73,44],\"population\":37},\"LightMuted\":{\"rgb\":[216,214,213],\"population\":76}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":224,\"comment_count\":15,\"child_comment_count\":13,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":175802370,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Nick Lane \u2013 Life as we know it is chemically inevitable\",\"social_title\":\"Nick Lane \u2013 Life as we know it is chemically inevitable\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"nick-lane\",\"post_date\":\"2025-10-10T15:29:40.052Z\",\"audience\":\"everyone\",\"podcast_duration\":4808.4375,\"video_upload_id\":\"c3eaa143-65d8-48aa-9bcb-992642d8cc70\",\"podcast_upload_id\":\"0225f2f5-ca4f-4d4a-8b1d-e0afc7595492\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/nick-lane\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":44},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Life is continuous with Earth\u2019s geochemistry\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/175802370/c3eaa143-65d8-48aa-9bcb-992642d8cc70/transcoded-1760110067.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/src\",\"videoUpload\":{\"id\":\"c3eaa143-65d8-48aa-9bcb-992642d8cc70\",\"name\":\"SUB Nick Lane.mp4\",\"created_at\":\"2025-10-10T13:57:38.607Z\",\"uploaded_at\":\"2025-10-10T13:59:47.706Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175802370,\"user_id\":262158209,\"duration\":4808.47,\"height\":1080,\"width\":1920,\"thumbnail_id\":1760110067,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":2211667025,\"is_mux\":true,\"mux_asset_id\":\"OK101S1Eh23EHkGdTNQvTrLpyjAuK02WfMSK9Y02AiF0202U\",\"mux_playback_id\":\"M2xlLfzhi5unyIezyesUFlqMJCr2tUT1jj3Z00Jn39Lo\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"extractedAudio\":{\"id\":\"0225f2f5-ca4f-4d4a-8b1d-e0afc7595492\",\"name\":null,\"created_at\":\"2025-10-10T13:59:47.737Z\",\"uploaded_at\":\"2025-10-10T13:59:47.706Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175802370,\"user_id\":262158209,\"duration\":4808.4375,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":76935565,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"c3eaa143-65d8-48aa-9bcb-992642d8cc70\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"0225f2f5-ca4f-4d4a-8b1d-e0afc7595492\",\"created_at\":\"2025-10-10T14:01:21.211Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K776T7FT8SRMWCFSA0W5X9EH\",\"approved_at\":\"2025-10-10T14:06:49.580Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/transcription.json?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=GpI7w2arruaRfu4vJ71v0p9Oxt1mhDVUvLTLX37~MlANcIPg3zzA9VDQDPjyoIbEvq0NI8bDZ9U0IJNdwilK6eBEgqm3LtywANareQU4kmkqALuey0VeIcsF-h1QSkPfeC7WZ3pMX9oWoP0qj-9ANG2BIpvn4lEdQgwQSNtyi68XhbYZ~xZ0VNoA62V0G4Z6TgPJuv55oLQB8~ZXB52KwPkzyzCyaPCt7MEPECv7FKe3bAFT1y0MDSiU4ThRrewiKQGhUab7wOu9111D6HtGiNVNPnpuKseA1o4~xpE~aeIalecqe8D9q3jAyvILTLMfp6DJqxp-CeUw9J6LpyIJzA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/unaligned_transcription.json?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=cS6MlPNSDdkuImY6RIqBtkl3oYUWnJtrwrUSn2EefxaxDDkqsVrVSJ6L9JKqiVx-TcBZmk4QhwIcCls1lYX6DwRiHTL9nKhwvc-FBSZHUDSMmHbzTUXVIpVLRRX3NUE3TjBBoqqqrrAI~DpDKXhRJ0He5k2buZ7kQgA1LFugJ8qCGdUDZdTM5D6cNu16XygzZjaW48eObYuTosE56Yg--u402s9mNKlh5tQ1aQFs4ZY6dRt4ftXeyOL4xsi63Rty~4DNbb7UcQoAAVzsiFGHGWDXqiVUMOg9Y5uxkwG6Ant73qyxXpUc9oW4tYTAbiQke0k55WhMh8VzNVW-REN5Hg__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/en.vtt?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=NnX3fAwyCnRcWl-yI8lglb2J-Z48ZisVq~o-th6LKjtObOi~7U8hz-XbJVyYNXvBxJ1XxymNo7vWQVcYHfzlo4Q6y8jBGmmpktv3t1AMZecCAMwzadk2sJk8a9GNmv2UpeWzazzETyt7o92chJNVOPI5rVXiBwZ8PuO7Zv8FgzaocBNkPuuTayHLXQdmCcp~K3rkpL4gl2Afltb7bPlgoLfWbTbizCxhUcgAg45b5-6LRS5ApPfouwFFoo~KepZ~n5op6p-5XNeR4fYoL63aGkkBR6gX9OBfESybPSPa3zhP90odZZjgZ0ffjTtkRy3MGd~LrxrFJCHWiGBj~uDqJg__\",\"original\":true}]}},\"transcription\":null},\"podcastFields\":{\"post_id\":175802370,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"0225f2f5-ca4f-4d4a-8b1d-e0afc7595492\",\"name\":null,\"created_at\":\"2025-10-10T13:59:47.737Z\",\"uploaded_at\":\"2025-10-10T13:59:47.706Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175802370,\"user_id\":262158209,\"duration\":4808.4375,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":76935565,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"c3eaa143-65d8-48aa-9bcb-992642d8cc70\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"0225f2f5-ca4f-4d4a-8b1d-e0afc7595492\",\"created_at\":\"2025-10-10T14:01:21.211Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K776T7FT8SRMWCFSA0W5X9EH\",\"approved_at\":\"2025-10-10T14:06:49.580Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/transcription.json?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=GpI7w2arruaRfu4vJ71v0p9Oxt1mhDVUvLTLX37~MlANcIPg3zzA9VDQDPjyoIbEvq0NI8bDZ9U0IJNdwilK6eBEgqm3LtywANareQU4kmkqALuey0VeIcsF-h1QSkPfeC7WZ3pMX9oWoP0qj-9ANG2BIpvn4lEdQgwQSNtyi68XhbYZ~xZ0VNoA62V0G4Z6TgPJuv55oLQB8~ZXB52KwPkzyzCyaPCt7MEPECv7FKe3bAFT1y0MDSiU4ThRrewiKQGhUab7wOu9111D6HtGiNVNPnpuKseA1o4~xpE~aeIalecqe8D9q3jAyvILTLMfp6DJqxp-CeUw9J6LpyIJzA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/unaligned_transcription.json?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=cS6MlPNSDdkuImY6RIqBtkl3oYUWnJtrwrUSn2EefxaxDDkqsVrVSJ6L9JKqiVx-TcBZmk4QhwIcCls1lYX6DwRiHTL9nKhwvc-FBSZHUDSMmHbzTUXVIpVLRRX3NUE3TjBBoqqqrrAI~DpDKXhRJ0He5k2buZ7kQgA1LFugJ8qCGdUDZdTM5D6cNu16XygzZjaW48eObYuTosE56Yg--u402s9mNKlh5tQ1aQFs4ZY6dRt4ftXeyOL4xsi63Rty~4DNbb7UcQoAAVzsiFGHGWDXqiVUMOg9Y5uxkwG6Ant73qyxXpUc9oW4tYTAbiQke0k55WhMh8VzNVW-REN5Hg__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/175802370/0225f2f5-ca4f-4d4a-8b1d-e0afc7595492/1760104895/en.vtt?Expires=1761630599&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=NnX3fAwyCnRcWl-yI8lglb2J-Z48ZisVq~o-th6LKjtObOi~7U8hz-XbJVyYNXvBxJ1XxymNo7vWQVcYHfzlo4Q6y8jBGmmpktv3t1AMZecCAMwzadk2sJk8a9GNmv2UpeWzazzETyt7o92chJNVOPI5rVXiBwZ8PuO7Zv8FgzaocBNkPuuTayHLXQdmCcp~K3rkpL4gl2Afltb7bPlgoLfWbTbizCxhUcgAg45b5-6LRS5ApPfouwFFoo~KepZ~n5op6p-5XNeR4fYoL63aGkkBR6gX9OBfESybPSPa3zhP90odZZjgZ0ffjTtkRy3MGd~LrxrFJCHWiGBj~uDqJg__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Life is continuous with Earth\u2019s geochemistry\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":13916,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[194,159,5],\"population\":107},\"DarkVibrant\":{\"rgb\":[118,96,9],\"population\":49},\"LightVibrant\":{\"rgb\":[237,213,106],\"population\":123},\"Muted\":{\"rgb\":[179,106,89],\"population\":447},\"DarkMuted\":{\"rgb\":[92,72,45],\"population\":105},\"LightMuted\":{\"rgb\":[195,161,150],\"population\":35}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":44,\"comment_count\":0,\"child_comment_count\":0,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":175283310,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Some thoughts on the Sutton interview\",\"social_title\":\"Some thoughts on the Sutton interview\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"thoughts-on-sutton\",\"post_date\":\"2025-10-04T17:45:00.586Z\",\"audience\":\"everyone\",\"podcast_duration\":699.40247,\"video_upload_id\":\"7e44846b-49c5-4241-86a4-c57622c6a7b8\",\"podcast_upload_id\":\"d53be007-c67e-4511-b0da-9245b9135430\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/thoughts-on-sutton\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":96},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"I have a much better understanding of Sutton\u2019s vision now\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/175283310/7e44846b-49c5-4241-86a4-c57622c6a7b8/transcoded-1759599175.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/d53be007-c67e-4511-b0da-9245b9135430/src\",\"videoUpload\":{\"id\":\"7e44846b-49c5-4241-86a4-c57622c6a7b8\",\"name\":\"TW Sutton Response.mp4\",\"created_at\":\"2025-10-04T16:31:39.947Z\",\"uploaded_at\":\"2025-10-04T16:31:54.929Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175283310,\"user_id\":262158209,\"duration\":699.4487,\"height\":1080,\"width\":1920,\"thumbnail_id\":1759599175,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":296589036,\"is_mux\":true,\"mux_asset_id\":\"tJ5fozy5uF01feZfUQIqp001BQFmJmGnaorYNhS7BGdfU\",\"mux_playback_id\":\"iq6Lqvlj00aG9rSfxkt6W24uB0075lbIGAStLSP01jkWS4\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"d53be007-c67e-4511-b0da-9245b9135430\",\"name\":null,\"created_at\":\"2025-10-04T16:31:54.963Z\",\"uploaded_at\":\"2025-10-04T16:31:54.929Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175283310,\"user_id\":262158209,\"duration\":699.40247,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":11191004,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"7e44846b-49c5-4241-86a4-c57622c6a7b8\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"d53be007-c67e-4511-b0da-9245b9135430\",\"created_at\":\"2025-10-04T16:32:15.887Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K6R128W7QNAQAP8WSYBEEPAX\",\"approved_at\":\"2025-10-04T16:33:06.565Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/transcription.json?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=Hr8KYgVHKVBsl~tmVcCJef3Td6iWpqOATGftHJ-Hl4JZD~H3UcVwScizKY7Zs~RDutM-1KbWbg8jN6~o3JXJGQvkJBJGKI64jpPQS9bIHyW04HJcH90RdEIvyS-hwzHVGb2Aa4ur8~5JuTmpz-NMrWzyPyC8O5TH~dV~HYM07GqpVzYu4gS4tL5yqR5Yc1JIAEaKkK~EDk-r9HNvKl69hNfbmB4jMADnj9vJPR1o6wLzej0fuIZtL~0mShZmXvicSzUUlxCs-0387G8eE69hu1Dq8THkav9-TZPqBPnW4NeIsdgDyaZHgOqYEHgt3qcbsuoZ1Hqgi0KwDF4a~PBS6g__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/unaligned_transcription.json?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hmBy0bYe0JUJQn9Um85wNc6NJUmakBVjzcf8SHECsTZ6cX2876lCUiAM0Ke7U0tZHV4x-XsToQ76hXaqVZI42y74LqC~WdLYN2BN-3E4ONR8HOlJH97KhlnTkjWvpYJaxZD4LbNhqvxadTBnhjDZRO7gwChZL4cQz4aUgEewSfx~D5910jzqM~-oRVkOxjY-t1Gu4W4ebo89ei6LIXyjGlpoxostpeMxSk4JLQ7moy6qBcEhGDpgGfwZAIT11T91eEb1yM6nm8~4wxcg0BMCeSCIpD9n4wjrG1nB6rG5Gij19QsJxnGiFNQOl041cF3NO2i6e~fk88lZVwX8V7ZcUA__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/en.vtt?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=URYSCPhvl9Hgom0PKBjbNaTmP3wdjDt2zIaN028eIPCcY5W2J45xZuf1KZNRWMTguh5UfntouSzPZG6NdBl20fL6nxi4Y5fpNirkKGrVqCpjl08oBmULsSH9T9JH6Lgb9YSBAve~1unAbyN~m0o1XyJu7trLWJKMkJ-3853qM49ZMLcs9qU0QUx3SIs3ksX9DT4sAmVcO2VoG4AFsJGEvX7CA021hdeCGpQ3UT0Gd5S1VQLj42Q~4saxyXDfhmXH-a-aKYFmdbZKCCMUWL68tZ2NQs~433BusmzTGyDGp0HtjLCIIthQs~jdJirctdKoRHCFm3IOIHPH8EiGidjilQ__\",\"original\":true}]}}},\"podcastFields\":{\"post_id\":175283310,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"d53be007-c67e-4511-b0da-9245b9135430\",\"name\":null,\"created_at\":\"2025-10-04T16:31:54.963Z\",\"uploaded_at\":\"2025-10-04T16:31:54.929Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":175283310,\"user_id\":262158209,\"duration\":699.40247,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":11191004,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"7e44846b-49c5-4241-86a4-c57622c6a7b8\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"d53be007-c67e-4511-b0da-9245b9135430\",\"created_at\":\"2025-10-04T16:32:15.887Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K6R128W7QNAQAP8WSYBEEPAX\",\"approved_at\":\"2025-10-04T16:33:06.565Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/transcription.json?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=Hr8KYgVHKVBsl~tmVcCJef3Td6iWpqOATGftHJ-Hl4JZD~H3UcVwScizKY7Zs~RDutM-1KbWbg8jN6~o3JXJGQvkJBJGKI64jpPQS9bIHyW04HJcH90RdEIvyS-hwzHVGb2Aa4ur8~5JuTmpz-NMrWzyPyC8O5TH~dV~HYM07GqpVzYu4gS4tL5yqR5Yc1JIAEaKkK~EDk-r9HNvKl69hNfbmB4jMADnj9vJPR1o6wLzej0fuIZtL~0mShZmXvicSzUUlxCs-0387G8eE69hu1Dq8THkav9-TZPqBPnW4NeIsdgDyaZHgOqYEHgt3qcbsuoZ1Hqgi0KwDF4a~PBS6g__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/unaligned_transcription.json?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=hmBy0bYe0JUJQn9Um85wNc6NJUmakBVjzcf8SHECsTZ6cX2876lCUiAM0Ke7U0tZHV4x-XsToQ76hXaqVZI42y74LqC~WdLYN2BN-3E4ONR8HOlJH97KhlnTkjWvpYJaxZD4LbNhqvxadTBnhjDZRO7gwChZL4cQz4aUgEewSfx~D5910jzqM~-oRVkOxjY-t1Gu4W4ebo89ei6LIXyjGlpoxostpeMxSk4JLQ7moy6qBcEhGDpgGfwZAIT11T91eEb1yM6nm8~4wxcg0BMCeSCIpD9n4wjrG1nB6rG5Gij19QsJxnGiFNQOl041cF3NO2i6e~fk88lZVwX8V7ZcUA__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/175283310/d53be007-c67e-4511-b0da-9245b9135430/1759595539/en.vtt?Expires=1761641293&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=URYSCPhvl9Hgom0PKBjbNaTmP3wdjDt2zIaN028eIPCcY5W2J45xZuf1KZNRWMTguh5UfntouSzPZG6NdBl20fL6nxi4Y5fpNirkKGrVqCpjl08oBmULsSH9T9JH6Lgb9YSBAve~1unAbyN~m0o1XyJu7trLWJKMkJ-3853qM49ZMLcs9qU0QUx3SIs3ksX9DT4sAmVcO2VoG4AFsJGEvX7CA021hdeCGpQ3UT0Gd5S1VQLj42Q~4saxyXDfhmXH-a-aKYFmdbZKCCMUWL68tZ2NQs~433BusmzTGyDGp0HtjLCIIthQs~jdJirctdKoRHCFm3IOIHPH8EiGidjilQ__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"I have a much better understanding of Sutton\u2019s vision now\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":1694,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[224,185,52],\"population\":514},\"DarkVibrant\":{\"rgb\":[76,60,4],\"population\":1},\"LightVibrant\":{\"rgb\":[237.4333333333333,215.33333333333331,139.9666666666667],\"population\":0},\"Muted\":{\"rgb\":[123,82,68],\"population\":686},\"DarkMuted\":{\"rgb\":[82,42,43],\"population\":696},\"LightMuted\":{\"rgb\":[153,152,152],\"population\":7}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":96,\"comment_count\":10,\"child_comment_count\":7,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":174609513,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Richard Sutton \u2013 Father of RL thinks LLMs are a dead end\",\"social_title\":\"Richard Sutton \u2013 Father of RL thinks LLMs are a dead end\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"richard-sutton\",\"post_date\":\"2025-09-26T14:48:59.697Z\",\"audience\":\"everyone\",\"podcast_duration\":3981.5315,\"video_upload_id\":\"bbf5d079-6be9-4aaf-82d2-57d8584f22d4\",\"podcast_upload_id\":\"61c2baeb-b5f5-49bb-9103-3dacf0d139f5\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/richard-sutton\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":103},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"LLMs aren\u2019t Bitter-Lesson-pilled\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/174609513/bbf5d079-6be9-4aaf-82d2-57d8584f22d4/transcoded-1758896358.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/src\",\"videoUpload\":{\"id\":\"bbf5d079-6be9-4aaf-82d2-57d8584f22d4\",\"name\":\"SUB3 Sutton.mp4\",\"created_at\":\"2025-09-26T14:09:13.455Z\",\"uploaded_at\":\"2025-09-26T14:12:59.395Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":174609513,\"user_id\":262158209,\"duration\":3981.561,\"height\":1080,\"width\":1920,\"thumbnail_id\":1758927553,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":2441057862,\"is_mux\":true,\"mux_asset_id\":\"yY7YcrT01kr52K02g8bhTbS88HFvzcFYT2H5aM61xSmE4\",\"mux_playback_id\":\"KBDRTF02c6zjSY3A8UkbbV018ttoKf39lyDO5RPQn9Kfc\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"extractedAudio\":{\"id\":\"61c2baeb-b5f5-49bb-9103-3dacf0d139f5\",\"name\":null,\"created_at\":\"2025-09-26T14:12:59.435Z\",\"uploaded_at\":\"2025-09-26T14:12:59.395Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":174609513,\"user_id\":262158209,\"duration\":3981.5315,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":63705067,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"bbf5d079-6be9-4aaf-82d2-57d8584f22d4\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"61c2baeb-b5f5-49bb-9103-3dacf0d139f5\",\"created_at\":\"2025-09-26T14:13:54.439Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K635Z5051TDNRXKMZ988XXWS\",\"approved_at\":\"2025-09-26T14:18:16.870Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/transcription.json?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=K3QdQMxzmbcz2ravYX85EmZMmAFykcSvi7jer50Lu3NgBgn7U34sS5jhqq57yFHsMRcJomAbPPgYF0cvq50O3-bbp5X2BmMID5Hq5QGkr0MVH0EqppE40DNrtXYp0ZZv-bp~9C4f0YVbGA1AYimmvY3M90vLhLaatW7~J6TFiX7CfFGPLtAPES37QQu8ARACm3lZ2Xi9hETPnIDQe~UNuoLrGFdhuKjY4kvBHsjKNAPGoOI-~sCY5CAH5s-C7cRN060k0Z6tk8IhfdPFTRB-BSspUqxrOw2z6X1V-DYOPR0m4mjMcK2RSLzkQcKakbwhouxjQqFOGbx7miVc1rWgIQ__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/unaligned_transcription.json?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=NKYFNwzklmJqFsroLttMSuln-IWU5XxIkaaxgNgR582GjjoauuaHjNPtJ4BSUna9JhsFQgUY4AUp5Cbgy1eVhUS8YNSu0urTR5IdA1DISWnw5ajy~44FoMK-M6ifMquJ5GLgoE3a7JZT6xtitySNBsjrL1V4pWKKJBJF6W~0Jd5Mkk~W1ShyW8F3zWZz5OXq3JQyzfA7UKzeCASqUO7BxYRr5Lpc7f-dZg4JSOYRDfGRIaAeRMUdXy3MmNXbm-Kiq2CmgnD3htSS7Zx-xO1bM~tNIAcEjW7-suTsmcpQWhY3S-71WkBFaNStoFjIWSHDZZQpWkS2wBcTwTOMHHjDLQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/en.vtt?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=GCRHP-NxVcyqVawpa3J-YCSLWZ6L10unm-cq6JmNSgV4jqvT4qOrqIkkLj41UstQzemjRvTQmxXgbJsb4o84N~-fRbsk0Mghq3PYKHe2nNLdHVci5Wx1Ls~DWscRQkAVmEOJl1qLnvbqWCSxrWI74jiuJanmY1k5xRcUbu9kuy9xMkU3V8S9NyjI6xtXs~lPfz9KYcqQe9n5wW5y8NQLFy74MlSr6CWUJsoPR-tpy2ep9VMsOMTKMwqVtjDFWKMqE9VG0~GHJOAwRO0WWHqF0IpJO52cqnZ6o~73uAVCPYzBVIrP5zGrR2HySYydT8vxPdgeMFjM2A9n0-RAma-0LQ__\",\"original\":true}]}},\"transcription\":null},\"podcastFields\":{\"post_id\":174609513,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"61c2baeb-b5f5-49bb-9103-3dacf0d139f5\",\"name\":null,\"created_at\":\"2025-09-26T14:12:59.435Z\",\"uploaded_at\":\"2025-09-26T14:12:59.395Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":174609513,\"user_id\":262158209,\"duration\":3981.5315,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":63705067,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"bbf5d079-6be9-4aaf-82d2-57d8584f22d4\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"61c2baeb-b5f5-49bb-9103-3dacf0d139f5\",\"created_at\":\"2025-09-26T14:13:54.439Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K635Z5051TDNRXKMZ988XXWS\",\"approved_at\":\"2025-09-26T14:18:16.870Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/transcription.json?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=K3QdQMxzmbcz2ravYX85EmZMmAFykcSvi7jer50Lu3NgBgn7U34sS5jhqq57yFHsMRcJomAbPPgYF0cvq50O3-bbp5X2BmMID5Hq5QGkr0MVH0EqppE40DNrtXYp0ZZv-bp~9C4f0YVbGA1AYimmvY3M90vLhLaatW7~J6TFiX7CfFGPLtAPES37QQu8ARACm3lZ2Xi9hETPnIDQe~UNuoLrGFdhuKjY4kvBHsjKNAPGoOI-~sCY5CAH5s-C7cRN060k0Z6tk8IhfdPFTRB-BSspUqxrOw2z6X1V-DYOPR0m4mjMcK2RSLzkQcKakbwhouxjQqFOGbx7miVc1rWgIQ__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/unaligned_transcription.json?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=NKYFNwzklmJqFsroLttMSuln-IWU5XxIkaaxgNgR582GjjoauuaHjNPtJ4BSUna9JhsFQgUY4AUp5Cbgy1eVhUS8YNSu0urTR5IdA1DISWnw5ajy~44FoMK-M6ifMquJ5GLgoE3a7JZT6xtitySNBsjrL1V4pWKKJBJF6W~0Jd5Mkk~W1ShyW8F3zWZz5OXq3JQyzfA7UKzeCASqUO7BxYRr5Lpc7f-dZg4JSOYRDfGRIaAeRMUdXy3MmNXbm-Kiq2CmgnD3htSS7Zx-xO1bM~tNIAcEjW7-suTsmcpQWhY3S-71WkBFaNStoFjIWSHDZZQpWkS2wBcTwTOMHHjDLQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/174609513/61c2baeb-b5f5-49bb-9103-3dacf0d139f5/1758896045/en.vtt?Expires=1761740431&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=GCRHP-NxVcyqVawpa3J-YCSLWZ6L10unm-cq6JmNSgV4jqvT4qOrqIkkLj41UstQzemjRvTQmxXgbJsb4o84N~-fRbsk0Mghq3PYKHe2nNLdHVci5Wx1Ls~DWscRQkAVmEOJl1qLnvbqWCSxrWI74jiuJanmY1k5xRcUbu9kuy9xMkU3V8S9NyjI6xtXs~lPfz9KYcqQe9n5wW5y8NQLFy74MlSr6CWUJsoPR-tpy2ep9VMsOMTKMwqVtjDFWKMqE9VG0~GHJOAwRO0WWHqF0IpJO52cqnZ6o~73uAVCPYzBVIrP5zGrR2HySYydT8vxPdgeMFjM2A9n0-RAma-0LQ__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Watch now (66 mins) | LLMs aren\u2019t Bitter-Lesson-pilled\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":10130,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[235,195,6],\"population\":190},\"DarkVibrant\":{\"rgb\":[120,84,18],\"population\":87},\"LightVibrant\":{\"rgb\":[233,175,154],\"population\":366},\"Muted\":{\"rgb\":[184,113,92],\"population\":353},\"DarkMuted\":{\"rgb\":[90,73,51],\"population\":58},\"LightMuted\":{\"rgb\":[238,232,229],\"population\":70}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":103,\"comment_count\":28,\"child_comment_count\":19,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":173427890,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Fully autonomous robots are much closer than you think \u2013 Sergey Levine\",\"social_title\":\"Fully autonomous robots are much closer than you think \u2013 Sergey Levine\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"sergey-levine\",\"post_date\":\"2025-09-12T15:03:02.106Z\",\"audience\":\"everyone\",\"podcast_duration\":5308.2124,\"video_upload_id\":\"bb23b2a7-219d-463e-9f5d-164721ef56d3\",\"podcast_upload_id\":\"e7bc5a49-0d4f-4bad-9981-e958d13dba1d\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/sergey-levine\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":38},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Plus, how VLA models work, self-improvement from deployment, and China\u2019s hardware advantage\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/173427890/bb23b2a7-219d-463e-9f5d-164721ef56d3/transcoded-1758898981.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/src\",\"videoUpload\":{\"id\":\"bb23b2a7-219d-463e-9f5d-164721ef56d3\",\"name\":\"SUB5 Sergey.mp4\",\"created_at\":\"2025-09-12T22:26:20.769Z\",\"uploaded_at\":\"2025-09-12T22:29:08.800Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":173427890,\"user_id\":262158209,\"duration\":5308.261,\"height\":1080,\"width\":1920,\"thumbnail_id\":1758898981,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":2172253771,\"is_mux\":true,\"mux_asset_id\":\"HQCiFTRqz01WMIsFaQJg92heVeDWZ102VqS47JTpzOblQ\",\"mux_playback_id\":\"FFrKGbvBxxleWFAtXypkN302s41h1SVCeWDkfd3CU01e8\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"e7bc5a49-0d4f-4bad-9981-e958d13dba1d\",\"name\":null,\"created_at\":\"2025-09-12T22:29:08.828Z\",\"uploaded_at\":\"2025-09-12T22:29:08.800Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":173427890,\"user_id\":262158209,\"duration\":5308.2124,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":84931960,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"bb23b2a7-219d-463e-9f5d-164721ef56d3\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"e7bc5a49-0d4f-4bad-9981-e958d13dba1d\",\"created_at\":\"2025-09-12T22:30:40.614Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K500TQ3N4RWKW6ARMDV9HAJG\",\"approved_at\":\"2025-09-12T22:36:56.766Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/transcription.json?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=LaV-zZODbYvK47aFjzAHnjybORvNV2KtRtPrAsfZj2TNgxRKqruXCbxOLUNH-MqO-92iB1Vk3IYZH062KRGnepfjtED7~~hhdLnzFW3Rmy4enVfNHg~iYzE0yzoLkbZPgOpUL7pDrDO2x7brRhn0yWn75rfLAcqbR4TCscl9X5lGh8QrrLcNBOl2XKsCxaFcoAHT5Rp~BfEHk7m9aIwgoWdW58r21bUBbogBK8Ssr3mrocYvGqW~E-K451fmFtFwwOI6M1ATHvfS8NfC-JjsqQsa8TFwuZQErKhzro4z6RbpjDBK5DTBGP2vGhHsExmiIHDPhN3o~mrquvvu4MDx8Q__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/unaligned_transcription.json?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=iOYEbCnGaAFg3Aeko-rOYl0PFmKshpeSn8u1RHyqZY55lkdeY1hFPtbcs9YtaJ~r6UKTVsezdaytRhUhAFUp379UV89DNx8lb1DgDsLZYBt~fr49MZq0GO3v2JhfMo3pmh7IXZUfZbotYB4MLWF0odgl9VcbcjJPC7QS6qm5yizsQmTRK4NXHu6UTJNZDHYrYSC7mP9ITOt2--b2n-24-x6Cbo950k74YHUz1C5xJTdvjN-~alYbD~lmp2Ik0S-mXQnpP9U-VVf1G9m0Y8uNUqN7xS1XzFTMHo24YEtYiHztp1s~y5c8~vhwu2y68KnZlPU1jF-Mh9hFPRsASvL3wg__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/en.vtt?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=bGWhQTQ4Zcrtrsk3JQM5TadgcbFxXtxoRKKLVKM2GbV5jZOgkXqjqsidRoxOcefZb6hr0fzjpOl0MLu1XQ5k0WqmgYvaGkWymZhGAG4Mr1e8EMPj4VtlOwI5npTT7N5a-whJZ8MrlDPvH28ojWnNzG19xEg~P6Z4ifSZ9XuJGABx3zSKRCCumOykhsw3DPT4JqiLaB-3Dpf6ngOTbyZ-FSPJXc7NlSV55flFh~d-U6Xi3O2BLCEYMXFIWSTf8BsfeK4JWA3-EZMBsqWzJZAxsOaGha-PWG5RS7ftfSp1RB1sKfRF~rEibxZaQsF0DujntZZ4efYw6ttstXZgZuJsIw__\",\"original\":true}]}}},\"podcastFields\":{\"post_id\":173427890,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"e7bc5a49-0d4f-4bad-9981-e958d13dba1d\",\"name\":null,\"created_at\":\"2025-09-12T22:29:08.828Z\",\"uploaded_at\":\"2025-09-12T22:29:08.800Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":173427890,\"user_id\":262158209,\"duration\":5308.2124,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":84931960,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"bb23b2a7-219d-463e-9f5d-164721ef56d3\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"e7bc5a49-0d4f-4bad-9981-e958d13dba1d\",\"created_at\":\"2025-09-12T22:30:40.614Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K500TQ3N4RWKW6ARMDV9HAJG\",\"approved_at\":\"2025-09-12T22:36:56.766Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/transcription.json?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=LaV-zZODbYvK47aFjzAHnjybORvNV2KtRtPrAsfZj2TNgxRKqruXCbxOLUNH-MqO-92iB1Vk3IYZH062KRGnepfjtED7~~hhdLnzFW3Rmy4enVfNHg~iYzE0yzoLkbZPgOpUL7pDrDO2x7brRhn0yWn75rfLAcqbR4TCscl9X5lGh8QrrLcNBOl2XKsCxaFcoAHT5Rp~BfEHk7m9aIwgoWdW58r21bUBbogBK8Ssr3mrocYvGqW~E-K451fmFtFwwOI6M1ATHvfS8NfC-JjsqQsa8TFwuZQErKhzro4z6RbpjDBK5DTBGP2vGhHsExmiIHDPhN3o~mrquvvu4MDx8Q__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/unaligned_transcription.json?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=iOYEbCnGaAFg3Aeko-rOYl0PFmKshpeSn8u1RHyqZY55lkdeY1hFPtbcs9YtaJ~r6UKTVsezdaytRhUhAFUp379UV89DNx8lb1DgDsLZYBt~fr49MZq0GO3v2JhfMo3pmh7IXZUfZbotYB4MLWF0odgl9VcbcjJPC7QS6qm5yizsQmTRK4NXHu6UTJNZDHYrYSC7mP9ITOt2--b2n-24-x6Cbo950k74YHUz1C5xJTdvjN-~alYbD~lmp2Ik0S-mXQnpP9U-VVf1G9m0Y8uNUqN7xS1XzFTMHo24YEtYiHztp1s~y5c8~vhwu2y68KnZlPU1jF-Mh9hFPRsASvL3wg__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/173427890/e7bc5a49-0d4f-4bad-9981-e958d13dba1d/1757716259/en.vtt?Expires=1761630420&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=bGWhQTQ4Zcrtrsk3JQM5TadgcbFxXtxoRKKLVKM2GbV5jZOgkXqjqsidRoxOcefZb6hr0fzjpOl0MLu1XQ5k0WqmgYvaGkWymZhGAG4Mr1e8EMPj4VtlOwI5npTT7N5a-whJZ8MrlDPvH28ojWnNzG19xEg~P6Z4ifSZ9XuJGABx3zSKRCCumOykhsw3DPT4JqiLaB-3Dpf6ngOTbyZ-FSPJXc7NlSV55flFh~d-U6Xi3O2BLCEYMXFIWSTf8BsfeK4JWA3-EZMBsqWzJZAxsOaGha-PWG5RS7ftfSp1RB1sKfRF~rEibxZaQsF0DujntZZ4efYw6ttstXZgZuJsIw__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Plus, how VLA models work, self-improvement from deployment, and China\u2019s hardware advantage\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":15321,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[251,209,4],\"population\":189},\"DarkVibrant\":{\"rgb\":[147,6,21],\"population\":31},\"LightVibrant\":{\"rgb\":[245,163,130],\"population\":59},\"Muted\":{\"rgb\":[174,129,110],\"population\":61},\"DarkMuted\":{\"rgb\":[84,71,41],\"population\":133},\"LightMuted\":{\"rgb\":[227,219,216],\"population\":103}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":38,\"comment_count\":6,\"child_comment_count\":6,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":172873013,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"How Hitler almost starved Britain \u2013 Sarah Paine\",\"social_title\":\"How Hitler almost starved Britain \u2013 Sarah Paine\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"sarah-paine-ww2\",\"post_date\":\"2025-09-05T15:05:14.631Z\",\"audience\":\"everyone\",\"podcast_duration\":5717.1333,\"video_upload_id\":\"0798040c-0395-47a1-a1c1-7b97cda60b1e\",\"podcast_upload_id\":\"1d505e7a-2541-4857-8269-f7fef1d61abc\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/sarah-paine-ww2\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":35},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\u201CThe only thing that really frightened me during the war was the U-boat peril\u201D \u2013 Winston Churchill\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/172873013/0798040c-0395-47a1-a1c1-7b97cda60b1e/transcoded-1757084500.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/1d505e7a-2541-4857-8269-f7fef1d61abc/src\",\"videoUpload\":{\"id\":\"0798040c-0395-47a1-a1c1-7b97cda60b1e\",\"name\":\"SUB Sarah Paine WW2.mp4\",\"created_at\":\"2025-09-05T13:18:37.624Z\",\"uploaded_at\":\"2025-09-05T13:19:29.165Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":172873013,\"user_id\":262158209,\"duration\":5717.1665,\"height\":1080,\"width\":1920,\"thumbnail_id\":1757084500,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":2282995283,\"is_mux\":true,\"mux_asset_id\":\"IFPm01ND418ugkVNeVYUmI9qYcxNAbbqk02Z7ncnj00uQo\",\"mux_playback_id\":\"W02Upzdy8Z3g3NBzZIoSs8HSN73iUWPRaoYcHAJf111U\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"1d505e7a-2541-4857-8269-f7fef1d61abc\",\"name\":null,\"created_at\":\"2025-09-05T13:19:29.185Z\",\"uploaded_at\":\"2025-09-05T13:19:29.165Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":172873013,\"user_id\":262158209,\"duration\":5717.1333,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":91474694,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"0798040c-0395-47a1-a1c1-7b97cda60b1e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"1d505e7a-2541-4857-8269-f7fef1d61abc\",\"created_at\":\"2025-09-05T13:21:35.573Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K4D0M8MDK0Z632GZKKCC7TTF\",\"approved_at\":\"2025-09-05T13:28:00.155Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/transcription.json?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=Kia1L5tUSfd-B84ktSzKclWB80Gpdg8AkCbd9PU9KlqqXz6eYsA8EOJJ9r~oKUDRbG3pjyznZMGnHTh1AtIWWUdsUDcrzPUcLIdZtcRdGNnd-XPEwA8Ji-Y1UHja5ssiuD6R3Uujewua2V4qLepH1NKuho6pL-5mG7dEE7Qj6zHeBrMWbw0kLktzrTjS1t1UXzZZ8c7nNFiu~4P3H1ife4CR13j2Uz3fMMrFfwr5vP4M0mTtYUzFFAdlSf9dzGHSoTmyoFEsBNwhsabsp0URvhbB82zcxrdtQw836DdAWMKKPZHkplhQ4Jc5oDuuwivBSOxC54v7l2QJx5F-0MC0LQ__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/unaligned_transcription.json?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=gsAhWC5fAZtmMDY3qHPx4NILB6lPkuYM3p6ubGrcwkcb0QZQ1DHo0yggWS0aoqM-s2cOBixOk8NNoAXydnEIhmFP2gNWychOqoL5D63AnS1w5zeLwNTVs8ssyn1kWqOCEfeT0c3-uFLZqk60MOzdHK5IU3sTL2Sn1Laa0OpR0rZs-iigpR-1GqdR9eBqOCcTlU~LUhNooIywEP~7S0X99Frf-0RVkeywb5BFliHiqgXlFtISnL0p9zt-WaXFxBFQEpYjgBJxMm2IXsJAVfxIsEffHsHnghSu8V-erRvM8kLB0Mo8MTlcKVgUNEU3Km~AvnOwpDDCLeYSTnmREwmkPQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/en.vtt?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=KX6J1ry93Bv6A32nu~64WoPWoe5sw3ONaRo22uLU85OQC1Sj50gK1liTCpuyqWQFGwI1fE4xt6IUgN2EvfrK9RpphMbbvXZNeesiJF82pVcupe5lRdPueExj4nNnOmRKhyQKOZIuR9OUcE6wbYy91lpD0iKA0DxVwq20hX7daku4ulo9Z5TgGAT97x60kSiwv93aN1YTvUGnoCNwKqV~fqe-5uxszXrcEgz4Y6EAwU3T5Ogs0ShxE4d4MT81o7HQdvwENtHL5Eo8ZlKh4vumVqsTbKkw0W~bkx~6GeZ-5Q89T-rJDKyLCvXd10x-7BIl-3EUqX9Piwpi5pMyBMEvVQ__\",\"original\":true}]}}},\"podcastFields\":{\"post_id\":172873013,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"1d505e7a-2541-4857-8269-f7fef1d61abc\",\"name\":null,\"created_at\":\"2025-09-05T13:19:29.185Z\",\"uploaded_at\":\"2025-09-05T13:19:29.165Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":172873013,\"user_id\":262158209,\"duration\":5717.1333,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":91474694,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"0798040c-0395-47a1-a1c1-7b97cda60b1e\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"1d505e7a-2541-4857-8269-f7fef1d61abc\",\"created_at\":\"2025-09-05T13:21:35.573Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K4D0M8MDK0Z632GZKKCC7TTF\",\"approved_at\":\"2025-09-05T13:28:00.155Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/transcription.json?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=Kia1L5tUSfd-B84ktSzKclWB80Gpdg8AkCbd9PU9KlqqXz6eYsA8EOJJ9r~oKUDRbG3pjyznZMGnHTh1AtIWWUdsUDcrzPUcLIdZtcRdGNnd-XPEwA8Ji-Y1UHja5ssiuD6R3Uujewua2V4qLepH1NKuho6pL-5mG7dEE7Qj6zHeBrMWbw0kLktzrTjS1t1UXzZZ8c7nNFiu~4P3H1ife4CR13j2Uz3fMMrFfwr5vP4M0mTtYUzFFAdlSf9dzGHSoTmyoFEsBNwhsabsp0URvhbB82zcxrdtQw836DdAWMKKPZHkplhQ4Jc5oDuuwivBSOxC54v7l2QJx5F-0MC0LQ__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/unaligned_transcription.json?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=gsAhWC5fAZtmMDY3qHPx4NILB6lPkuYM3p6ubGrcwkcb0QZQ1DHo0yggWS0aoqM-s2cOBixOk8NNoAXydnEIhmFP2gNWychOqoL5D63AnS1w5zeLwNTVs8ssyn1kWqOCEfeT0c3-uFLZqk60MOzdHK5IU3sTL2Sn1Laa0OpR0rZs-iigpR-1GqdR9eBqOCcTlU~LUhNooIywEP~7S0X99Frf-0RVkeywb5BFliHiqgXlFtISnL0p9zt-WaXFxBFQEpYjgBJxMm2IXsJAVfxIsEffHsHnghSu8V-erRvM8kLB0Mo8MTlcKVgUNEU3Km~AvnOwpDDCLeYSTnmREwmkPQ__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/172873013/1d505e7a-2541-4857-8269-f7fef1d61abc/1757078514/en.vtt?Expires=1761630067&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=KX6J1ry93Bv6A32nu~64WoPWoe5sw3ONaRo22uLU85OQC1Sj50gK1liTCpuyqWQFGwI1fE4xt6IUgN2EvfrK9RpphMbbvXZNeesiJF82pVcupe5lRdPueExj4nNnOmRKhyQKOZIuR9OUcE6wbYy91lpD0iKA0DxVwq20hX7daku4ulo9Z5TgGAT97x60kSiwv93aN1YTvUGnoCNwKqV~fqe-5uxszXrcEgz4Y6EAwU3T5Ogs0ShxE4d4MT81o7HQdvwENtHL5Eo8ZlKh4vumVqsTbKkw0W~bkx~6GeZ-5Q89T-rJDKyLCvXd10x-7BIl-3EUqX9Piwpi5pMyBMEvVQ__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"\u201CThe only thing that really frightened me during the war was the U-boat peril\u201D \u2013 Winston Churchill\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":15150,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[239,201,8],\"population\":197},\"DarkVibrant\":{\"rgb\":[16,51,124],\"population\":321},\"LightVibrant\":{\"rgb\":[237,182,145],\"population\":366},\"Muted\":{\"rgb\":[129,145,188],\"population\":3},\"DarkMuted\":{\"rgb\":[44,60,92],\"population\":1},\"LightMuted\":{\"rgb\":[132,156,188],\"population\":1}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":35,\"comment_count\":0,\"child_comment_count\":0,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":171561010,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"Evolution designed us to die fast; we can change that \u2014 Jacob Kimmel\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"jacob-kimmel\",\"post_date\":\"2025-08-21T16:29:18.613Z\",\"audience\":\"everyone\",\"podcast_duration\":6279.654,\"video_upload_id\":\"6b6c7dda-3309-4f82-a4e4-788e85444986\",\"podcast_upload_id\":\"178b1921-de5d-49b6-871e-1a10739871b5\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/jacob-kimmel\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":35},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Is the answer already in our genome?\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/171561010/6b6c7dda-3309-4f82-a4e4-788e85444986/transcoded-1755793692.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/178b1921-de5d-49b6-871e-1a10739871b5/src\",\"videoUpload\":{\"id\":\"6b6c7dda-3309-4f82-a4e4-788e85444986\",\"name\":\"SUB3_Kimmel.mp4\",\"created_at\":\"2025-08-21T16:19:19.767Z\",\"uploaded_at\":\"2025-08-21T16:20:22.021Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171561010,\"user_id\":262158209,\"duration\":6279.69,\"height\":1080,\"width\":1920,\"thumbnail_id\":1755793692,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":2809786573,\"is_mux\":true,\"mux_asset_id\":\"D900Q7qP8qFsNsIgeNiYETi00Zf01SnGlEJOesxF4EwSRo\",\"mux_playback_id\":\"eGpZqfMxUz00Zz5N3qZFv01PNJ302OrBdzt6b8le02xTEWg\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"transcription\":null,\"extractedAudio\":{\"id\":\"178b1921-de5d-49b6-871e-1a10739871b5\",\"name\":null,\"created_at\":\"2025-08-21T16:20:22.042Z\",\"uploaded_at\":\"2025-08-21T16:20:22.021Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171561010,\"user_id\":262158209,\"duration\":6279.654,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":100475027,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"6b6c7dda-3309-4f82-a4e4-788e85444986\",\"live_stream_id\":null,\"transcription\":null}},\"podcastFields\":{\"post_id\":171561010,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"178b1921-de5d-49b6-871e-1a10739871b5\",\"name\":null,\"created_at\":\"2025-08-21T16:20:22.042Z\",\"uploaded_at\":\"2025-08-21T16:20:22.021Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171561010,\"user_id\":262158209,\"duration\":6279.654,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":100475027,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"6b6c7dda-3309-4f82-a4e4-788e85444986\",\"live_stream_id\":null,\"transcription\":null},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Is the answer already in our genome?\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":21476,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[209,172,5],\"population\":168},\"DarkVibrant\":{\"rgb\":[100,62,12],\"population\":16},\"LightVibrant\":{\"rgb\":[232,153,133],\"population\":64},\"Muted\":{\"rgb\":[187,118,99],\"population\":413},\"DarkMuted\":{\"rgb\":[81,72,41],\"population\":66},\"LightMuted\":{\"rgb\":[207,169,158],\"population\":85}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":35,\"comment_count\":0,\"child_comment_count\":0,\"is_geoblocked\":false,\"hasCashtag\":false},{\"id\":171017457,\"editor_v2\":false,\"publication_id\":69345,\"title\":\"China is killing the US on energy. Does that mean they\u2019ll win AGI? \u2014 Casey Handmer\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"podcast\",\"slug\":\"casey-handmer\",\"post_date\":\"2025-08-15T14:54:54.243Z\",\"audience\":\"everyone\",\"podcast_duration\":4101.5117,\"video_upload_id\":\"8f59424c-5a86-421d-8d62-fc66c40ff4e8\",\"podcast_upload_id\":\"ac26befa-eb00-4553-9f75-ce494ac20a3d\",\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://www.dwarkesh.com/p/casey-handmer\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":55},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Plus, how solar fits into the next 10 years\",\"cover_image\":\"https://substack-video.s3.amazonaws.com/video_upload/post/171017457/8f59424c-5a86-421d-8d62-fc66c40ff4e8/transcoded-1755268166.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_episode_image_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"podcast_episode_image_info\":{\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f946699e-ce46-482d-9cfb-18114cbe94f9_2500x2500.png\",\"isDefaultArt\":false,\"isDefault\":false},\"podcast_url\":\"https://api.substack.com/api/v1/audio/upload/ac26befa-eb00-4553-9f75-ce494ac20a3d/src\",\"videoUpload\":{\"id\":\"8f59424c-5a86-421d-8d62-fc66c40ff4e8\",\"name\":\"SUB Casey Handmer.mp4\",\"created_at\":\"2025-08-15T11:55:16.711Z\",\"uploaded_at\":\"2025-08-15T11:57:25.978Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171017457,\"user_id\":262158209,\"duration\":4101.5557,\"height\":1080,\"width\":1920,\"thumbnail_id\":1755268166,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"video\",\"primary_file_size\":1653885702,\"is_mux\":true,\"mux_asset_id\":\"f3dU4y68q22fAYdhhTl8WRrTZHCIejfMlHoT1NJrjwU\",\"mux_playback_id\":\"BVJBf4d3bjyjKv00PHnvVziJgTeiVDgUlykU9mVMxhro\",\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":\"high\",\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":null,\"live_stream_id\":null,\"extractedAudio\":{\"id\":\"ac26befa-eb00-4553-9f75-ce494ac20a3d\",\"name\":null,\"created_at\":\"2025-08-15T11:57:26.000Z\",\"uploaded_at\":\"2025-08-15T11:57:25.978Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171017457,\"user_id\":262158209,\"duration\":4101.5117,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":65624754,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"8f59424c-5a86-421d-8d62-fc66c40ff4e8\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"ac26befa-eb00-4553-9f75-ce494ac20a3d\",\"created_at\":\"2025-08-15T11:59:01.357Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K2PSHZHYPQ5GBNW1CW3Y7YY7\",\"approved_at\":\"2025-08-15T12:03:45.579Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/transcription.json?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=RMOBRbpT05bMoh2~JZDkAI14iGTweGRB5VMeZFhDhkk11xJGEyHnkwwuQIH4kA36vR2y9iJNEP8Y1LRrxizFav2psj-Y8EwgsqLprBHz-~vxSSywH~xeSL-qcZ74w~d19lJd5xofCDkzauj6bd65wGyE758pYILFIVM4mgAWH9sy3EWD6bEAjO0C5aY0EgIhX6tC4P-6PP8Lp~KY0JSJe-zYum1FhLktAdnLfkYD-jHsb-gXqa-CLacwYjTGEF7D1V19uts4H7a9k18-43xvBObY~EirQBQzm5nrZbJpDpHtsU-HsoFVkLD2RjXMBAwIXuOMOTMErz9FY2rTUSrsSA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/unaligned_transcription.json?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=UQumXSwqwsAg8hitFruFWAuWDyRjAfftdIsoj8ZYeWu2WjMeF0tl6HzAW-aYoKyu-QOOldcdSHeLhi02gw9eelTl0gbVMY1MqEpff94DbWm--eAOQRETfjsmWO0hCImNuQnuPydbL6h7p83etOhQsJOKvBQyYYPT4Mg22I1RtCBMjsBX7RqWXoHHR~26JzbOCzr7-nkQaD5xbNmzQdtpt1yxZCXmiPkXsIFJ~7hBP0YasqEpmlq44DieRU7BFF6-uvHdACkl3f1NBEyr8z0Jja6hyqEDJ5Tbg~USuARBsy-b-NVBFbVRciXJv3XDnzO48rsyOIXNlYK1A8EpqNFqVw__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/en.vtt?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=c0rjMXCw-TU4SSSpDfJUTgA23Nu65dqWUGaXnKmSxwsBkFPbbJIecZ~e4wZuy6fVKK7qSihEPhinuTMUN-BZe-wjPj7DYqsoKl-AOSeaz5vfnSgEFjXyVIGdTnCCSq62dvSqS0pz1ZKYzM0EH1aajBwmdkzmExb24NPjpqz7sSrw-VNxw8PR5S~qHdpsKLE7U6PUp~8cswPJ7GWwED0Ik1jSoqdRzPUXreDdOex8J8xYDeUWgvjIW6E9afRvSA0P9qL9hrrBeOGRI4u9YLM9J49YIW4VeMoRKtx4m~CXv15F9r~HoMonRTYWRYW9pFJaOh9CYfYU~R~c~pTcn6333A__\",\"original\":true}]}},\"transcription\":null},\"podcastFields\":{\"post_id\":171017457,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":{\"id\":\"ac26befa-eb00-4553-9f75-ce494ac20a3d\",\"name\":null,\"created_at\":\"2025-08-15T11:57:26.000Z\",\"uploaded_at\":\"2025-08-15T11:57:25.978Z\",\"publication_id\":69345,\"state\":\"transcoded\",\"post_id\":171017457,\"user_id\":262158209,\"duration\":4101.5117,\"height\":null,\"width\":null,\"thumbnail_id\":1,\"preview_start\":null,\"preview_duration\":null,\"media_type\":\"audio\",\"primary_file_size\":65624754,\"is_mux\":null,\"mux_asset_id\":null,\"mux_playback_id\":null,\"mux_preview_asset_id\":null,\"mux_preview_playback_id\":null,\"mux_rendition_quality\":null,\"mux_preview_rendition_quality\":null,\"explicit\":false,\"copyright_infringement\":null,\"src_media_upload_id\":\"8f59424c-5a86-421d-8d62-fc66c40ff4e8\",\"live_stream_id\":null,\"transcription\":{\"media_upload_id\":\"ac26befa-eb00-4553-9f75-ce494ac20a3d\",\"created_at\":\"2025-08-15T11:59:01.357Z\",\"requested_by\":262158209,\"status\":\"transcribed\",\"modal_call_id\":\"fc-01K2PSHZHYPQ5GBNW1CW3Y7YY7\",\"approved_at\":\"2025-08-15T12:03:45.579Z\",\"transcript_url\":\"s3://substack-video/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/transcription.json\",\"attention_vocab\":null,\"speaker_map\":null,\"captions_map\":{\"en\":{\"url\":\"s3://substack-video/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/en.vtt\",\"language\":\"en\",\"original\":true}},\"cdn_url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/transcription.json?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=RMOBRbpT05bMoh2~JZDkAI14iGTweGRB5VMeZFhDhkk11xJGEyHnkwwuQIH4kA36vR2y9iJNEP8Y1LRrxizFav2psj-Y8EwgsqLprBHz-~vxSSywH~xeSL-qcZ74w~d19lJd5xofCDkzauj6bd65wGyE758pYILFIVM4mgAWH9sy3EWD6bEAjO0C5aY0EgIhX6tC4P-6PP8Lp~KY0JSJe-zYum1FhLktAdnLfkYD-jHsb-gXqa-CLacwYjTGEF7D1V19uts4H7a9k18-43xvBObY~EirQBQzm5nrZbJpDpHtsU-HsoFVkLD2RjXMBAwIXuOMOTMErz9FY2rTUSrsSA__\",\"cdn_unaligned_url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/unaligned_transcription.json?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=UQumXSwqwsAg8hitFruFWAuWDyRjAfftdIsoj8ZYeWu2WjMeF0tl6HzAW-aYoKyu-QOOldcdSHeLhi02gw9eelTl0gbVMY1MqEpff94DbWm--eAOQRETfjsmWO0hCImNuQnuPydbL6h7p83etOhQsJOKvBQyYYPT4Mg22I1RtCBMjsBX7RqWXoHHR~26JzbOCzr7-nkQaD5xbNmzQdtpt1yxZCXmiPkXsIFJ~7hBP0YasqEpmlq44DieRU7BFF6-uvHdACkl3f1NBEyr8z0Jja6hyqEDJ5Tbg~USuARBsy-b-NVBFbVRciXJv3XDnzO48rsyOIXNlYK1A8EpqNFqVw__\",\"signed_captions\":[{\"language\":\"en\",\"url\":\"https://substackcdn.com/video_upload/post/171017457/ac26befa-eb00-4553-9f75-ce494ac20a3d/1755259151/en.vtt?Expires=1761461136&Key-Pair-Id=APKAIVDA3NPSMPSPESQQ&Signature=c0rjMXCw-TU4SSSpDfJUTgA23Nu65dqWUGaXnKmSxwsBkFPbbJIecZ~e4wZuy6fVKK7qSihEPhinuTMUN-BZe-wjPj7DYqsoKl-AOSeaz5vfnSgEFjXyVIGdTnCCSq62dvSqS0pz1ZKYzM0EH1aajBwmdkzmExb24NPjpqz7sSrw-VNxw8PR5S~qHdpsKLE7U6PUp~8cswPJ7GWwED0Ik1jSoqdRzPUXreDdOex8J8xYDeUWgvjIW6E9afRvSA0P9qL9hrrBeOGRI4u9YLM9J49YIW4VeMoRKtx4m~CXv15F9r~HoMonRTYWRYW9pFJaOh9CYfYU~R~c~pTcn6333A__\",\"original\":true}]}},\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Plus, how solar fits into the next 10 years\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":null,\"wordcount\":13737,\"postTags\":[{\"id\":\"98ee3140-8529-4d25-b653-0679afbe4dd4\",\"publication_id\":69345,\"name\":\"Podcast\",\"slug\":\"podcast\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[151,85,60],\"population\":7},\"DarkVibrant\":{\"rgb\":[68,52,4],\"population\":1},\"LightVibrant\":{\"rgb\":[227,159,137],\"population\":138},\"Muted\":{\"rgb\":[176,116,95],\"population\":461},\"DarkMuted\":{\"rgb\":[82,57,47],\"population\":57},\"LightMuted\":{\"rgb\":[218,201,190],\"population\":245}},\"publishedBylines\":[{\"id\":4281466,\"name\":\"Dwarkesh Patel\",\"handle\":\"dwarkesh\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb715ffd1-f7d7-4755-af88-c48efe647f5b_400x400.jpeg\",\"bio\":\"Host of Dwarkesh Podcast\",\"profile_set_up_at\":\"2021-06-09T22:58:10.864Z\",\"reader_installed_at\":\"2022-04-03T20:37:19.142Z\",\"publicationUsers\":[{\"id\":246192,\"user_id\":4281466,\"publication_id\":69345,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":69345,\"name\":\"Dwarkesh Podcast\",\"subdomain\":\"dwarkesh\",\"custom_domain\":\"www.dwarkesh.com\",\"custom_domain_optional\":false,\"hero_text\":\"Deeply researched interviews\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/90fa9666-5b8b-4685-a8fb-4b64cb7e0333_1080x1080.png\",\"author_id\":4281466,\"primary_user_id\":4281466,\"theme_var_background_pop\":\"#D10000\",\"created_at\":\"2020-07-18T16:36:25.723Z\",\"email_from_name\":\"Dwarkesh Patel\",\"copyright\":\"Dwarkesh Patel\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":null,\"is_personal_mode\":false}}],\"twitter_screen_name\":\"dwarkesh_sp\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":5,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100},\"paidPublicationIds\":[89120,1163860,1271258,104058,3087928,2118966,1501429,94899],\"subscriber\":null}}],\"reaction\":null,\"reaction_count\":55,\"comment_count\":8,\"child_comment_count\":7,\"is_geoblocked\":false,\"hasCashtag\":false}],\"trackFrontendVisit\":true,\"activeLiveStream\":null,\"freeTrialCoupon\":null,\"isChatActive\":false,\"isMeetingsActive\":false,\"hasViralGiftsCount\":0,\"features\":{},\"browser\":{\"name\":\"Chrome\",\"version\":\"141.0.0.0\",\"major\":\"141\"},\"showCookieBanner\":true,\"disabledCookies\":[\"intro_popup_last_hidden_at\",\"muxData-substack\",\"like_upsell_last_shown_at\",\"chatbot_terms_last_accepted_at\",\"preferred_language\",\"visit_id\",\"ajs_anonymous_id\",\"ab_testing_id\",\"ab_experiment_sampled\",\"_ga_tracking-substack\",\"ad_quick_tracking_pixel-substack\",\"meta_tracking_pixel-substack\",\"_dd_s-substack\",\"fs_uid-substack\",\"__zlcmid-substack\",\"disable_html_pixels\",\"_ga_tracking-publisher\",\"_ga_tag_manager-publisher\",\"fb_pixel-publisher\",\"twitter_pixel-publisher\",\"parsely_pixel-publisher\"],\"dd_env\":\"prod\",\"dd_ti\":false}")</script>
        <script>window._analyticsConfig = JSON.parse("{\"properties\":{\"subdomain\":\"dwarkesh\",\"publication_id\":69345,\"has_plans\":true,\"pub_community_enabled\":true,\"is_personal_publication\":false,\"is_subscribed\":false,\"is_free_subscribed\":false,\"is_author\":false,\"is_contributor\":false,\"is_admin\":false,\"is_founding\":false},\"adwordsAccountId\":\"AW-316245675\",\"adwordsEventSendTo\":\"Tf76CKqcyL4DEKuN5pYB\"}")</script>

        
        
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3013.109ef439.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7495.9abec3c8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8343.97738136.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6348.64858477.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8117.8222379f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/main.d69f3bbc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5761.b8188847.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4763.d85e3a43.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8477.f1c13451.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6635.3a65999c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2878.02a48890.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5502.691068f4.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7407.fa64eded.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8834.edca0d70.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4490.9edb5510.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2816.a4d45b91.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2560.d52071db.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9302.f9d278b8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/948.ab4d34e3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8268.b9d95bf8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6722.e946e60d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3572.a4b83f7d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1933.ae003e6c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7065.18c0dbcf.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1473.387c4879.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6042.8ebed607.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7648.57104179.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5759.13d040a1.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4405.88ee6abc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1549.cfa3b4c6.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5441.d320515b.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6053.cd489296.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6924.c04f272b.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8917.6b8e89cc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4721.d9ff61c9.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8992.a8da08ee.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8455.a96d2740.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/838.606783de.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6333.ff037e6c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3223.c4114606.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/438.8a2feea6.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9158.14c35dba.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5581.62931c2c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7161.5d7bcda9.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3794.accddd33.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1065.4ae78d14.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6790.88de9007.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2524.59dec7a8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7322.065de7ae.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9055.b45d2d9f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8336.6f519370.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3051.0577bf68.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6243.1fcef8c6.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7230.1e111cb5.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5817.51121f34.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7833.ea4932aa.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8432.c7b80932.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8553.824612bf.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2061.1fb83cf1.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4466.02aabe48.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2340.f26f800a.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4937.49f4c5c0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7213.a2965adf.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9883.afdf0572.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3826.78d50da6.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4903.049ade3c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9669.41147939.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1003.c9dce3df.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2274.edc27c4c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6855.cf804307.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/695.bf361adc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7895.1cb6a5e0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5242.79cb6578.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7309.a4841ad3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6247.f8056779.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6158.55bdf3ee.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1262.27d6ef77.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5340.74a2bd3a.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6208.10d834de.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/602.03843908.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6296.73a3cfd3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4800.dac6a388.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8440.5c73e8f7.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7426.abda70dc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9269.eb26da67.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2909.3a2c9fe4.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9236.e1cddef8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/837.5adf6be0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7337.d4eca00f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/224.548e2465.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1689.52165dce.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/565.a55f1176.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8324.3ed5719d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5924.78fb1361.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/710.bbfad871.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2035.58660d40.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4489.47ad61f2.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8721.aeb9df13.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6705.c7f0d324.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7342.74bfd802.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8903.744a91ca.js" charset="utf-8"></script>
            
        
        <script nomodule>
            (function() {
                var message = 'Your browser does not support modern JavaScript modules. Please upgrade your browser for the best experience.';
                var warningDiv = document.createElement('div');
                warningDiv.style.color = 'red';
                warningDiv.style.padding = '10px';
                warningDiv.style.margin = '10px 0';
                warningDiv.style.border = '1px solid red';
                warningDiv.style.backgroundColor = 'lightyellow';
                warningDiv.innerText = message;
                document.body.prepend(warningDiv);
            })();
        </script>

        

        <!-- Fallback tracking pixels -->
        

        

        <noscript>
    <style>
        #nojs-banner {
            position: fixed;
            bottom: 0;
            left: 0;
            padding: 16px 16px 16px 32px;
            width: 100%;
            box-sizing: border-box;
            background: red;
            color: white;
            font-family: -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            font-size: 13px;
            line-height: 13px;
        }
        #nojs-banner a {
            color: inherit;
            text-decoration: underline;
        }
    </style>

    <div id="nojs-banner">
        This site requires JavaScript to run correctly. Please <a href="https://enable-javascript.com/" target="_blank">turn on JavaScript</a> or unblock scripts
    </div>
</noscript>


        

        

        
        
    </body>
</html>
