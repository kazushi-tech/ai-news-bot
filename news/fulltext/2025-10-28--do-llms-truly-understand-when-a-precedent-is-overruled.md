---
title: "Do LLMs Truly Understand When a Precedent Is Overruled?"
date: 2025-10-28
url: https://arxiv.org/abs/2510.20941
domain: arxiv.org
lang: en
tags: ["Paper"]
---
# Do LLMs Truly Understand When a Precedent Is Overruled?

## 🔗 引用元
- **URL**: https://arxiv.org/abs/2510.20941
- **サイト**: arXiv.org
- **著者**: [Submitted on 23 Oct 2025]
- **言語**: English
## 🧭 概要
View PDF
    HTML (experimental)
            Abstract:Large language models (LLMs) with extended context windows show promise for complex legal reasoning tasks, yet their ability to understand long legal documents remains insufficiently evaluated. Developing long-context benchmarks that capture realistic, high-stakes tasks remains a significant challenge in the field, as most existing evaluations rely on simplified synthetic tasks that fail to represent the complexity of real-world document understanding. Overruling relationships are foundational to common-law doctrine and commonly found in judicial opinions. They provide a focused and important testbed for long-document legal understanding that closely resembles what legal professionals actually do.

## 📝 詳細レポート
[View PDF](https://arxiv.org/pdf/2510.20941) [HTML (experimental)](https://arxiv.org/html/2510.20941v1)

> Abstract:Large language models (LLMs) with extended context windows show promise for complex legal reasoning tasks, yet their ability to understand long legal documents remains insufficiently evaluated. Developing long-context benchmarks that capture realistic, high-stakes tasks remains a significant challenge in the field, as most existing evaluations rely on simplified synthetic tasks that fail to represent the complexity of real-world document understanding. Overruling relationships are foundational to common-law doctrine and commonly found in judicial opinions. They provide a focused and important testbed for long-document legal understanding that closely resembles what legal professionals actually do. We present an assessment of state-of-the-art LLMs on identifying overruling relationships from U.S. Supreme Court cases using a dataset of 236 case pairs. Our evaluation reveals three critical limitations: (1) era sensitivity -- the models show degraded performance on historical cases compared to modern ones, revealing fundamental temporal bias in their training; (2) shallow reasoning -- models rely on shallow logical heuristics rather than deep legal comprehension; and (3) context-dependent reasoning failures -- models produce temporally impossible relationships in complex open-ended tasks despite maintaining basic temporal awareness in simple contexts. Our work contributes a benchmark that addresses the critical gap in realistic long-context evaluation, providing an environment that mirrors the complexity and stakes of actual legal reasoning tasks.

Submission history
------------------

From: Li Zhang \[[view email](https://arxiv.org/show-email/ff3ad865/2510.20941)\]  
**\[v1\]** Thu, 23 Oct 2025 19:07:42 UTC (79 KB)
