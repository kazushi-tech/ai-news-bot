---
title: "FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction"
date: 2025-10-28
url: https://arxiv.org/abs/2510.20926
domain: arxiv.org
lang: en
tags: ["Paper"]
---
# FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction

## 🔗 引用元
- **URL**: https://arxiv.org/abs/2510.20926
- **サイト**: arXiv.org
- **著者**: [Submitted on 23 Oct 2025]
- **言語**: English
## 🧭 概要
View PDF
    HTML (experimental)
            Abstract:As language models become capable of processing increasingly long and complex texts, there has been growing interest in their application within computational literary studies. However, evaluating the usefulness of these models for such tasks remains challenging due to the cost of fine-grained annotation for long-form texts and the data contamination concerns inherent in using public-domain literature. Current embedding similarity datasets are not suitable for evaluating literary-domain tasks because of a focus on coarse-grained similarity and primarily on very short text. We assemble and release FICSIM, a dataset of long-form, recently written fiction, including scores along 12 axes of similarity informed by author-produced metadata and validated by digital humanities scholars.

## 📝 詳細レポート
[View PDF](https://arxiv.org/pdf/2510.20926) [HTML (experimental)](https://arxiv.org/html/2510.20926v1)

> Abstract:As language models become capable of processing increasingly long and complex texts, there has been growing interest in their application within computational literary studies. However, evaluating the usefulness of these models for such tasks remains challenging due to the cost of fine-grained annotation for long-form texts and the data contamination concerns inherent in using public-domain literature. Current embedding similarity datasets are not suitable for evaluating literary-domain tasks because of a focus on coarse-grained similarity and primarily on very short text. We assemble and release FICSIM, a dataset of long-form, recently written fiction, including scores along 12 axes of similarity informed by author-produced metadata and validated by digital humanities scholars. We evaluate a suite of embedding models on this task, demonstrating a tendency across models to focus on surface-level features over semantic categories that would be useful for computational literary studies tasks. Throughout our data-collection process, we prioritize author agency and rely on continual, informed author consent.

Submission history
------------------

From: Natasha Johnson \[[view email](https://arxiv.org/show-email/7bae5152/2510.20926)\]  
**\[v1\]** Thu, 23 Oct 2025 18:30:19 UTC (617 KB)
