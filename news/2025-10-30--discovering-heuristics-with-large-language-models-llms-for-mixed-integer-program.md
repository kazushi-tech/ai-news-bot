# Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling

プロパティ  
- **リンク**: https://arxiv.org/abs/2510.24013  
- **日付**: 2025-10-30  

## 引用元
https://arxiv.org/abs/2510.24013

## 要約
Data provided by: ### Bookmark Bibliographic Tools # Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media # Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos # Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers # Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs # arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Subjects: Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Combinatorics (math.CO); Optimization and Control (math.OC) Cite as: arXiv:2510.24013 [cs.AI] &nbsp; (or arXiv:2510.24013v1 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.2510.24013 Focus to learn more arXiv-issued DOI via DataCite (pending registration) ## Submission history From: Ibrahim Cetinkaya [view email] [v1] Tue, 28 Oct 2025 02:43:04 UTC (1,092 KB) Full-text links: ## Access Paper: View a PDF of the paper titled Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling, by \.Ibrahim O\u{g}uz \c{C}etinkaya and 3 other authorsView PDFHTML (experimental)TeX Source view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-10 Change to browse by: cs cs.LG cs.NE math math.CO math.OC ### References &amp; Citations NASA ADSGoogle Scholar Semantic Scholar export BibTeX citation Loading... Donate &gt; cs &gt; arXiv:2510.24013 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu ## quick links Login Help Pages About --> # Computer Science > Artificial Intelligence arXiv:2510.24013 (cs) [Submitted on 28 Oct 2025] # Title:Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling Authors:İbrahim Oğuz Çetinkaya, İ.

## 詳細レポート
[2510.24013] Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling
  
  - Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate &gt; cs &gt; arXiv:2510.24013 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu ## quick links Login Help Pages About --> # Computer Science > Artificial Intelligence arXiv:2510.24013 (cs) [Submitted on 28 Oct 2025] # Title:Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling Authors:İbrahim Oğuz Çetinkaya, İ. Esra Büyüktahtakın, Parshin Shojaee, Chandan K. Reddy View a PDF of the paper titled Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling, by \.Ibrahim O\u{g}uz \c{C}etinkaya and 3 other authors View PDF HTML (experimental) Abstract:Our study contributes to the scheduling and combinatorial optimization literature with new heuristics discovered by leveraging the power of Large Language Models (LLMs). We focus on the single-machine total tardiness (SMTT) problem, which aims to minimize total tardiness by sequencing n jobs on a single processor without preemption, given processing times and due dates. We develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger (EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date (EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that employed simpler rule-based heuristics, we evaluate our LLM-discovered algorithms using rigorous criteria, including optimality gaps and solution time derived from a mixed-integer programming (MIP) formulation of SMTT. We compare their performance against state-of-the-art heuristics and exact methods across various job sizes (20, 100, 200, and 500 jobs). For instances with more than 100 jobs, exact methods such as MIP and dynamic programming become computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD rule and another widely used algorithm in the literature. MDDC consistently outperforms traditional heuristics and remains competitive with exact approaches, particularly on larger and more complex instances. This study shows that human-LLM collaboration can produce scalable, high-performing heuristics for NP-hard constrained combinatorial optimization, even under limited resources when effectively configured. Subjects: Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Combinatorics (math.CO); Optimization and Control (math.OC) Cite as: arXiv:2510.24013 [cs.AI] &nbsp; (or arXiv:2510.24013v1 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.2510.24013 Focus to learn more arXiv-issued DOI via DataCite (pending registration) ## Submission history From: Ibrahim Cetinkaya [view email] [v1] Tue, 28 Oct 2025 02:43:04 UTC (1,092 KB) Full-text links: ## Access Paper: View a PDF of the paper titled Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling, by \.Ibrahim O\u{g}uz \c{C}etinkaya and 3 other authorsView PDFHTML (experimental)TeX Source view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-10 Change to browse by: cs cs.LG cs.NE math math.CO math.OC ### References &amp; Citations NASA ADSGoogle Scholar Semantic Scholar export BibTeX citation Loading... ## BibTeX formatted citation &times; loading... Data provided by: ### Bookmark Bibliographic Tools # Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media # Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos # Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers # Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs # arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About

                - Help

              

            
            
              

                - contact arXivClick here to contact arXivContact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status
